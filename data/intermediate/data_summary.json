{
  "total_records": 2827,
  "benchmarks": [
    "adversarial_nli",
    "aider_polyglot",
    "arc_agi",
    "arc_ai2",
    "balrog",
    "bbh",
    "bool_q",
    "cad_eval",
    "common_sense_qa_2",
    "cybench",
    "deepresearchbench",
    "factorio_learning_environment",
    "fictionlivebench",
    "frontiermath",
    "frontiermath_tier_4",
    "geobench",
    "gpqa_diamond",
    "gsm8k",
    "gso",
    "hella_swag",
    "lambada",
    "lech_mazur_writing",
    "live_bench",
    "math_level_5",
    "metr_time_horizons",
    "mmlu",
    "open_book_qa",
    "os_universe",
    "os_world",
    "otis_mock_aime_2024_2025",
    "piqa",
    "science_qa",
    "simplebench",
    "superglue",
    "swe_bench_verified",
    "terminalbench",
    "the_agent_company",
    "trivia_qa",
    "weirdml",
    "wino_grande"
  ],
  "capabilities": [
    "abstract_reasoning",
    "advanced_mathematics",
    "advanced_reasoning",
    "agent_reasoning",
    "basic_tasks",
    "boolean_reasoning",
    "cad_design",
    "code_generation",
    "commonsense_reasoning",
    "competition_math",
    "complex_reasoning",
    "creative_writing",
    "cybersecurity",
    "factual_knowledge",
    "game_playing",
    "game_strategy",
    "general_knowledge",
    "geographic_knowledge",
    "language_modeling",
    "language_understanding",
    "long_horizon_planning",
    "mathematical_reasoning",
    "natural_language_inference",
    "os_interaction",
    "os_navigation",
    "physical_intuition",
    "reading_comprehension",
    "research_ability",
    "scientific_reasoning",
    "spatial_reasoning",
    "terminal_usage",
    "unusual_tasks",
    "writing_quality"
  ]
}