Model version,Score (AVG@5),Release date,Organization,Country,Training compute (FLOP),Training compute notes,Source,Source link (site from table),Notes,id
gemini-2.5-pro-preview-06-05,0.624,2025-06-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,SimpleBench Leaderboard,https://simple-bench.com/,,recViPEOMOrSB6VKK
gpt-5-pro-2025-10-06_high,0.611,2025-10-07,OpenAI,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,recbQ9dn731JxU1au
grok-4-0709,0.605,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",SimpleBench Leaderboard,https://simple-bench.com/,,reccvzfzhvNO8LxS5
claude-opus-4-1-20250805,0.6,2025-08-05,Anthropic,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,recNq0JntySeETz1j
claude-opus-4-20250514_12K,0.588,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,SimpleBench Leaderboard,https://simple-bench.com/,,recCa8wyF9haBALeE
gpt-5-2025-08-07_high,0.567,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",SimpleBench Leaderboard,https://simple-bench.com/,,recYzgCOm2JdsaO4v
claude-sonnet-4-5-20250929_12K,0.543,2025-09-29,Anthropic,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,recWdgpLTxKnAX8LL
o3-2025-04-16_high,0.531,2025-04-16,OpenAI,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,recI5aJlmr2NrvE0V
gemini-2.5-pro-exp-03-25,0.516,2025-03-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,SimpleBench Leaderboard,https://simple-bench.com/,,reccAb0YstbYurrte
claude-3-7-sonnet-20250219_12K,0.464,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,SimpleBench Leaderboard,https://simple-bench.com/,,reckxfqYaBADs0cCf
claude-sonnet-4-20250514_12K,0.455,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,SimpleBench Leaderboard,https://simple-bench.com/,,recOF5ZoFqZHYxpwD
claude-3-7-sonnet-20250219,0.449,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,SimpleBench Leaderboard,https://simple-bench.com/,,rectGr4imJANQzE34
o1-preview-2024-09-12,0.417,2024-09-12,OpenAI,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,rec5At0uBqsEuLVVH
claude-3-5-sonnet-20241022,0.414,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",SimpleBench Leaderboard,https://simple-bench.com/,,rec55sHDKD0gxxg5R
gemini-2.5-flash,0.412,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,SimpleBench Leaderboard,https://simple-bench.com/,,reciIN7KQ8EiMs6s9
DeepSeek-R1-0528,0.408,2025-05-28,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",SimpleBench Leaderboard,https://simple-bench.com/,,rec345Y86bjDNWZmV
o1-2024-12-17_high,0.401,2024-12-17,OpenAI,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,rec6j46lH0wCisYVL
DeepSeek-V3.1,0.4,2025-08-21,DeepSeek,China,3.594058e+24,3.407799999999999e+24 FLOP [base model] + 1.86258e+23 FLOP = 3.594058e+24 FLOP,SimpleBench Leaderboard,https://simple-bench.com/,,recAiu3FFJglQ0FdI
o4-mini-2025-04-16_high,0.387,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",SimpleBench Leaderboard,https://simple-bench.com/,,recodGIOZHwIsosFv
o1-2024-12-17_medium,0.367,2024-12-17,OpenAI,United States of America,,,SimpleBench Leaderboard,https://simple-bench.com/,,recCGePx18ITu5c6O
grok-3,0.361,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",SimpleBench Leaderboard,https://simple-bench.com/,,recoLH9cFM9CmL37j
gpt-4.5-preview-2025-02-27,0.345,2025-02-27,OpenAI,United States of America,2.1000001e+26,"Analysis of GPT-4.5's training cluster, in combination with 30% utilization relative to H100 16-bit output (or a plausible range of 20 to 50% utilization, given the possibility of FP8 training), yields a median estimate of ~2e25 FLOP. See notebook below for details.

https://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw 

This is consistent with OpenAI's statement that GPT-4.5 was a “new order of magnitude in compute” compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. 

In the ""Pretraining GPT-4.5"" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",SimpleBench Leaderboard,https://simple-bench.com/,,recNFMYJcGpWOttKB
gemini-exp-1206,0.311,2024-12-06,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",SimpleBench Leaderboard,https://simple-bench.com/,,rechDCACKgdAKATMz
qwen3-235b-a22b,0.31,2025-04-29,Alibaba,China,4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,SimpleBench Leaderboard,https://simple-bench.com/,,recZifiu78SQWQAFX
DeepSeek-R1,0.309,2025-01-20,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",SimpleBench Leaderboard,https://simple-bench.com/,,recVHI1CZTBTCgTmM
gemini-2.0-flash-thinking-exp-01-21,0.307,2025-01-21,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,,SimpleBench Leaderboard,https://simple-bench.com/,Assuming later of two experimental versions,recuXcLAsfUz4Bx9u
Llama-4-Maverick-17B-128E-Instruct,0.277,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",SimpleBench Leaderboard,https://simple-bench.com/,,recXdYB88sX3J1uDn
claude-3-5-sonnet-20240620,0.275,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",SimpleBench Leaderboard,https://simple-bench.com/,,recmO9zJFuB6oSrz8
DeepSeek-V3-0324,0.272,2025-03-24,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",SimpleBench Leaderboard,https://simple-bench.com/,,rec0QIxD9Dbi1PU3M
gemini-1.5-pro-002,0.271,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,SimpleBench Leaderboard,https://simple-bench.com/,,recJPOyD1j4nEFjGF
gpt-4.1-2025-04-14,0.27,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,SimpleBench Leaderboard,https://simple-bench.com/,,recXeUQqhdaSXasO2
Kimi-K2-Instruct,0.263,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,SimpleBench Leaderboard,https://simple-bench.com/,,recX4og5KAbGbOhwv
gpt-4-turbo-2024-04-09,0.251,2024-04-09,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,SimpleBench Leaderboard,https://simple-bench.com/,,reczQMtVDyVdqr3Fw
claude-3-opus-20240229,0.235,2024-02-29,Anthropic,United States of America,,Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,SimpleBench Leaderboard,https://simple-bench.com/,,reckWZi9nZmUCMK0d
Llama-3.1-405B-Instruct,0.23,2024-07-23,Meta AI,United States of America,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",SimpleBench Leaderboard,https://simple-bench.com/,,recHm5YlK5f56Qa1p
o3-mini-2025-01-31_high,0.228,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",SimpleBench Leaderboard,https://simple-bench.com/,,recsKDEJe3WUTzlXB
grok-2-1212,0.227,2024-12-12,xAI,United States of America,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,SimpleBench Leaderboard,https://simple-bench.com/,,recU76IB1e4qQPMy0
mistral-large-2407,0.225,2024-07-24,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",SimpleBench Leaderboard,https://simple-bench.com/,,recqB8MS2LaGRGShV
gpt-oss-120b,0.221,2025-08-05,OpenAI,United States of America,4.94e+24,"""The training run for gpt-oss-120b required 2.1 million H100-hours to complete""
(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24
They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24

",SimpleBench Leaderboard,https://simple-bench.com/,,recFVmMzFAlgDScJC
Llama-3.3-70B-Instruct,0.199,2024-12-06,Meta AI,United States of America,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",SimpleBench Leaderboard,https://simple-bench.com/,,recSRP5IhOYxwzPxz
DeepSeek-V3,0.189,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",SimpleBench Leaderboard,https://simple-bench.com/,,recc5o0Xjz6mlrIKU
gemini-2.0-flash-exp,0.189,2024-12-11,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",SimpleBench Leaderboard,https://simple-bench.com/,,recpuJCN7xqjCU4hn
o1-mini-2024-09-12_medium,0.181,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",SimpleBench Leaderboard,https://simple-bench.com/,Assuming medium reasoning effort,recY4JPURbdldMOQR
gpt-4o-2024-08-06,0.178,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,SimpleBench Leaderboard,https://simple-bench.com/,,receDGtlCRZB7DW1z
c4ai-command-r-plus-08-2024,0.174,2024-08-30,"Cohere,Cohere for AI",Canada,,,SimpleBench Leaderboard,https://simple-bench.com/,,recMrDg2K2bDD6w9J
gpt-4o-mini-2024-07-18,0.107,2024-07-18,OpenAI,United States of America,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",SimpleBench Leaderboard,https://simple-bench.com/,,recRSFVLtD45NI9Tm
