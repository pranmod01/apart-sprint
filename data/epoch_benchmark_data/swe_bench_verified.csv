Model version,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gpt-5-mini-2025-08-07_high,0.616,2025-08-07,OpenAI,United States of America,,,0.02177236946554712,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FHGLcT8RytVK548aPQ73s7f.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/HGLcT8RytVK548aPQ73s7f.eval,2025-10-30T10:54:58.088Z,HGLcT8RytVK548aPQ73s7f
claude-haiku-4-5-20251001,0.606,2025-10-15,Anthropic,United States of America,,,0.02187429930168918,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNNRfKgR23F8kXjFh8jin8g.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NNRfKgR23F8kXjFh8jin8g.eval,2025-10-16T09:36:29.346Z,NNRfKgR23F8kXjFh8jin8g
claude-sonnet-4-5-20250929,0.648,2025-09-29,Anthropic,United States of America,,,0.02138004238594605,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FiZoPmtRrd5k6qnrRraiLZo.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/iZoPmtRrd5k6qnrRraiLZo.eval,2025-09-29T18:41:46.048Z,iZoPmtRrd5k6qnrRraiLZo
DeepSeek-V3.1,0.5210420841683366,2025-08-21,DeepSeek,China,3.594058e+24,3.407799999999999e+24 FLOP [base model] + 1.86258e+23 FLOP = 3.594058e+24 FLOP,0.022385685987618295,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F2VVQUtzK8sSDs4FAFZnWWB.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/2VVQUtzK8sSDs4FAFZnWWB.eval,2025-09-08T19:07:08.124Z,2VVQUtzK8sSDs4FAFZnWWB
o3-2025-04-16_medium,0.43687374749499,2025-04-16,OpenAI,United States of America,,,0.02222624973310027,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F5bSiKbVpt5fDXcBEf7YS68.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/5bSiKbVpt5fDXcBEf7YS68.eval,2025-08-12T20:28:47.323Z,5bSiKbVpt5fDXcBEf7YS68
gpt-4o-2024-11-20,0.254,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.01948659680164342,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPazJb6CuZfUTKf6dtz5Z45.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PazJb6CuZfUTKf6dtz5Z45.eval,2025-08-07T20:02:18.179Z,PazJb6CuZfUTKf6dtz5Z45
gpt-5-mini-2025-08-07_medium,0.578,2025-08-07,OpenAI,United States of America,,,0.022109039310618563,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FRRbQkGCVhZCeRDt8kB27TB.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/RRbQkGCVhZCeRDt8kB27TB.eval,2025-08-07T19:36:05.576Z,RRbQkGCVhZCeRDt8kB27TB
gpt-5-2025-08-07_medium,0.588,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.02203367799374094,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FknygsTpEGHyBFW4iSAWzn6.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/knygsTpEGHyBFW4iSAWzn6.eval,2025-08-07T19:34:24.226Z,knygsTpEGHyBFW4iSAWzn6
Kimi-K2-Instruct,0.164,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.016575811142446696,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FKKdaasPdbUXmehYK7QNLyc.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/KKdaasPdbUXmehYK7QNLyc.eval,2025-08-07T10:57:45.745Z,KKdaasPdbUXmehYK7QNLyc
claude-opus-4-1-20250805,0.632,2025-08-05,Anthropic,United States of America,,,0.021588982568353548,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9CTibDqrsrcQzMuFTaCZem.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9CTibDqrsrcQzMuFTaCZem.eval,2025-08-05T16:47:35.000Z,9CTibDqrsrcQzMuFTaCZem
DeepSeek-R1-0528,0.33266533066132264,2025-05-28,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.021113534574855188,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FH4X3AsrkcXMTQNKdrAgJe7.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/H4X3AsrkcXMTQNKdrAgJe7.eval,2025-05-29T18:54:36.387Z,H4X3AsrkcXMTQNKdrAgJe7
claude-opus-4-20250514,0.622,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02170655082451827,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6fbE9sokzKiRhNPfRnVnZA.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6fbE9sokzKiRhNPfRnVnZA.eval,2025-05-27T19:16:08.967Z,6fbE9sokzKiRhNPfRnVnZA
claude-sonnet-4-20250514,0.606,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02187429930168918,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FBL9e5eBaVdxVcws6qPwrdh.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/BL9e5eBaVdxVcws6qPwrdh.eval,2025-05-22T23:06:02.013Z,BL9e5eBaVdxVcws6qPwrdh
claude-3-5-sonnet-20240620,0.32,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.020882340488761718,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FH2sQhavY5BPYxyYHva8mgS.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/H2sQhavY5BPYxyYHva8mgS.eval,2025-05-14T14:00:28.961Z,H2sQhavY5BPYxyYHva8mgS
qwen-plus-2025-04-28,0.28,2025-04-28,Alibaba,China,,,0.020099950647503192,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FSCpoD9h4p73bd6SwebPop4.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/SCpoD9h4p73bd6SwebPop4.eval,2025-05-08T11:28:22.466Z,SCpoD9h4p73bd6SwebPop4
o3-mini-2025-01-31_medium,0.378,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.02170655082451827,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FkvVyU6crkxAnG3DAUciMGc.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/kvVyU6crkxAnG3DAUciMGc.eval,2025-05-08T10:07:01.108Z,kvVyU6crkxAnG3DAUciMGc
claude-3-5-sonnet-20241022,0.406,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.02198396209008642,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FGUbGX7GABSbhxMXiYC5NFr.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/GUbGX7GABSbhxMXiYC5NFr.eval,2025-05-02T17:35:05.525Z,GUbGX7GABSbhxMXiYC5NFr
gpt-4.1-mini-2025-04-14,0.328,2025-04-14,OpenAI,United States of America,,,0.021017027165175468,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdD8n3SgLkV3Xse6bbG8mMB.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dD8n3SgLkV3Xse6bbG8mMB.eval,2025-04-30T11:13:40.513Z,dD8n3SgLkV3Xse6bbG8mMB
gpt-4.1-2025-04-14,0.41,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.022017482578127686,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQ6UB2pBAwruHWXCNQvKGVv.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Q6UB2pBAwruHWXCNQvKGVv.eval,2025-04-30T11:13:27.193Z,Q6UB2pBAwruHWXCNQvKGVv
grok-3-beta,0.386,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.021793529219281196,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FSp6nNR4N7zRGrqPCkgJ4Mn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Sp6nNR4N7zRGrqPCkgJ4Mn.eval,2025-04-30T09:43:26.462Z,Sp6nNR4N7zRGrqPCkgJ4Mn
gemini-2.0-flash-001,0.22,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",0.01854421137582028,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fne2Anciuxh2k3VXGYG4Nqy.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ne2Anciuxh2k3VXGYG4Nqy.eval,2025-04-25T07:22:41.529Z,ne2Anciuxh2k3VXGYG4Nqy
grok-3-mini-beta_low,0.152,2025-04-09,xAI,United States of America,,,0.016071982367911835,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMgQteQgHEpQAAQw7DFbh2u.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MgQteQgHEpQAAQw7DFbh2u.eval,2025-04-25T06:42:35.525Z,MgQteQgHEpQAAQw7DFbh2u
claude-3-7-sonnet-20250219,0.522,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.02236139673920787,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMCv2wDLoTzqMRrN8dATPEv.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MCv2wDLoTzqMRrN8dATPEv.eval,2025-04-24T22:49:08.467Z,MCv2wDLoTzqMRrN8dATPEv
o4-mini-2025-04-16_medium,0.346,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.021294951277234693,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FeTCmM2smJzrY4xjJnrboT2.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/eTCmM2smJzrY4xjJnrboT2.eval,2025-04-24T19:01:47.014Z,eTCmM2smJzrY4xjJnrboT2
