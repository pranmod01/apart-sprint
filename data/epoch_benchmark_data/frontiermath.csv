Model version,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gpt-5-mini-2025-08-07_high,0.19655172413793104,2025-08-07,OpenAI,United States of America,,,0.023375906908472147,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/UShNRMFSDZu2tLwcR8FG8o.eval,2025-10-30T16:22:06.331Z,UShNRMFSDZu2tLwcR8FG8o
gpt-5-nano-2025-08-07_high,0.08275862068965517,2025-08-07,OpenAI,United States of America,,,0.01620688385836769,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/2eMHpXXR9h7P2oNPDQkTBY.eval,2025-10-30T16:20:13.144Z,2eMHpXXR9h7P2oNPDQkTBY
gpt-5-2025-08-07_high,0.2655172413793103,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.02597695517914575,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/cMUzhAsfM285ES67ynVy6x.eval,2025-10-30T02:11:45.854Z,cMUzhAsfM285ES67ynVy6x
claude-haiku-4-5-20251001_32K,0.059027777777777776,2025-10-15,Anthropic,United States of America,,,0.013911554772795832,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/fdNzD5mp8r2bHxy2PXx6Gh.eval,2025-10-22T12:29:30.662Z,fdNzD5mp8r2bHxy2PXx6Gh
claude-sonnet-4-5-20250929_32K,0.10344827586206896,2025-09-29,Anthropic,United States of America,,,0.017914322244072713,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/4JcXA2wWLXGDc5vDiAGazy.eval,2025-10-22T11:04:23.184Z,4JcXA2wWLXGDc5vDiAGazy
claude-haiku-4-5-20251001,0.041379310344827586,2025-10-15,Anthropic,United States of America,,,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8sTJmYZ7MSk8obK9vjUjHm.eval,2025-10-16T09:34:31.339Z,8sTJmYZ7MSk8obK9vjUjHm
claude-sonnet-4-5-20250929,0.05172413793103448,2025-09-29,Anthropic,United States of America,,,0.013027619248139074,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/J3Rucr85Lfo67UcSRxWyBB.eval,2025-09-29T18:08:24.203Z,J3Rucr85Lfo67UcSRxWyBB
gpt-5-mini-2025-08-07_medium,0.19310344827586207,2025-08-07,OpenAI,United States of America,,,0.023219615450311044,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/9osUw9BJpUhScjwjhk7b4d.eval,2025-08-07T18:43:36.879Z,9osUw9BJpUhScjwjhk7b4d
gpt-5-2025-08-07_medium,0.2482758620689655,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.02541251077219606,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/J4g9pcSnmeKzZYfhZmXjVY.eval,2025-08-07T18:43:29.196Z,J4g9pcSnmeKzZYfhZmXjVY
gpt-5-nano-2025-08-07_medium,0.07241379310344828,2025-08-07,OpenAI,United States of America,,,0.015245401561146893,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/dHQzrNGjuR5kiHKtNxG62X.eval,2025-08-07T18:43:21.321Z,dHQzrNGjuR5kiHKtNxG62X
Kimi-K2-Instruct,0.020689655172413793,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.008373130807525473,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/RtYPbmy8Uxxn4dVaMrhaX9.eval,2025-08-07T15:08:21.704Z,RtYPbmy8Uxxn4dVaMrhaX9
claude-opus-4-20250514_27K,0.041379310344827586,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZKroUnGN6qoNssoSQbtzzz.eval,2025-08-05T23:13:11.048Z,ZKroUnGN6qoNssoSQbtzzz
claude-opus-4-1-20250805_27K,0.07241379310344828,2025-08-05,Anthropic,United States of America,,,0.015245401561146893,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ezJ5C38svWJkQMjxTcrgg7.eval,2025-08-05T19:56:41.845Z,ezJ5C38svWJkQMjxTcrgg7
claude-opus-4-1-20250805,0.05862068965517241,2025-08-05,Anthropic,United States of America,,,0.013818435156390344,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/TQQnqmugDk3GX5hkr8HMWz.eval,2025-08-05T19:56:40.259Z,TQQnqmugDk3GX5hkr8HMWz
claude-opus-4-20250514,0.04482758620689655,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.012172075609474642,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/GKD2ELzaXHx3giARevEetr.eval,2025-07-04T10:42:31.483Z,GKD2ELzaXHx3giARevEetr
claude-sonnet-4-20250514,0.041379310344827586,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/UcfxxJmo4ftRzPEnXrRdSw.eval,2025-07-04T10:41:31.264Z,UcfxxJmo4ftRzPEnXrRdSw
gemini-2.5-pro,0.1103448275862069,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.018430534185153016,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/HN7YDEAzGvPRMBDo7mGuXG.eval,2025-07-03T15:43:09.196Z,HN7YDEAzGvPRMBDo7mGuXG
gemini-2.5-pro-preview-06-05,0.10344827586206896,2025-06-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.01791432224407271,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/hNLQNC6TASXNNzat8kVS77.eval,2025-07-03T13:57:21.637Z,hNLQNC6TASXNNzat8kVS77
qwen-plus-2025-04-28,0.017241379310344827,2025-04-28,Alibaba,China,,,0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bVFEkuHiUSxjoh8SQm74jo.eval,2025-05-14T13:10:51.812Z,bVFEkuHiUSxjoh8SQm74jo
mistral-medium-2505,0.0034602076124567475,2025-05-07,Mistral AI,France,,"Benchmarks match with models like GPT-4o, Mistral's previous largest runs were ~1e25 FLOP scale, so plausibly they might have trained Medium with this much compute.",0.003460207612456747,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/PZRX6ME25bn26B6BTRWx6W.eval,2025-05-07T17:33:33.886Z,PZRX6ME25bn26B6BTRWx6W
o4-mini-2025-04-16_medium,0.19310344827586207,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.023219615450311044,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8ozSh7wRxGu4zL4yjyEF2L.eval,2025-04-22T14:05:40.240Z,8ozSh7wRxGu4zL4yjyEF2L
o4-mini-2025-04-16_low,0.09655172413793103,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.017373316921187713,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/HTqZUWGZ8bXbJ4dBRUZ3wv.eval,2025-04-22T14:03:22.274Z,HTqZUWGZ8bXbJ4dBRUZ3wv
o3-2025-04-16_low,0.10344827586206896,2025-04-16,OpenAI,United States of America,,,0.01791432224407271,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/4yHiPUjmcqGZijjnQ6V25s.eval,2025-04-22T01:30:58.458Z,4yHiPUjmcqGZijjnQ6V25s
o3-2025-04-16_medium,0.1,2025-04-16,OpenAI,United States of America,,,0.017647058823529408,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/jMt3i7znSKdhTniR6o6b5C.eval,2025-04-22T01:30:43.524Z,jMt3i7znSKdhTniR6o6b5C
o3-2025-04-16_high,0.10344827586206896,2025-04-16,OpenAI,United States of America,,,0.01791432224407271,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/dECNMk4kcUtMHLzb3tgaCX.eval,2025-04-17T16:24:28.832Z,dECNMk4kcUtMHLzb3tgaCX
o4-mini-2025-04-16_high,0.1724137931034483,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.02221998204889112,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ir92cSRbauVXmQfqTSaiSx.eval,2025-04-17T14:58:22.342Z,ir92cSRbauVXmQfqTSaiSx
gpt-4.1-nano-2025-04-14,0.010344827586206896,2025-04-14,OpenAI,United States of America,,,0.0059518867144507945,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/chtQvebcoaE33KkhAjbCnw.eval,2025-04-14T22:51:44.342Z,chtQvebcoaE33KkhAjbCnw
gpt-4.1-mini-2025-04-14,0.04482758620689655,2025-04-14,OpenAI,United States of America,,,0.01217207560947464,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ephMMUGDANc9iVTNNuuoZx.eval,2025-04-14T22:49:44.216Z,ephMMUGDANc9iVTNNuuoZx
gpt-4.1-2025-04-14,0.05517241379310345,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.013430381628597847,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/NPeiYpjbscreYnFkHPvcNB.eval,2025-04-14T22:40:01.283Z,NPeiYpjbscreYnFkHPvcNB
grok-3-mini-beta_low,0.027586206896551724,2025-04-09,xAI,United States of America,,,0.009634354634513523,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/9FTMS2t6Ksm43CduHeqtJr.eval,2025-04-10T20:30:21.697Z,9FTMS2t6Ksm43CduHeqtJr
grok-3-mini-beta_high,0.05862068965517241,2025-04-09,xAI,United States of America,,,0.013818435156390344,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z83F6bJ7Ne5rXaXwgR2xWT.eval,2025-04-10T18:02:17.992Z,Z83F6bJ7Ne5rXaXwgR2xWT
grok-3-beta,0.03793103448275862,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.011237029601999628,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DzjFQArKFTZHioQVwKYVtX.eval,2025-04-10T17:57:03.446Z,DzjFQArKFTZHioQVwKYVtX
Llama-4-Maverick-17B-128E-Instruct-FP8,0.006896551724137931,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.004868154158215009,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/QjvBS34fsngfBBMZUirSyh.eval,2025-04-08T10:17:16.175Z,QjvBS34fsngfBBMZUirSyh
Llama-4-Scout-17B-16E-Instruct,0.0,2025-04-05,Meta AI,United States of America,4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bhjLDA5A7gddsX5cAYKma8.eval,2025-04-08T10:14:56.486Z,bhjLDA5A7gddsX5cAYKma8
qwen-max-2025-01-25,0.010344827586206896,2025-01-25,Alibaba,China,,,0.005951886714450794,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/F77vDPynTKJbz7m2bt7Py8.eval,2025-04-02T16:55:37.430Z,F77vDPynTKJbz7m2bt7Py8
claude-3-7-sonnet-20250219_64K,0.03103448275862069,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.01020064175308735,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/DoU54Xy93J3MxztC4uxwYy.eval,2025-03-13T16:49:59.754Z,DoU54Xy93J3MxztC4uxwYy
claude-3-7-sonnet-20250219_32K,0.034482758620689655,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.010733271038801586,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/aZqVpPa4SWHapN9QHEoEm3.eval,2025-03-13T09:34:11.446Z,aZqVpPa4SWHapN9QHEoEm3
gemini-2.0-flash-001,0.017241379310344827,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/5RpSr8gJZfcZn586aV2hDW.eval,2025-03-09T17:18:49.858Z,5RpSr8gJZfcZn586aV2hDW
DeepSeek-V3,0.017241379310344827,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/aXcfCytWFYNk7E5AZGMzLb.eval,2025-03-07T23:36:27.507Z,aXcfCytWFYNk7E5AZGMzLb
o1-2024-12-17_high,0.09310344827586207,2024-12-17,OpenAI,United States of America,,,0.017092785280147835,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/bynpttv6jLvbXTY7ktnBpk.eval,2025-03-07T19:46:02.827Z,bynpttv6jLvbXTY7ktnBpk
gpt-4o-2024-08-06,0.0034482758620689655,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/SmT4ukjkxsXZvFExMMVFgW.eval,2025-03-07T18:56:32.370Z,SmT4ukjkxsXZvFExMMVFgW
claude-3-5-sonnet-20240620,0.010344827586206896,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.0059518867144507945,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/h7f9jZVWzZNDHNxP5eewNH.eval,2025-03-07T18:54:11.499Z,h7f9jZVWzZNDHNxP5eewNH
gemini-1.5-flash-002,0.0,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8FtfWCm8DdAqF9rnMVwv8h.eval,2025-03-07T18:40:43.945Z,8FtfWCm8DdAqF9rnMVwv8h
claude-3-5-haiku-20241022,0.0034482758620689655,2024-10-22,Anthropic,United States of America,,,0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/JQcqpW9xPfJRKJkHne6DpK.eval,2025-03-07T18:11:31.647Z,JQcqpW9xPfJRKJkHne6DpK
claude-3-5-sonnet-20241022,0.020689655172413793,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.008373130807525473,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/MVnDrt94wyokzx3a7ZJkph.eval,2025-03-06T23:54:46.118Z,MVnDrt94wyokzx3a7ZJkph
o1-mini-2024-09-12_medium,0.017241379310344827,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.007657032895812118,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/hC2FY4WuKm6BLgPhbWeZDY.eval,2025-03-06T22:32:06.577Z,hC2FY4WuKm6BLgPhbWeZDY
o1-mini-2024-09-12_high,0.013793103448275862,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.006860663093423027,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/2ekT2UzaebvmGprKvmeeEa.eval,2025-03-06T22:30:16.233Z,2ekT2UzaebvmGprKvmeeEa
claude-3-7-sonnet-20250219_16K,0.041379310344827586,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.011715642254112493,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/PU7LXbdNB8Unfvu4qCeJZF.eval,2025-03-06T19:16:31.497Z,PU7LXbdNB8Unfvu4qCeJZF
o3-mini-2025-01-31_high,0.1103448275862069,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.018430534185153016,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/GZeqEeaiEhnLJSzVogT9N4.eval,2025-03-06T19:10:51.727Z,GZeqEeaiEhnLJSzVogT9N4
grok-2-1212,0.006896551724137931,2024-12-12,xAI,United States of America,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,0.004868154158215009,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/kinyvFjqSqUiY5FyYv7tsA.eval,2025-03-06T18:06:22.963Z,kinyvFjqSqUiY5FyYv7tsA
o3-mini-2025-01-31_medium,0.08081896551724138,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.012761417139125066,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/U8CjtcxudQeP2cnMNixfJH.eval,2025-03-06T18:06:08.042Z,U8CjtcxudQeP2cnMNixfJH
claude-3-7-sonnet-20250219,0.03103448275862069,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.010200641753087351,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/m7X2ppkiEkweCtFppiRW8S.eval,2025-03-06T18:05:58.538Z,m7X2ppkiEkweCtFppiRW8S
mistral-large-2411,0.0034482758620689655,2024-11-18,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/cEvTUVyxAqq5UDgsGQbvLM.eval,2025-03-06T18:05:57.178Z,cEvTUVyxAqq5UDgsGQbvLM
gpt-4o-2024-11-20,0.0034482758620689655,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.003448275862068965,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Z7HU95T8eema56EpLtECvG.eval,2025-03-06T18:03:58.731Z,Z7HU95T8eema56EpLtECvG
grok-4-0709,0.1211,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.0192,,,,gda5UeWrA8HcbDCRuLJ56H
gemini-2.5-deep-think-2025-08-01-webapp,0.29,2025-08-01,"Google,Google DeepMind","United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.027,,,,manual_gemini_2.5_deep_think_run_frontiermath_tiers_1_to_3
