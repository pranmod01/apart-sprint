Model version,Tools,ACW Avg Score,Release date,Organization,Country,Training compute (FLOP),Training compute notes,ACW Country %,ACW Median Score,ACW Median Distance (km),ACW Refusal,AVW Country %,AVW Avg Score,AVW Median Score,AVW Median Distance (km),AVW Refusal,Rural Country %,Rural Avg Score,Rural Median Score,Rural Median Distance,Rural Refusal,Urban Country %,Urban Avg Score,Urban Median Score,Urban Median Distance (km),Urban Refusal,Photos Country %,Photos Avg Score,Photos Median Score,Photos Median Distance (km),Photos Refusal,Notes,Source,Source link,id
gemini-2.5-pro-exp-03-25,Search,4093,2025-03-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.84,4575,165,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recKwlVhpDLw3jftZ
gemini-2.5-pro-exp-03-25,None,3999,2025-03-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.78,4559,171,0.0,0.76,4097.0,4409.0,234.0,0.0,0.7,3726.0,4098.0,296.0,0.0,0.98,4118.0,4481.0,153.0,0.0,0.68,3871.0,4523.0,139.0,0.0,,GeoBench leaderboard,https://geobench.org/,recg6GWc9cY6qgqhY
gemini-1.5-flash-002,None,3934,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.76,4376,248,0.0,,,,,,,,,,,,,,,,0.64,3458.0,4179.0,258.0,0.0,Assuming latest Flash version,GeoBench leaderboard,https://geobench.org/,recZO3tP9x1pQsytO
o3-2025-04-16_high,None,3930,2025-04-16,OpenAI,United States of America,,,0.6,4433,235,0.18,,,,,,,,,,,,,,,,0.56,3789.0,4194.0,244.0,0.1,,GeoBench leaderboard,https://geobench.org/,recMNCiUt6YNqVDKL
o1-2024-12-17_medium,None,3925,2024-12-17,OpenAI,United States of America,,,0.8,4252,337,0.0,,,,,,,,,,,,,,,,0.52,3330.0,4061.0,313.0,0.0,Assuming medium reasoning effort.,GeoBench leaderboard,https://geobench.org/,recu2g66G4TcuHV94
gemini-2.0-flash-001,None,3897,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",0.77,4328,268,0.0,0.68,3743.0,4281.0,315.0,0.0,0.58,3426.0,3750.0,443.0,0.0,0.94,3898.0,4229.0,245.0,0.0,0.6,3659.0,4073.0,284.0,0.0,,GeoBench leaderboard,https://geobench.org/,rec7kPZI1YDSo2DgB
gemini-2.0-pro-exp-02-05,None,3884,2025-02-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in early 2025; very likely it used >1e25 FLOP.,0.78,4503,194,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,reclHgkpvjV780Uoa
o3-2025-04-16_medium,None,3858,2025-04-16,OpenAI,United States of America,,,0.74,4389,242,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recyDcak8Z17j2dz8
gemini-2.5-flash-preview-04-17,None,3854,2025-04-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.73,4292,283,0.0,,,,,,,,,,,,,,,,,,,,,"Assuming ""Gemini 2.5 Flash Experimental"" refers to gemini-2.5-flash-preview-04-17 since there is no version with experimental in the name",GeoBench leaderboard,https://geobench.org/,recZQKTTSvZulfgVB
claude-3-7-sonnet-20250219_15K,None,3844,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.65,4386,243,0.0,,,,,,,,,,,,,,,,0.42,3027.0,3764.0,472.0,0.0,,GeoBench leaderboard,https://geobench.org/,recavCfmmQ6tMEwtu
claude-3-7-sonnet-20250219,None,3791,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.68,4253,300,0.0,,,,,,,,,,,,,,,,0.48,3123.0,3898.0,371.0,0.0,,GeoBench leaderboard,https://geobench.org/,recz7nkHBBRKXIWzd
gpt-4.1-2025-04-14,None,3791,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.72,4373,272,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,rec85xrmzw8WaUL5H
gemini-2.0-flash-thinking-exp-01-21,None,3767,2025-01-21,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.7,4382,245,0.0,,,,,,,,,,,,,,,,0.66,3873.0,4352.0,216.0,0.0,Assumung latest Gemini 2.0 Flash thinking version,GeoBench leaderboard,https://geobench.org/,recmnSNuTyajmziUQ
o4-mini-2025-04-16_high,None,3717,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.64,4312,274,0.09,,,,,,,,,,,,,,,,0.52,3437.0,4150.0,258.0,0.02,,GeoBench leaderboard,https://geobench.org/,rec1VvZVwbTipNWEC
gpt-4o-2024-11-20,None,3696,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.71,4300,279,0.0,0.68,3740.0,4186.0,330.0,0.0,0.6,3485.0,3970.0,351.0,0.0,0.88,4099.0,4563.0,128.0,0.0,0.6,3426.0,4152.0,294.0,0.0,Assuming latest vesrsion of gpt-4o,GeoBench leaderboard,https://geobench.org/,rec2Ind0RGchMKj3j
o4-mini-2025-04-16_medium,None,3651,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.64,4303,294,0.1,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recUod4Wgb4Jaz8cA
Qwen2.5-VL-72B-Instruct,None,3448,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",0.62,4008,442,0.0,,,,,,,,,,,,,,,,0.5,3286.0,3826.0,379.0,0.0,,GeoBench leaderboard,https://geobench.org/,recYbzPfMGsVvfzsa
gpt-4o-mini-2024-07-18,None,3401,2024-07-18,OpenAI,United States of America,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",0.64,3876,472,0.0,,,,,,0.48,2964.0,3587.0,544.0,0.0,0.74,3329.0,3868.0,391.0,0.0,0.44,2831.0,3394.0,538.0,0.0,,GeoBench leaderboard,https://geobench.org/,recpyh8ly8Rnrqvgj
claude-3-5-sonnet-20241022,None,3327,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.62,4167,338,0.0,,,,,,,,,,,,,,,,0.48,2919.0,3713.0,486.0,0.0,Assuming latest Sonnet model,GeoBench leaderboard,https://geobench.org/,rec3IW78fmDClmevf
gemma-3-27b-it,None,3087,2025-03-12,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",2.268e+24,6ND =  6 * 27B parameters * 14T training tokens = 2.268 × 10^24 FLOP,0.52,3822,502,0.0,,,,,,,,,,,,,,,,0.4,2857.0,3564.0,474.0,0.0,,GeoBench leaderboard,https://geobench.org/,recNtW91J7ujwVEd3
Llama-4-Maverick-17B-128E-Instruct,None,3069,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.52,3650,583,0.0,,,,,,,,,,,,,,,,0.5,2942.0,3580.0,463.0,0.02,,GeoBench leaderboard,https://geobench.org/,recZ3e3Hb3wbvXFIV
Llama-3.2-90B-Vision-Instruct,None,2798,2024-09-24,Meta AI,United States of America,,,0.52,3529,646,0.07,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recuaxYkSjCYJAD6e
claude-3-5-haiku-20241022,None,2507,2024-10-22,Anthropic,United States of America,,,0.34,2862,1035,0.0,0.36,2528.0,3130.0,928.0,0.0,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recno5e6hTQGMAJmt
Pixtral-12B-2409,None,2131,2024-09-17,Mistral AI,France,,,0.37,2227,1500,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,rectZf8KZ5UNsoaMx
gemini-2.5-pro-preview-05-06,None,4085,2025-05-06,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.86,4581,162,0.0,,,,,,,,,,,,,,,,0.7,3836.0,4435.0,187.0,0.0,,GeoBench leaderboard,https://geobench.org/,recVj2Vj168vkJXaT
gemini-2.5-pro-preview-05-06,Search,3983,2025-05-06,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.81,4466,209,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recEx1qcKpzEhL29G
claude-opus-4-20250514_32K,None,2915,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.49,3626,596,0.0,,,,,,,,,,,,,,,,,,,,,"Thinking token budget was 31,448. Rounded to 32k to reference existing model version.",GeoBench leaderboard,https://geobench.org/,rec9ARZdPBfnWVjCp
claude-sonnet-4-20250514,None,2468,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.37,2791,1091,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recc6aFqMfrhbx2q0
claude-sonnet-4-20250514_32K,None,2369,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.32,2726,1192,0.0,,,,,,,,,,,,,,,,,,,,,"Thinking token budget was 31,448. Rounded to 32k to reference existing model version.",GeoBench leaderboard,https://geobench.org/,rec633RXeiU9CsPRR
gemini-2.5-flash-preview-05-20,None,3854,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.73,4292,283,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recdQQcryv5aailik
gemini-2.5-pro,None,4019,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.81,4472,207,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,recumPLYAlNSqKfbH
grok-4-0709,None,2719,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.45,3543,639,0.0,,,,,,,,,,,,,,,,,,,,,,GeoBench leaderboard,https://geobench.org/,reckc8skUG0WI9i4f
gpt-5-2025-08-07_medium,None,3937,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.81,4458,217,0.0,,,,,,,,,,,,,,,,0.58,3498.0,4082.0,288.0,0.0,,GeoBench leaderboard,https://geobench.org/,recFgJeRfhlSbQeu9
