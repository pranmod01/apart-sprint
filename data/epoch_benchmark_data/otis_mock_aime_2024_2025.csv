Model version,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gpt-5-nano-2025-08-07_high,0.8111111111111111,2025-08-07,OpenAI,United States of America,,,0.04626556218905063,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FY9YnaJQWpPFAPhe6jtKMhH.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Y9YnaJQWpPFAPhe6jtKMhH.eval,2025-10-31T09:55:38.159Z,Y9YnaJQWpPFAPhe6jtKMhH
gpt-5-mini-2025-08-07_high,0.8666666666666667,2025-08-07,OpenAI,United States of America,,,0.04286165199944133,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FR82gycJfm5jaCdh32tFkH4.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/R82gycJfm5jaCdh32tFkH4.eval,2025-10-30T09:59:54.926Z,R82gycJfm5jaCdh32tFkH4
gpt-5-2025-08-07_high,0.9138888888888889,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.03658872186755414,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F3D8aaFBLZwQcnWHYKwvnmq.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/3D8aaFBLZwQcnWHYKwvnmq.eval,2025-10-29T22:29:41.753Z,3D8aaFBLZwQcnWHYKwvnmq
claude-sonnet-4-5-20250929_59K,0.7777777777777778,,Anthropic,United States of America,,,0.06267511942419625,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fk5PDL6MpCC6P5eLAhodJiS.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/k5PDL6MpCC6P5eLAhodJiS.eval,2025-10-28T10:21:02.293Z,k5PDL6MpCC6P5eLAhodJiS
claude-sonnet-4-5-20250929_16K,0.7222222222222222,2025-09-29,Anthropic,United States of America,,,0.06749804954603589,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F7QkPRbRPpRx8nJT2ZcqSx6.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/7QkPRbRPpRx8nJT2ZcqSx6.eval,2025-10-28T09:57:34.696Z,7QkPRbRPpRx8nJT2ZcqSx6
gpt-4-0613,0.011111111111111112,2023-06-13,OpenAI,United States of America,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.005362818328685681,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FhGVxg4EhSSJSCdH88m9soq.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/hGVxg4EhSSJSCdH88m9soq.eval,2025-10-23T16:09:26.069Z,hGVxg4EhSSJSCdH88m9soq
gpt-4-0314,0.005555555555555556,2023-03-14,OpenAI,United States of America,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.003883473863441843,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FcLTgetxaaTCZoA9vwY4SdL.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/cLTgetxaaTCZoA9vwY4SdL.eval,2025-10-22T16:49:17.118Z,cLTgetxaaTCZoA9vwY4SdL
claude-haiku-4-5-20251001_32K,0.7,2025-10-15,Anthropic,United States of America,,,0.06886678605926841,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FneZvEzLi6yeL9z5KjGef7H.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/neZvEzLi6yeL9z5KjGef7H.eval,2025-10-22T10:31:07.212Z,neZvEzLi6yeL9z5KjGef7H
claude-sonnet-4-5-20250929_32K,0.7777777777777778,2025-09-29,Anthropic,United States of America,,,0.06267511942419625,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FkqTyhmHyT9irTAWk3xBt2x.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/kqTyhmHyT9irTAWk3xBt2x.eval,2025-10-21T12:33:20.414Z,kqTyhmHyT9irTAWk3xBt2x
claude-haiku-4-5-20251001,0.35833333333333334,2025-10-15,Anthropic,United States of America,,,0.06238626014232183,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWf2kwcXu8YXNgzDCxChtvj.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Wf2kwcXu8YXNgzDCxChtvj.eval,2025-10-16T11:58:20.630Z,Wf2kwcXu8YXNgzDCxChtvj
qwen3-max-2025-09-23,0.7333333333333333,2025-09-24,Alibaba,China,1.512e+25,"6ND with:
36T tokens is taken from the qwen3 technical report
70B active params is based on it having >1T params, and the architectures of Qwen3-235B-A22B and Qwen3-Coder-480B-A35B",0.059139451700829515,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FULeuPtwp3ZoyuXQYuuQ82i.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ULeuPtwp3ZoyuXQYuuQ82i.eval,2025-10-06T14:32:01.826Z,ULeuPtwp3ZoyuXQYuuQ82i
claude-sonnet-4-5-20250929,0.35555555555555557,2025-09-29,Anthropic,United States of America,,,0.0721639236343101,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdEVgttSw8u6Tt2RsPWTc3C.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dEVgttSw8u6Tt2RsPWTc3C.eval,2025-09-29T18:39:45.629Z,dEVgttSw8u6Tt2RsPWTc3C
gpt-5-2025-08-07_medium,0.8722222222222222,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.03882118926889545,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FctsGxm6WkzCGEqiAbJR3kk.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ctsGxm6WkzCGEqiAbJR3kk.eval,2025-08-07T21:58:31.032Z,ctsGxm6WkzCGEqiAbJR3kk
gpt-5-mini-2025-08-07_medium,0.7833333333333333,2025-08-07,OpenAI,United States of America,,,0.04790431329560243,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F65DJZN2XzPw4RGpa4TCfA5.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/65DJZN2XzPw4RGpa4TCfA5.eval,2025-08-07T19:34:33.118Z,65DJZN2XzPw4RGpa4TCfA5
gpt-5-nano-2025-08-07_medium,0.7416666666666667,2025-08-07,OpenAI,United States of America,,,0.05231388366621641,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FXRFS8R4WV7q6RMv5keEAdM.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/XRFS8R4WV7q6RMv5keEAdM.eval,2025-08-07T19:34:30.191Z,XRFS8R4WV7q6RMv5keEAdM
Kimi-K2-Instruct,0.33055555555555555,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.06444640366231162,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FgS4PPEnXDQZGkdJmdMHcph.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/gS4PPEnXDQZGkdJmdMHcph.eval,2025-08-07T14:28:50.093Z,gS4PPEnXDQZGkdJmdMHcph
claude-opus-4-1-20250805_27K,0.6888888888888889,2025-08-05,Anthropic,United States of America,,,0.06979205927323111,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FUH9RY8MCAotDgFocHKA4j6.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/UH9RY8MCAotDgFocHKA4j6.eval,2025-08-05T16:53:30.846Z,UH9RY8MCAotDgFocHKA4j6
claude-opus-4-1-20250805_16K,0.6444444444444445,2025-08-05,Anthropic,United States of America,,,0.0721639236343101,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fkf9vy9sMjMBBswrX6NMzce.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/kf9vy9sMjMBBswrX6NMzce.eval,2025-08-05T16:51:45.842Z,kf9vy9sMjMBBswrX6NMzce
claude-opus-4-1-20250805,0.4,2025-08-05,Anthropic,United States of America,,,0.07385489458759964,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FRaiKuDSzUJYDVMxn4jfzNC.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/RaiKuDSzUJYDVMxn4jfzNC.eval,2025-08-05T16:45:26.245Z,RaiKuDSzUJYDVMxn4jfzNC
gpt-4o-mini-2024-07-18,0.06944444444444445,2024-07-18,OpenAI,United States of America,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",0.0273963714244758,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMPJkYPEMsQrowqtecK3mmz.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MPJkYPEMsQrowqtecK3mmz.eval,2025-07-30T15:47:06.669Z,MPJkYPEMsQrowqtecK3mmz
magistral-small-2506,0.3,2025-06-10,Mistral AI,France,,,0.05830627077871823,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FmJsWT3i6UHaxDmyy8vEDqq.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/mJsWT3i6UHaxDmyy8vEDqq.eval,2025-06-10T14:53:14.877Z,mJsWT3i6UHaxDmyy8vEDqq
DeepSeek-R1-0528,0.6638888888888889,2025-05-28,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.04911177955575991,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbBV5wCrrZtihrgxEsg9jPD.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/bBV5wCrrZtihrgxEsg9jPD.eval,2025-05-29T10:59:10.779Z,bBV5wCrrZtihrgxEsg9jPD
claude-opus-4-20250514_27K,0.6444444444444445,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.0721639236343101,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVrRccga4uRVuCjNp637MeY.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/VrRccga4uRVuCjNp637MeY.eval,2025-05-28T18:42:14.809Z,VrRccga4uRVuCjNp637MeY
claude-sonnet-4-20250514_59K,0.6888888888888889,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.06979205927323111,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fg8qaq3s2TJqfvrskHYmapM.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/g8qaq3s2TJqfvrskHYmapM.eval,2025-05-23T21:01:53.668Z,g8qaq3s2TJqfvrskHYmapM
claude-sonnet-4-20250514_32K,0.7111111111111111,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.06832943242540508,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FFZHzYjwAQexVykNQTnRtwT.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/FZHzYjwAQexVykNQTnRtwT.eval,2025-05-22T23:06:11.220Z,FZHzYjwAQexVykNQTnRtwT
claude-opus-4-20250514_16K,0.6,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.07385489458759964,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FXz2F28L5k8HFNfY7Lo8Ggw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Xz2F28L5k8HFNfY7Lo8Ggw.eval,2025-05-22T22:54:06.990Z,Xz2F28L5k8HFNfY7Lo8Ggw
claude-sonnet-4-20250514_16K,0.5333333333333333,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.07521014330903548,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FnDrnRwsADSyj8U6BD4in4r.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/nDrnRwsADSyj8U6BD4in4r.eval,2025-05-22T22:54:05.950Z,nDrnRwsADSyj8U6BD4in4r
claude-opus-4-20250514,0.4222222222222222,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.07446027270295805,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FaCbfcxGpLpPc9XkZNcuF4o.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/aCbfcxGpLpPc9XkZNcuF4o.eval,2025-05-22T17:26:48.979Z,aCbfcxGpLpPc9XkZNcuF4o
claude-sonnet-4-20250514,0.28888888888888886,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.06832943242540508,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FUa8sCkk3R3mBLWeSnnPG7a.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Ua8sCkk3R3mBLWeSnnPG7a.eval,2025-05-22T17:26:46.358Z,Ua8sCkk3R3mBLWeSnnPG7a
gemini-2.5-flash-preview-05-20,0.7083333333333334,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.05715807427254223,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9e62UjoHQbcZD2xjgfRkdK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9e62UjoHQbcZD2xjgfRkdK.eval,2025-05-20T21:22:01.376Z,9e62UjoHQbcZD2xjgfRkdK
mistral-medium-2505,0.32222222222222224,2025-05-07,Mistral AI,France,,"Benchmarks match with models like GPT-4o, Mistral's previous largest runs were ~1e25 FLOP scale, so plausibly they might have trained Medium with this much compute.",0.05965611798482264,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQWw65xeTn8x42SymMsVREN.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QWw65xeTn8x42SymMsVREN.eval,2025-05-07T17:32:21.604Z,QWw65xeTn8x42SymMsVREN
gemini-2.5-flash-preview-04-17,0.7305555555555555,2025-04-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.05532781604070319,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FKpZYG5nvdJ6QYweLz6dn5a.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/KpZYG5nvdJ6QYweLz6dn5a.eval,2025-04-22T10:03:18.492Z,KpZYG5nvdJ6QYweLz6dn5a
o4-mini-2025-04-16_high,0.8166666666666667,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.046939139620180814,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTVsxkJ8ZyfMVmVx88Qq3KN.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/TVsxkJ8ZyfMVmVx88Qq3KN.eval,2025-04-16T18:17:08.217Z,TVsxkJ8ZyfMVmVx88Qq3KN
o3-2025-04-16_high,0.8388888888888889,2025-04-16,OpenAI,United States of America,,,0.04381258550587166,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6anLU9CfBBNQkWyJG2GDCa.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6anLU9CfBBNQkWyJG2GDCa.eval,2025-04-16T18:17:05.608Z,6anLU9CfBBNQkWyJG2GDCa
gpt-4.1-nano-2025-04-14,0.28888888888888886,2025-04-14,OpenAI,United States of America,,,0.05970019560922746,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F3qLHwibDhgtWEbKGwxUYVa.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/3qLHwibDhgtWEbKGwxUYVa.eval,2025-04-14T22:50:00.937Z,3qLHwibDhgtWEbKGwxUYVa
gpt-4.1-mini-2025-04-14,0.44722222222222224,2025-04-14,OpenAI,United States of America,,,0.06148017573965347,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FcXbbdQDW2jeYassmRuZYpU.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/cXbbdQDW2jeYassmRuZYpU.eval,2025-04-14T22:49:56.953Z,cXbbdQDW2jeYassmRuZYpU
gpt-4.1-2025-04-14,0.38333333333333336,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.06324056203789373,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVAQtARXUPTpdxGKMekUSbX.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/VAQtARXUPTpdxGKMekUSbX.eval,2025-04-14T18:34:29.241Z,VAQtARXUPTpdxGKMekUSbX
grok-3-mini-beta_low,0.6222222222222222,2025-04-09,xAI,United States of America,,,0.07309112127323451,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTTmhagb2ovbi7GJnDxEfrj.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/TTmhagb2ovbi7GJnDxEfrj.eval,2025-04-10T20:30:32.715Z,TTmhagb2ovbi7GJnDxEfrj
grok-3-mini-beta_high,0.7777777777777778,2025-04-09,xAI,United States of America,,,0.06267511942419625,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfPtize44vDPcDAzMFuAurw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fPtize44vDPcDAzMFuAurw.eval,2025-04-10T18:02:28.291Z,fPtize44vDPcDAzMFuAurw
grok-3-beta,0.5555555555555556,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.07491109582924914,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FDKTzh9kSoipZGkDsC4NGf3.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/DKTzh9kSoipZGkDsC4NGf3.eval,2025-04-10T17:55:19.398Z,DKTzh9kSoipZGkDsC4NGf3
Llama-4-Maverick-17B-128E-Instruct-FP8,0.20555555555555555,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.04948528559541873,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FGJEpPThqUxXJBSbvkuXVnw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/GJEpPThqUxXJBSbvkuXVnw.eval,2025-04-08T09:58:55.652Z,GJEpPThqUxXJBSbvkuXVnw
Llama-4-Scout-17B-16E-Instruct,0.07777777777777778,2025-04-05,Meta AI,United States of America,4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",0.030069597533844094,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FT3X5DeC47iqCaoRYXu5k8L.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/T3X5DeC47iqCaoRYXu5k8L.eval,2025-04-08T09:43:28.090Z,T3X5DeC47iqCaoRYXu5k8L
qwen-turbo-2024-11-01,0.06111111111111111,2024-11-01,Alibaba,China,,,0.022340279838327228,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FG5gxbTH33S7LXA29o5fgmv.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/G5gxbTH33S7LXA29o5fgmv.eval,2025-04-07T19:18:53.147Z,G5gxbTH33S7LXA29o5fgmv
qwen-plus-2025-01-25,0.17777777777777778,2025-01-25,Alibaba,China,,,0.04360797453817489,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FAULNJG2C75JBP6uaiAgj3P.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/AULNJG2C75JBP6uaiAgj3P.eval,2025-04-07T19:09:43.970Z,AULNJG2C75JBP6uaiAgj3P
qwen-max-2025-01-25,0.16111111111111112,2025-01-25,Alibaba,China,,,0.04102194808229969,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPZt4Yb9bpXVPDoPi4uTxwK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PZt4Yb9bpXVPDoPi4uTxwK.eval,2025-04-01T17:11:08.937Z,PZt4Yb9bpXVPDoPi4uTxwK
Hermes-2-Theta-Llama-3-70B,0.025,2024-06-20,"Nous Research,Arcee AI",United States of America,,,0.012309149097933271,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F63MnE4e4AXvbQ585MaWTFv.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/63MnE4e4AXvbQ585MaWTFv.eval,2025-04-01T15:03:10.644Z,63MnE4e4AXvbQ585MaWTFv
DeepSeek-V3-0324,0.37777777777777777,2025-03-24,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",0.062372203800719374,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVpsMexBEhXkPYJBe7v6Fk9.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/VpsMexBEhXkPYJBe7v6Fk9.eval,2025-04-01T14:33:01.704Z,VpsMexBEhXkPYJBe7v6Fk9
mistral-small-2501,0.05277777777777778,2025-01-25,Mistral AI,France,1.152e+24,6ND = 6*8T tokens * 24B parameters = 1.152e+24 FLOP,0.02589584594604903,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fb4vEwq8UVmGmAKh4eqp6G3.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/b4vEwq8UVmGmAKh4eqp6G3.eval,2025-03-18T16:39:25.325Z,b4vEwq8UVmGmAKh4eqp6G3
mistral-small-2503,0.058333333333333334,2025-03-17,Mistral AI,France,,At least 1.152e+24 FLOP (base model Mistral Small 3 training compute),0.0226830377599695,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJxqtHWrwCtYoQYBB3jVqB9.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/JxqtHWrwCtYoQYBB3jVqB9.eval,2025-03-18T16:08:02.049Z,JxqtHWrwCtYoQYBB3jVqB9
claude-3-7-sonnet-20250219_64K,0.5777777777777777,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.07446027270295805,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQW3FUBn5qtAFEX4oQuzEAV.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QW3FUBn5qtAFEX4oQuzEAV.eval,2025-03-13T21:20:00.444Z,QW3FUBn5qtAFEX4oQuzEAV
gemma-3-27b-it,0.19722222222222222,2025-03-12,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",2.268e+24,6ND =  6 * 27B parameters * 14T training tokens = 2.268 × 10^24 FLOP,0.054252392666026966,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FA3ZGeCNhHMp55ZjtRtCGVB.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/A3ZGeCNhHMp55ZjtRtCGVB.eval,2025-03-13T12:46:57.184Z,A3ZGeCNhHMp55ZjtRtCGVB
claude-3-7-sonnet-20250219_32K,0.5333333333333333,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.07521014330903548,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdSuGgEbnQErengjEboPn62.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dSuGgEbnQErengjEboPn62.eval,2025-03-12T19:36:44.645Z,dSuGgEbnQErengjEboPn62
Llama-3.1-Tulu-3-70B-DPO,0.044444444444444446,2024-11-21,"Allen Institute for AI,University of Washington",United States of America,,,0.01827278031830863,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FFGcofc566CradiNmHUGqVB.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/FGcofc566CradiNmHUGqVB.eval,2025-03-07T23:13:12.814Z,FGcofc566CradiNmHUGqVB
gemma-2-27b-it,0.013888888888888888,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.00907862525871088,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FjD6rJdvfNPErKEz9qkpssj.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/jD6rJdvfNPErKEz9qkpssj.eval,2025-03-07T23:11:28.104Z,jD6rJdvfNPErKEz9qkpssj
gemma-2-9b-it,0.005555555555555556,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP",0.003883473863441843,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWNE7cveVN8rsi7DmsFsFVy.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/WNE7cveVN8rsi7DmsFsFVy.eval,2025-03-07T23:11:27.610Z,WNE7cveVN8rsi7DmsFsFVy
DeepSeek-R1-Distill-Llama-70B,0.5138888888888888,2025-01-20,DeepSeek,China,,"base model compute: 6.8649768e+24 FLOP
fine tune compute: 2.016e+22 FLOP",0.060540263104731575,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F89zcekqEYnE8zhwG4pR5Nu.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/89zcekqEYnE8zhwG4pR5Nu.eval,2025-03-07T23:03:59.002Z,89zcekqEYnE8zhwG4pR5Nu
claude-2.1,0.019444444444444445,2023-11-21,Anthropic,United States of America,,,0.010477515876628662,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F2vg9qw9pd8zSyRMu9YNiVm.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/2vg9qw9pd8zSyRMu9YNiVm.eval,2025-03-07T22:18:53.415Z,2vg9qw9pd8zSyRMu9YNiVm
claude-2.0,0.025,2023-07-11,Anthropic,United States of America,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,0.015179897658722631,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F7LLTmqs4LC5GLVfJEqJWSC.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/7LLTmqs4LC5GLVfJEqJWSC.eval,2025-03-07T22:18:52.824Z,7LLTmqs4LC5GLVfJEqJWSC
gemini-2.0-flash-thinking-exp-01-21,0.5777777777777777,2025-01-21,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.07446027270295805,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FcXz2gsCmGYSvQ6Ey4LDiN7.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/cXz2gsCmGYSvQ6Ey4LDiN7.eval,2025-03-07T22:18:52.419Z,cXz2gsCmGYSvQ6Ey4LDiN7
o1-preview-2024-09-12,0.3111111111111111,2024-09-12,OpenAI,United States of America,,,0.0697920592732311,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FCprjaj9JQ9MY5NCXqeQR5H.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Cprjaj9JQ9MY5NCXqeQR5H.eval,2025-03-07T22:14:04.083Z,Cprjaj9JQ9MY5NCXqeQR5H
o1-mini-2024-09-12_high,0.46944444444444444,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.06456873681038247,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9zhPZLvNeUPQ7JBSpTwKPp.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9zhPZLvNeUPQ7JBSpTwKPp.eval,2025-03-06T21:47:29.432Z,9zhPZLvNeUPQ7JBSpTwKPp
o1-mini-2024-09-12_medium,0.44722222222222224,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.06312676751311953,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQUTfpiKjphedrKg8H6fGzF.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QUTfpiKjphedrKg8H6fGzF.eval,2025-03-06T21:47:28.091Z,QUTfpiKjphedrKg8H6fGzF
gpt-4.5-preview-2025-02-27,0.37777777777777777,2025-02-27,OpenAI,United States of America,2.1000001e+26,"Analysis of GPT-4.5's training cluster, in combination with 30% utilization relative to H100 16-bit output (or a plausible range of 20 to 50% utilization, given the possibility of FP8 training), yields a median estimate of ~2e25 FLOP. See notebook below for details.

https://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw 

This is consistent with OpenAI's statement that GPT-4.5 was a “new order of magnitude in compute” compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. 

In the ""Pretraining GPT-4.5"" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",0.07309112127323451,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9DCJMt7YsTMDenTdX4Db6V.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9DCJMt7YsTMDenTdX4Db6V.eval,2025-02-28T09:17:11.571Z,9DCJMt7YsTMDenTdX4Db6V
o3-mini-2025-01-31_high,0.7694444444444445,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.05239762076828802,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMjx6vYbgAb2DHXK8xcupX8.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Mjx6vYbgAb2DHXK8xcupX8.eval,2025-02-27T14:29:45.288Z,Mjx6vYbgAb2DHXK8xcupX8
gpt-4-turbo-2024-04-09,0.06666666666666667,2024-04-09,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.024034381938205007,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMZABVPEY8VCPAyCZ4iir68.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MZABVPEY8VCPAyCZ4iir68.eval,2025-02-27T14:29:44.767Z,MZABVPEY8VCPAyCZ4iir68
o1-2024-12-17_medium,0.7333333333333333,2024-12-17,OpenAI,United States of America,,,0.06666666666666667,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FKvcqycaJpCQMBigiEosyzC.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/KvcqycaJpCQMBigiEosyzC.eval,2025-02-27T14:16:13.444Z,KvcqycaJpCQMBigiEosyzC
claude-3-7-sonnet-20250219_16K,0.4666666666666667,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.07521014330903548,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F64LAJymM6eXDkRZb92Mdg8.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/64LAJymM6eXDkRZb92Mdg8.eval,2025-02-26T16:56:46.931Z,64LAJymM6eXDkRZb92Mdg8
DeepSeek-R1,0.5333333333333333,2025-01-20,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.07521014330903548,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbBY5VZzpDkmTBdSSvu5Ups.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/bBY5VZzpDkmTBdSSvu5Ups.eval,2025-02-26T16:56:45.594Z,bBY5VZzpDkmTBdSSvu5Ups
Meta-Llama-3-70B-Instruct,0.043055555555555555,2024-04-18,Meta AI,United States of America,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.02192429710598427,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6RD3A6J432TZzxrKYRxph2.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6RD3A6J432TZzxrKYRxph2.eval,2025-02-25T14:38:33.050Z,6RD3A6J432TZzxrKYRxph2
gemini-1.5-pro-001,0.06805555555555555,2024-05-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.01893034295514878,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F3S6fjGAMKWKK3zcdxkmh9H.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/3S6fjGAMKWKK3zcdxkmh9H.eval,2025-02-25T14:38:32.379Z,3S6fjGAMKWKK3zcdxkmh9H
grok-2-1212,0.11527777777777778,2024-12-12,xAI,United States of America,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,0.03382424700930314,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPpSVDb67Ucbxpcd8JQv5ii.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PpSVDb67Ucbxpcd8JQv5ii.eval,2025-02-25T14:38:30.971Z,PpSVDb67Ucbxpcd8JQv5ii
Llama-3.3-70B-Instruct,0.05138888888888889,2024-12-06,Meta AI,United States of America,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",0.023611111111111107,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZjsXQpZ7SsejXDo52aetZZ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZjsXQpZ7SsejXDo52aetZZ.eval,2025-02-25T14:05:10.812Z,ZjsXQpZ7SsejXDo52aetZZ
Llama-3.2-90B-Vision-Instruct,0.02638888888888889,2024-09-24,Meta AI,United States of America,,,0.00941990275434065,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FLHRL6nQJZXGJmYppR7kbGw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/LHRL6nQJZXGJmYppR7kbGw.eval,2025-02-25T14:05:10.587Z,LHRL6nQJZXGJmYppR7kbGw
Llama-2-70b-chat-hf,0.0,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.0,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNLk2oE4rzSFihPzF95peHr.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NLk2oE4rzSFihPzF95peHr.eval,2025-02-25T14:05:09.775Z,NLk2oE4rzSFihPzF95peHr
claude-3-5-sonnet-20240620,0.06527777777777778,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.022293131815660016,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F2xNPhKLsiWLHQBLLDGU4VJ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/2xNPhKLsiWLHQBLLDGU4VJ.eval,2025-02-25T14:05:09.497Z,2xNPhKLsiWLHQBLLDGU4VJ
gemini-1.5-pro-002,0.23055555555555557,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.05186782011216038,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfhCdyX9uUS4WPtjLiVdBdK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fhCdyX9uUS4WPtjLiVdBdK.eval,2025-02-25T14:05:08.808Z,fhCdyX9uUS4WPtjLiVdBdK
gpt-4o-2024-08-06,0.06388888888888888,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.025630380139261548,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FmBUKqiVQsqXd3qmdtKP5UP.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/mBUKqiVQsqXd3qmdtKP5UP.eval,2025-02-25T14:05:08.205Z,mBUKqiVQsqXd3qmdtKP5UP
gemini-1.5-flash-002,0.1625,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.04111375571619442,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQdGZ2m52PWt3EkLPbcoRwz.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QdGZ2m52PWt3EkLPbcoRwz.eval,2025-02-25T14:05:07.974Z,QdGZ2m52PWt3EkLPbcoRwz
qwen2.5-32b-instruct,0.07361111111111111,2024-09-17,Alibaba,China,3.51e+24,6 FLOP / parameter / token * 32.5B parameters * 18 trillion tokens = 3.51 × 10^24 FLOP,0.021420595022864277,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNRCNnCCL5aQKsZf6cKC69Z.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NRCNnCCL5aQKsZf6cKC69Z.eval,2025-02-25T14:05:07.648Z,NRCNnCCL5aQKsZf6cKC69Z
mistral-large-2402,0.019444444444444445,2024-02-26,Mistral AI,France,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.00948945749721607,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZVHvk3GTPUNTgjc5w3vkhW.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZVHvk3GTPUNTgjc5w3vkhW.eval,2025-02-25T14:05:07.542Z,ZVHvk3GTPUNTgjc5w3vkhW
gemini-1.5-flash-8b-001,0.04583333333333333,2024-10-03,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.020100756305184247,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FjGY5Ee5P9dyStA4Fw5Pa6q.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/jGY5Ee5P9dyStA4Fw5Pa6q.eval,2025-02-25T14:05:06.918Z,jGY5Ee5P9dyStA4Fw5Pa6q
mistral-large-2411,0.07777777777777778,2024-11-18,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",0.024662778950157508,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbeRGgJXTJggTi8PPEZNoZ5.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/beRGgJXTJggTi8PPEZNoZ5.eval,2025-02-25T14:05:06.293Z,beRGgJXTJggTi8PPEZNoZ5
claude-3-sonnet-20240229,0.025,2024-02-29,Anthropic,United States of America,,,0.01950297827553939,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FAeRCEiG87bMFpxbxbmjCyn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/AeRCEiG87bMFpxbxbmjCyn.eval,2025-02-25T14:05:05.872Z,AeRCEiG87bMFpxbxbmjCyn
DeepSeek-V3,0.15833333333333333,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",0.0433650088193118,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F7saSEhfVvXuz4yTcGy5u2y.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/7saSEhfVvXuz4yTcGy5u2y.eval,2025-02-25T14:05:05.855Z,7saSEhfVvXuz4yTcGy5u2y
gemini-1.0-pro-001,0.011111111111111112,2024-02-15,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"Training compute estimated to be 1.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing 

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",0.006054026310473157,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNpJvYQDcmwbGgS5MzZUPUg.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NpJvYQDcmwbGgS5MzZUPUg.eval,2025-02-25T14:05:05.526Z,NpJvYQDcmwbGgS5MzZUPUg
Llama-3.1-8B-Instruct,0.025,2024-07-23,Meta AI,United States of America,1.224e+24,"Huggingface page says 3.1-8B used 1.46M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 8B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 8B = 7.2e23 FLOPs

Hardware:
1.46M * 9.9e14 * 3600 * 0.4 = 2.08e24 FLOPs

Geometric mean: sqrt(7.2e23 * 2.08e24) = 1.224e24

Note that Llama 3-8B also said it used 15T tokens, but only 1.3M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.",0.02233242874694496,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F6SY8iSf5uKn7SrCzzrabPb.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/6SY8iSf5uKn7SrCzzrabPb.eval,2025-02-25T14:05:05.419Z,6SY8iSf5uKn7SrCzzrabPb
Llama-3.1-405B-Instruct,0.09722222222222222,2024-07-23,Meta AI,United States of America,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.03218644609162339,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F2fQJP3brRApfBdQq7Zy3bm.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/2fQJP3brRApfBdQq7Zy3bm.eval,2025-02-25T14:05:05.407Z,2fQJP3brRApfBdQq7Zy3bm
claude-3-7-sonnet-20250219,0.21944444444444444,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.04815168903711295,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FN2NcmgmHyGJ9HNmQe5yvcS.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/N2NcmgmHyGJ9HNmQe5yvcS.eval,2025-02-25T14:05:04.923Z,N2NcmgmHyGJ9HNmQe5yvcS
claude-3-5-sonnet-20241022,0.08472222222222223,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.026363956855902478,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FgeVVaRWxuprKUqnW2VzvCa.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/geVVaRWxuprKUqnW2VzvCa.eval,2025-02-25T14:05:03.625Z,geVVaRWxuprKUqnW2VzvCa
qwen2.5-72b-instruct,0.08055555555555556,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",0.026098213018967212,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F67YbVQfJh7sd77KH3zFgTn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/67YbVQfJh7sd77KH3zFgTn.eval,2025-02-25T14:05:03.625Z,67YbVQfJh7sd77KH3zFgTn
gpt-4o-2024-05-13,0.0625,2024-05-13,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.023249986423369555,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9cCn5aW522cTRxJSE69crm.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9cCn5aW522cTRxJSE69crm.eval,2025-02-25T14:05:03.623Z,9cCn5aW522cTRxJSE69crm
Llama-3.1-70B-Instruct,0.03611111111111111,2024-07-23,Meta AI,United States of America,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",0.01691212350465157,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FX2tQXGELXtEbc2VX6gARd7.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/X2tQXGELXtEbc2VX6gARd7.eval,2025-02-25T14:05:03.270Z,X2tQXGELXtEbc2VX6gARd7
claude-3-5-haiku-20241022,0.043055555555555555,2024-10-22,Anthropic,United States of America,,,0.016237638827815495,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FcqhrR5KnwpcuAVpHkZGQFd.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/cqhrR5KnwpcuAVpHkZGQFd.eval,2025-02-25T14:05:03.257Z,cqhrR5KnwpcuAVpHkZGQFd
gemini-1.5-flash-001,0.03888888888888889,2024-05-23,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.014363726487520755,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FU9GVBhYjRzU55YVjna5mMT.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/U9GVBhYjRzU55YVjna5mMT.eval,2025-02-25T14:05:02.246Z,U9GVBhYjRzU55YVjna5mMT
claude-3-haiku-20240307,0.018055555555555554,2024-03-07,Anthropic,United States of America,,,0.008098544298396251,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FThMd6DNDTtyWxhPcb9Kq4y.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ThMd6DNDTtyWxhPcb9Kq4y.eval,2025-02-25T14:05:02.017Z,ThMd6DNDTtyWxhPcb9Kq4y
Meta-Llama-3-8B-Instruct,0.008333333333333333,2024-04-18,Meta AI,United States of America,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2×10^23 FLOP

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872×10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",0.005103103630798287,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F7P5HLaaD6DBjNYFx7o6Jma.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/7P5HLaaD6DBjNYFx7o6Jma.eval,2025-02-25T14:05:01.786Z,7P5HLaaD6DBjNYFx7o6Jma
claude-3-opus-20240229,0.04722222222222222,2024-02-29,Anthropic,United States of America,,Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.019579258477123922,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FmYb9MGQD39RS8VdDyMqujr.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/mYb9MGQD39RS8VdDyMqujr.eval,2025-02-25T14:05:00.461Z,mYb9MGQD39RS8VdDyMqujr
o3-mini-2025-01-31_medium,0.6388888888888888,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.060377105937901555,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FajytUBY9bDPNNxuBfi5UBS.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ajytUBY9bDPNNxuBfi5UBS.eval,2025-02-25T13:33:25.508Z,ajytUBY9bDPNNxuBfi5UBS
phi-4,0.1375,2024-12-12,Microsoft Research,United States of America,9.3202015e+23,"6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP

989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP

geometric mean
sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23",0.03698083382925635,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F5mmrBSQb7hVSJ9VYtNm46i.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/5mmrBSQb7hVSJ9VYtNm46i.eval,2025-02-25T13:19:42.221Z,5mmrBSQb7hVSJ9VYtNm46i
gpt-4o-2024-11-20,0.0625,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.024165360466267546,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbuNVL2TWcdVgxAcMfVE8xJ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/buNVL2TWcdVgxAcMfVE8xJ.eval,2025-02-25T13:10:02.200Z,buNVL2TWcdVgxAcMfVE8xJ
gemini-2.0-flash-001,0.3111111111111111,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",0.06311218144454454,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZnGSRrKbiEnuju4kk4sgVb.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZnGSRrKbiEnuju4kk4sgVb.eval,2025-02-25T12:25:50.002Z,ZnGSRrKbiEnuju4kk4sgVb
mistral-large-2407,0.08472222222222223,2024-07-24,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",0.024742642797709363,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbHLfPfZj8HgD9Gef75NtrM.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/bHLfPfZj8HgD9Gef75NtrM.eval,2025-02-25T11:59:42.758Z,bHLfPfZj8HgD9Gef75NtrM
grok-4-0709,0.84,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.05,,,,cvTPRDCM38zSTn9Y3MUb9d
