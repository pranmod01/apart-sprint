Model version,EM,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Name,Shots,Source,Source link,Notes,id
amazon.nova-lite-v1:0,0.77,2024-12-03,Amazon,United States of America,,,Amazon Nova Lite,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,reccNX56mRCIhx9ok
amazon.nova-micro-v1:0,0.708,2024-12-03,Amazon,United States of America,,,Amazon Nova Micro,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recMu6jniW4Q1aHhj
amazon.nova-pro-v1:0,0.82,2024-12-03,Amazon,United States of America,6.000010000000001e+24,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",Amazon Nova Pro,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recIRlQ5EAWn5dxqD
,0.652,,,,,,AquilaChat2 34B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recyryG18K2x30duJ
,0.667,,,,,,AquilaChat2 34B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,rechbRaRakeh7m35k
,0.677,,,,,,Arctic Instruct,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,receF4qeBwFbaGr6r
Baichuan-7B,0.423,2023-06-01,Baichuan,China,5.04e+22,7b parameters * 1.2t tokens * 6 FLOP / parameter / token = 5.04e22 FLOP,Baichuan 1-7B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,rechuNXbQCiXvI5bb
Baichuan-13B-Base,0.516,2023-07-11,Baichuan,China,9.36e+22,13b parameters * 1.2t tokens * 6 FLOP / parameter / token = 9.36e22 FLOP,Baichuan 1-13B-Base,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recCxbVxrA9fWanQp
Baichuan-2-7B-Base,0.5416,2023-09-20,Baichuan,China,1.092e+23,"7b * 2.6t * 6 = 1.092e23
Also mentions 1,024 NVIDIA A800 GPUs at 180 TFLOPS per GPU",Baichuan 2-7B-Base,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recL0KmTLb9YnVqAX
Baichuan-2-13B-Base,0.5917,2023-09-06,Baichuan,China,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",Baichuan 2-13B-Base,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,rec2lAAR2TzfQNZIn
Baichuan-2-13B-Base,0.592,2023-09-06,Baichuan,China,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",Baichuan-2 13B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,recg7CzyJtjHNlw9z
Baichuan2-13B-Chat,0.551,2023-09-06,,,,,Baichuan2-Chat 13B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recic0LQTjjN2UHF2
Baichuan2-13B-Chat,0.501,2023-09-06,,,,,Baichuan2-Chat 13B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recLNxS9WuckNhW0w
Cerebras-GPT-13B,0.262,2023-03-20,Cerebras Systems,United States of America,2.3e+22,"2.3e22, per table 2",Cerebras-GPT-13B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recjibIVUpfsNYJiW
Cerebras-GPT-13B,0.246,2023-03-20,Cerebras Systems,United States of America,2.3e+22,"2.3e22, per table 2",Cerebras-GPT-13B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recxEX8BTSBULeO3F
,0.4786,,,,,,ChatGLM 2-6B (base)*,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,rec5vUXZIJlzUijla
chatglm2-6b,0.479,2023-06-24,,,,,ChatGLM2 6B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recJMYoLpkfhVACXE
Chinchilla (70B),0.675,2022-03-29,DeepMind,United Kingdom of Great Britain and Northern Ireland,5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3",Chinchilla 70B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recblkzYf2HuW8IIY
,0.439,,,,,,Chinese-Alpaca-Plus-13B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recwJ71gOFSsbxqzt
claude-1.3,0.77,2023-04-18,Anthropic,United States of America,,,Claude 1.3,5,Model Card and Evaluations for Claude Models,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf,5-shot CoT,recMiCaKbG8RgAL4z
claude-2.0,0.785,2023-07-11,Anthropic,United States of America,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,Claude 2,5,Model Card and Evaluations for Claude Models,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf,5-shot CoT,recrV38I6BcgNumAG
claude-2.1,0.735,2023-11-21,Anthropic,United States of America,,,Claude 2.1,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec3x29hlWlm49CQP
claude-3-haiku-20240307,0.738,2024-03-07,Anthropic,United States of America,,,Claude 3 Haiku (20240307),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recFv7M6aEx8RaC1z
claude-3-opus-20240229,0.846,2024-02-29,Anthropic,United States of America,,Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Claude 3 Opus (20240229),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,receM7r5loSsHAQw3
claude-3-sonnet-20240229,0.759,2024-02-29,Anthropic,United States of America,,,Claude 3 Sonnet (20240229),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recE3DF6ClfScS431
claude-3-5-haiku-20241022,0.743,2024-10-22,Anthropic,United States of America,,,Claude 3.5 Haiku (20241022),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recYAn1b7tiMc4vIW
claude-3-5-sonnet-20240620,0.865,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Claude 3.5 Sonnet (20240620),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rectxjtMFzEfdNd3Q
claude-3-5-sonnet-20241022,0.873,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Claude 3.5 Sonnet (20241022),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recoTsRA8ILsdFhsM
claude-instant-1.1,0.734,,Anthropic,United States of America,,,Claude instant,5,Model Card and Evaluations for Claude Models,https://www-cdn.anthropic.com/5c49cc247484cecf107c699baf29250302e5da70/claude-2-model-card.pdf,5-shot CoT,recvrlr1UHnsu2xMG
claude-instant-1.1,0.734,,Anthropic,United States of America,,,Claude Instant 1.1,5,Releasing Claude Instant 1.2,https://www.anthropic.com/news/releasing-claude-instant-1-2,,rec8sbiLCZ655gCFv
claude-instant-1.2,0.688,2023-08-09,Anthropic,United States of America,,,Claude Instant 1.2,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec5zO5kkq0fxUAov
claude-instant-1.2,0.732,2023-08-09,Anthropic,United States of America,,,Claude Instant 1.2,5,Releasing Claude Instant 1.2,https://www.anthropic.com/news/releasing-claude-instant-1-2,,rece9mgHXM0QhmiDh
CodeQwen1.5-7B,0.405,2024-04-15,,,,,CodeQwen1.5-7B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recBAF9M2T4QfIHAR
c4ai-command-r-08-2024,0.652,2024-08-30,,,,,Command R,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recalVfY5uZ17cXy1
c4ai-command-r-plus-08-2024,0.694,2024-08-30,"Cohere,Cohere for AI",Canada,,,Command R Plus,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rectFxVzTinEXE5UP
,0.741,,,,,,DBRX Instruct,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recNBl9C5PMTK1z60
,0.725,,,,,,DeepSeek LLM Chat (67B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recSzBzlqINbmZstc
DeepSeek-V3,0.872,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",DeepSeek v3,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recZAYS2AtKUwDAot
DeepSeek-V2,0.784,2024-05-07,DeepSeek,China,1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,DeepSeek-V2 Base,5,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437,,recUe3PPejBJft24I
DeepSeek-V3,0.871,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",DeepSeek-V3 Base,5,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437,,reciHYY3MaHRp7o7O
dolly-v2-12b,0.262,2023-04-11,Databricks,United States of America,,,Dolly-v2-12B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,rec97rlUByGZ2zt8v
dolly-v2-12b,0.254,2023-04-11,Databricks,United States of America,,,Dolly-v2-12B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recWcPAtUKdfuuDcu
deepseek-coder-1.3b-base,0.258,2023-11-02,"DeepSeek,Peking University",China,1.56e+22,2T tokens * 1.3B parameters * 6 FLOP / parameter / token = 15.6 * 10^21 = 1.56 * 10^22 FLOP,DS-Coder-1.3B-Base,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,reca7wIho24nf1lbz
deepseek-coder-6.7b-base,0.364,2023-11-02,"DeepSeek,Peking University",China,8.04e+22,2T tokens * 6.7B parameters * 6 FLOP / parameter / token = 8.04*10^22 FLOP,DS-Coder-6.7B-Base,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,rec9GGQ2SbDsozdsi
deepseek-coder-33b-base,0.394,2023-11-02,"DeepSeek,Peking University",China,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",DS-Coder-33B-Base,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recWXbEo1J1SGz2Pc
DeepSeek-Coder-V2-Lite-Base,0.605,2024-06-13,,,,,DS-Coder-V2-Lite-Base,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recvb9WHxy2ct1iLh
falcon-7b,0.35,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon 7B,5,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885,,recZxu6DjE1qtZ1S3
falcon-7b,0.262,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon 7B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recWf7mxbT8We02eo
falcon-7b,0.262,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon 7B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recCqMLHKTiaKiBA0
falcon-40b,0.569,2023-03-15,Technology Innovation Institute,United Arab Emirates,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",Falcon 40B,5,Falcon2-11B Technical ReportFalcon2-11B Technical Report,http://arxiv.org/abs/2407.14885,,recz02AVGuolhygEY
falcon-40b,0.554,2023-03-15,Technology Innovation Institute,United Arab Emirates,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",Falcon 40B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recRH4MTY2hMvXK0H
falcon-40b,0.554,2023-03-15,Technology Innovation Institute,United Arab Emirates,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",Falcon 40B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recsdVJaTPLaSRk6Q
falcon-7b,0.2603,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon-7B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,reci0A7hEp1NCjbaL
falcon-7b,0.269,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon-7B,2,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,,recqBkEih0PUjiHHs
falcon-7b,0.269,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon-7B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recT6jUES7TrDPOPE
falcon-7b,0.239,2023-04-24,Technology Innovation Institute,United Arab Emirates,6.3e+22,"6ND = 6 * 7B * 1.5T = 6.3e22
""Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.""

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

730 petaflop-days * 1e15 * 24 * 3600 = 6.3072e+22 FLOPs",Falcon-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recoziexUycUxDIif
falcon-180B,0.706,2023-09-06,Technology Innovation Institute,United Arab Emirates,3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",Falcon-180B,5,The Falcon Series of Open Language Models,http://arxiv.org/abs/2311.16867,,rec47dDuJMqw0UtNo
,0.259,,,,,,Falcon-rw-1.3B,2,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,,recbWgy7ZGi6kS6J4
falcon-11b,0.584,2024-05-09,Technology Innovation Institute,United Arab Emirates,3.6e+23,"trained on 5.5T tokens

6 * 11B * 5.5T = 3.6e23",Falcon2-11B,5,Falcon2-11B Technical Report,http://arxiv.org/abs/2407.14885,,reca5QOZscaMohhwc
gemini-1.0-pro-001,0.7,2024-02-15,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"Training compute estimated to be 1.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing 

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",Gemini 1.0 Pro (001),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recp3h3zrPXM6K5E6
gemini-1.5-flash-001,0.779,2024-05-23,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",Gemini 1.5 Flash (001),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recya8zx93N6YFh4J
gemini-1.5-flash-002,0.739,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",Gemini 1.5 Flash (002),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recIlo0YrdpQ2gmEe
gemini-1.5-flash-0514,0.778,2024-05-14,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",Gemini 1.5 Flash (0514 preview),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recvHsglkezP7vlhw
gemini-1.5-pro-001,0.827,2024-05-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Gemini 1.5 Pro (001),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recWbQsttPCe4RW4T
gemini-1.5-pro-002,0.869,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Gemini 1.5 Pro (002),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recosbed3WAJ7x0aX
,0.81,,,,,,Gemini 1.5 Pro (0409 preview),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recIoOlvBkY3ltQHG
gemini-2.0-flash-exp,0.797,2024-12-11,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google‚Äôs most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",Gemini 2.0 Flash (Experimental),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recGFT80UG5ZvEXom
gemma-7b,0.661,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be ‚àº 131 ùë°ùê∂ùëÇ2ùëíùëû. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Gemma (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec9XaJ2TmQNbln0Q
gemma-2-9b-it,0.721,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP",Gemma 2 Instruct (9B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,receSMvaZsWAJGCwz
gemma-2-27b-it,0.757,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",Gemma 2 Instruct (27B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recvSF9bkUFYJFbvW
gemma-2b,0.423,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",4.5115822e+22,"6ND = 6*2506434560.00 parameters * 3*10^12 training tokens = 4.5115822e+22

(assuming 1 epoch)",Gemma 2B,5,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295,"5-shot top-1
",recFxjYqOdtTFaKbb
gemma-7b,0.643,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be ‚àº 131 ùë°ùê∂ùëÇ2ùëíùëû. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Gemma 7B,5,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295,"5-shot top-1
",recVGRLvxWiFAKPkj
gemma-7b,0.643,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be ‚àº 131 ùë°ùê∂ùëÇ2ùëíùëû. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Gemma 7B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,recZ4bWHun8k676qr
gemma-7b,0.636,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be ‚àº 131 ùë°ùê∂ùëÇ2ùëíùëû. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Gemma 7b               ,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recw9AShxMdslSHLM
Gopher (280B),0.6,2021-12-08,DeepMind,United Kingdom of Great Britain and Northern Ireland,6.31e+23,"Table A26
6.31E+08 Train PFLOPs",Gopher,5,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",http://arxiv.org/abs/2112.11446,,recNkiYry1lAvuSwK
Gopher (280B),0.6,2021-12-08,DeepMind,United Kingdom of Great Britain and Northern Ireland,6.31e+23,"Table A26
6.31E+08 Train PFLOPs",Gopher 280B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,rec5n8Sdwq7WFYI2x
gpt-4o-mini-2024-07-18,0.818,2024-07-18,OpenAI,United States of America,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",GPT 4o-mini,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"Specifically, we used log-likelihood evaluations for MMLU (5-shot)",rec9BQTN7H8M50fyc
text-davinci-001,0.439,2022-01-27,OpenAI,United States of America,3.19181e+23,"""training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)""

60/3640 = +1.65% to base model compute

base model was reported 3.14e+23 FLOP

3.14e+23 * 1.0165 = 319181000000000000000000",GPT-3,5,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",http://arxiv.org/abs/2112.11446,,recuAsFQiyYxqtm21
text-davinci-001,0.439,2022-01-27,OpenAI,United States of America,3.19181e+23,"""training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)""

60/3640 = +1.65% to base model compute

base model was reported 3.14e+23 FLOP

3.14e+23 * 1.0165 = 319181000000000000000000",GPT-3 175B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recvqThL1Bym2eWPd
text-davinci-002,0.7,2022-03-15,OpenAI,United States of America,2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,GPT-3.5,5,GPT-4 Technical Report,https://arxiv.org/pdf/2303.08774,,recuAOzevJhAkVd3J
,0.6854,,,,,,GPT-3.5 Turbo,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,reciwAV6BQ14hlLxX
gpt-3.5-turbo-0125,0.673,2024-01-25,OpenAI,United States of America,,,GPT-3.5 Turbo (0125),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recUxR8bLyjemBFQ2
gpt-3.5-turbo-0613,0.689,2023-06-13,OpenAI,United States of America,,,GPT-3.5 Turbo (0613),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,receZjlh6qpQqkcqQ
gpt-3.5-turbo-1106,0.714,2023-11-06,OpenAI,United States of America,,,GPT-3.5 version 1106   ,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recGJbQCjDKrqzKXr
,0.8393,,,,,,GPT-4,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recFBhqAXAnkehHJj
gpt-4-0613,0.824,2023-06-13,OpenAI,United States of America,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",GPT-4 (0613),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,reccKxm0VqsklwpFa
gpt-4-0314,0.864,2023-03-14,OpenAI,United States of America,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",GPT-4 (initial release),5,GPT-4 Technical Report,https://arxiv.org/pdf/2303.08774,,recP6zUYu6rVqvynx
gpt-4-turbo,0.796,2023-11-06,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,GPT-4 Turbo (1106 preview),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec6xx0AwDEsiaUS8
gpt-4-turbo-2024-04-09,0.813,2024-04-09,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,GPT-4 Turbo (2024-04-09),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recTZoT00f1052qmh
gpt-4o-2024-11-20,0.881,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,GPT-4o,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"Specifically, we used log-likelihood evaluations for MMLU (5-shot)",recTErMEF70Oq8pZU
gpt-4o-2024-05-13,0.842,2024-05-13,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,GPT-4o (2024-05-13),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec7uXJcDwu5vAB0v
gpt-4o-2024-08-06,0.843,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,GPT-4o (2024-08-06),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recngZX6kgPdpEuvJ
gpt-4o-mini-2024-07-18,0.767,2024-07-18,OpenAI,United States of America,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",GPT-4o mini (2024-07-18),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec66ukiUXgRuE5uX
gpt-j-6b,0.251,2021-08-05,"EleutherAI,LAION","United States of America,Germany",1.5e+22,source: zero shot evaluation table in GitHub,GPT-J-6B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recSU7vpnDawA53bF
gpt-j-6b,0.257,2021-08-05,"EleutherAI,LAION","United States of America,Germany",1.5e+22,source: zero shot evaluation table in GitHub,GPT-J-6B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recnPxCnkNQT6Bl8j
,0.336,,,,,,GPT-NeoX 20B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recSr00Py8AW3SFO3
Inflection-1,0.727,2023-06-22,Inflection AI,United States of America,1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",Inflection 1,5,Inflection-1 ,https://inflection.ai/blog/inflection-1,,rechQVncuCixOKIxc
,0.672,,,,,,InternLM (104B),5,InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities,https://static.aminer.cn/upload/pdf/127/1564/656/6481884993eaf7045294a0c4_0.pdf,,recwT2hhSXlrSRq1X
internlm-7b,0.51,2023-07-05,,,,,InternLM 7B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recCRfqyDW6e5Cxj0
internlm-chat-20b,0.556,2023-09-17,,,,,InternLM-Chat 20B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recdCL1os0x7URkOn
internlm-chat-20b,0.574,2023-09-17,,,,,InternLM-Chat 20B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recQkCpGZV2zmuda4
,0.782,,,,,,Jamba 1.5 Large,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec1aPizKw4Q63HiH
,0.699,,,,,,Jamba 1.5 Mini,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recTOe7AnWWDiRdsD
,0.659,,,,,,Jamba Instruct,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recg1lDZgeQXcOhJk
LLaMA-7B,0.351,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",LLAMA 1 7B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,reciRq6HujhO9z9nl
LLaMA-13B,0.469,2023-02-27,Meta AI,United States of America,7.8e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22

from paper, Llama-13B took 135,168 GPU hours using A100s

312 trillion * 135,168 * 3600 = 1.518e23 FLOPs at full utilization
This implies that the actual utilization was:
MFU = 7.8e22/1.518e23 = 0.514",LLAMA 1 13B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recN0xjCpJlAJpL3g
LLaMA-33B,0.578,2023-02-27,Meta AI,United States of America,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,LLAMA 1 33B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,rec3LN7X83bnNLYmy
LLaMA-33B,0.568,2023-02-27,Meta AI,United States of America,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,LLaMA 1 33B    ,5,Mixtral of Experts,https://arxiv.org/pdf/2401.04088,,rec8UiAKlK7vPondE
LLaMA-65B,0.634,2023-02-24,Meta AI,United States of America,5.5e+23,1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP,LLAMA 1 65B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,rec1vmtLgykjCiHqG
Llama-2-7b,0.458,2023-07-18,Meta AI,United States of America,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",Llama 2 (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recDRfiRYG3HCQq5K
Llama-2-13b,0.554,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,Llama 2 (13B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recHeILlp9WyfKuQk
Llama-2-70b-hf ,0.695,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",Llama 2 (70B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recOpGv66pVIzJc2y
Llama-2-7b,0.453,2023-07-18,Meta AI,United States of America,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",LLAMA 2 7B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,rec8cJlYZZHD6DliH
Llama-2-7b,0.444,2023-07-18,Meta AI,United States of America,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",LLaMA 2 7B     ,5,Mixtral of Experts,https://arxiv.org/pdf/2401.04088,,recQtVvlLn7Mwe42i
Llama-2-13b,0.548,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLAMA 2 13B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recAtLiMOQMukjkr4
Llama-2-13b,0.556,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLaMA 2 13B    ,5,Mixtral of Experts,https://arxiv.org/pdf/2401.04088,,recQabHfvgaByHw50
Llama-2-34b,0.626,2023-07-18,Meta AI,United States of America,4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",LLAMA 2 34B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recXGb1eMBkdHVVPt
Llama-2-34b,0.626,2023-07-18,Meta AI,United States of America,4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",LLaMA 2 34B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,rec4EA8N1sXX54njV
Llama-2-70b-hf ,0.689,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",LLAMA 2 70B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recCIOv8PV47XWiXE
Llama-2-70b-hf ,0.698,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",LLaMA 2 70B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recEQPQjoxax64ZSN
Llama-2-70b-hf ,0.699,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",LLaMA 2 70B    ,5,Mixtral of Experts,https://arxiv.org/pdf/2401.04088,,recvvZSkhoBXMrVwo
Llama-2-7b,0.4573,2023-07-18,Meta AI,United States of America,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",LLaMA 2-7B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,rech0sV4TGdAsCGBx
Llama-2-13b,0.5509,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLaMA 2-13B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,rec4FrJ61uDhzqdol
Meta-Llama-3-8B-Instruct,0.688,2024-04-18,Meta AI,United States of America,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2√ó10^23 FLOP

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872√ó10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",Llama 3 (8B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recShUHfQj3yvMmET
Meta-Llama-3-70B-Instruct,0.793,2024-04-18,Meta AI,United States of America,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",Llama 3 (70B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recWvSXL72D2hpbVl
Llama-3.1-8B-Instruct,0.561,2024-07-23,Meta AI,United States of America,1.224e+24,"Huggingface page says 3.1-8B used 1.46M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 8B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 8B = 7.2e23 FLOPs

Hardware:
1.46M * 9.9e14 * 3600 * 0.4 = 2.08e24 FLOPs

Geometric mean: sqrt(7.2e23 * 2.08e24) = 1.224e24

Note that Llama 3-8B also said it used 15T tokens, but only 1.3M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.",Llama 3.1 Instruct Turbo (8B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,Seems like the Turbo versions are quantized: https://huggingface.co/spaces/AtlaAI/judge-arena/discussions/4 (that's a different ,recaaHZaz730PgETl
Llama-3.1-70B-Instruct,0.801,2024-07-23,Meta AI,United States of America,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",Llama 3.1 Instruct Turbo (70B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,Seems like the Turbo versions are quantized: https://huggingface.co/spaces/AtlaAI/judge-arena/discussions/4 (that's a different ,recBq5JDLuhxief9r
Llama-3.1-405B-Instruct,0.845,2024-07-23,Meta AI,United States of America,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",Llama 3.1 Instruct Turbo (405B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,Seems like the Turbo versions are quantized: https://huggingface.co/spaces/AtlaAI/judge-arena/discussions/4 (that's a different ,recqvxNx4ASzVf1pr
Llama-3.2-11B-Vision-Instruct,0.565,2024-09-24,Meta AI,United States of America,5.79e+23,"Tensor type is BF16 (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).

‚ÄúTraining utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency‚Ä¶ Training time: Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours.‚Äù (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#hardware-and-software).

The Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).

Assuming 33% utilization rate,
Training compute
~= 0.33 * ( 147000 + 98000 + 896 + 224 ) hours * 3600 s / hour * 1979e12 FLOPS / GPU
~= 5.79e23 FLOPS",Llama 3.2 Vision Instruct Turbo (11B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,Seems like the Turbo versions are quantized: https://huggingface.co/spaces/AtlaAI/judge-arena/discussions/4 (that's a different ,rec91E3tiY2SItmiw
Llama-3.2-90B-Vision-Instruct,0.803,2024-09-24,Meta AI,United States of America,,,Llama 3.2 Vision Instruct Turbo (90B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,Seems like the Turbo versions are quantized: https://huggingface.co/spaces/AtlaAI/judge-arena/discussions/4 (that's a different ,recib3s0R87GkOIJ6
Llama-3.3-70B-Instruct,0.791,2024-12-06,Meta AI,United States of America,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",Llama 3.3 Instruct Turbo (70B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,Seems like the Turbo versions are quantized: https://huggingface.co/spaces/AtlaAI/judge-arena/discussions/4 (that's a different ,recYtSuvfsDMgxpQC
LLaMA-7B,0.356,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",LLaMA 7B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,rec43wZbfZGjTvUxq
LLaMA-7B,0.351,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",LLaMA 7B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recpktJbxIumcFneG
LLaMA-13B,0.477,2023-02-27,Meta AI,United States of America,7.8e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22

from paper, Llama-13B took 135,168 GPU hours using A100s

312 trillion * 135,168 * 3600 = 1.518e23 FLOPs at full utilization
This implies that the actual utilization was:
MFU = 7.8e22/1.518e23 = 0.514",LLaMA 13B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recvzXv36THWKaBgb
LLaMA-13B,0.469,2023-02-27,Meta AI,United States of America,7.8e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22

from paper, Llama-13B took 135,168 GPU hours using A100s

312 trillion * 135,168 * 3600 = 1.518e23 FLOPs at full utilization
This implies that the actual utilization was:
MFU = 7.8e22/1.518e23 = 0.514",LLaMA 13B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recIiOs7bTAVwyNpa
LLaMA-33B,0.587,2023-02-27,Meta AI,United States of America,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,LLaMA 33B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,receAb9h5D57B0gYh
LLaMA-33B,0.578,2023-02-27,Meta AI,United States of America,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,LLaMA 33B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recvQI3hl09uCHYs8
LLaMA-65B,0.634,2023-02-24,Meta AI,United States of America,5.5e+23,1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP,LLaMA 65B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,rec9IhkT5TRasgNag
Llama-2-7b,0.453,2023-07-18,Meta AI,United States of America,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",LLaMA-2 7B,5,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295,"5-shot top-1
",receYgtoMItZ5byvT
Llama-2-13b,0.548,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLaMA-2 13B,5,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295,"5-shot top-1
",recm3ckG9rQdlSf3G
Llama-2-13b,0.548,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLaMA-2 13B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,recQpivyiHncdya7p
Llama-2-34b,0.626,2023-07-18,Meta AI,United States of America,4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",LLaMA-2 34B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,reccZ0kQv849Nx6S0
Meta-Llama-3-8B-Instruct,0.665,2024-04-18,Meta AI,United States of America,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2√ó10^23 FLOP

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872√ó10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",Llama-3-In 8b          ,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recKeTJ0WxkfqaXc6
Llama-3.1-405B,0.844,2024-07-23,Meta AI,United States of America,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",LLaMA-3.1 405B Base,5,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437,,rec7Kb0dlps0bAwVq
LLaMA-7B,0.351,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",LLaMA-7B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recOK4kloLRVBYWyC
LLaMA-7B,0.351,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",LLaMA-7B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recyV0OffUnewduYa
LLaMA-7B,0.32,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",LLaMA-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,reck2fsCW8rwEIBV3
LLaMA-7B,0.352,2023-02-24,Meta AI,United States of America,4.00000001e+22,"1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP
",Llama-7B,2,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,,recXt1b7mmd7pBCEN
LLaMA-13B,0.463,2023-02-27,Meta AI,United States of America,7.8e+22,"1T tokens * 13B parameters * 6 FLOP/token/parameter = 7.8e22

from paper, Llama-13B took 135,168 GPU hours using A100s

312 trillion * 135,168 * 3600 = 1.518e23 FLOPs at full utilization
This implies that the actual utilization was:
MFU = 7.8e22/1.518e23 = 0.514",LLaMA-13B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recC5kLFkYdywVLTE
Llama-2-7b,0.453,2023-07-18,Meta AI,United States of America,8.4e+22,"Trained on 2 trillion tokens per Table 1. 

C = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.

Also, 7B model was trained on 184320 GPU-hours

312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",Llama2-7B,2,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,,rechfMlLcdL4d6JYV
Llama-2-13b-chat,0.509,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLaMA2-Chat 13B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,reccr6kNapl4XhHMh
Llama-2-13b-chat,0.473,2023-07-18,Meta AI,United States of America,1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,LLaMA2-Chat 13B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recP7LM6vdowGXldC
Llama-2-70b-chat,0.594,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",LLaMA2-Chat 70B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,rechEaZxx7y6EW89C
Llama-2-70b-chat,0.599,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",LLaMA2-Chat 70B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recq7vZurZvyzSaF8
Llama-3.3-70B-Instruct,0.863,2024-12-06,Meta AI,United States of America,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",Llama3.3 70b instruct,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"Specifically, we used log-likelihood evaluations for MMLU (5-shot)",recFwE99pUPS1RrqR
Mistral-7B-Instruct-v0.2,0.625,,Mistral AI,France,,,Mistral 7B,5,Gemma: Open Models Based on Gemini Research and Technology,https://arxiv.org/pdf/2403.08295,"5-shot top-1
",rec5aVLAAe2uwnAJ3
Mistral-7B-v0.1,0.625,2023-09-27,Mistral AI,France,,,Mistral 7B,5,Mixtral of Experts,https://arxiv.org/pdf/2401.04088,,reclqttZHU3FrMenP
Mistral-7B-v0.1,0.601,2023-09-27,Mistral AI,France,,,Mistral 7B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,recobovxkli2yXD7Q
Mistral-7B-v0.1,0.617,2023-09-27,Mistral AI,France,,,Mistral 7b             ,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recrKIaLtBWuoRKy0
Mistral-7B-Instruct-v0.3,0.599,2024-05-27,Mistral AI,France,,,Mistral Instruct v0.3 (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recxGM6lCcs0CnKWO
mistral-large-2402,0.688,2024-02-26,Mistral AI,France,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are ‚Ç¨1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",Mistral Large (2402),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recMfHwyb3g0GU3WP
mistral-large-2407,0.8,2024-07-24,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",Mistral Large 2 (2407),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recKDxkbfoASopZz7
,0.653,,,,,,Mistral NeMo (2402),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,reccvR6S88KxSkuED
mistral-small-2402,0.687,2024-02-26,,,,,Mistral Small (2402),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recKStU0sPWNiLWCM
Mistral-7B-v0.1,0.566,2023-09-27,Mistral AI,France,,,Mistral v0.1 (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recdbbEpQDnuQx1SM
,0.717,,,,,,Mixtral (8x7B 32K seqlen),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recHxyrzN6ysHgwup
Mixtral-8x22B-v0.1,0.778,2024-04-17,Mistral AI,France,2.34e+24,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 39 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 2.34e+24 FLOP",Mixtral (8x22B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,reczWpbUvk8LZHdwj
Mixtral-8x7B-v0.1,0.706,2023-12-11,Mistral AI,France,7.74e+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",Mixtral 8x7B,5,Mixtral of Experts,https://arxiv.org/pdf/2401.04088,,rec2QbPaiM3fs3ojk
Mixtral-8x7B-v0.1,0.705,2023-12-11,Mistral AI,France,7.74e+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",Mixtral 8x7b           ,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recVbhuiQ3mCmSZWs
mpt-7b,0.308,2023-05-05,MosaicML,United States of America,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k.""",MPT 7B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recXxnvP42eSSUf1r
mpt-7b,0.268,2023-05-05,MosaicML,United States of America,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k.""",MPT 7B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,rec6fhu1fu0tDpFCM
mpt-30b,0.479,2023-06-22,MosaicML,United States of America,1.8900000000001e+23,"According to their blog post,
""MPT-30B FLOPs ~= 6 * 30e9 [params] * 1.05e12 [tokens] = 1.89e23 FLOPs""",MPT 30B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recsvME0N7N3lHCNM
mpt-30b,0.469,2023-06-22,MosaicML,United States of America,1.8900000000001e+23,"According to their blog post,
""MPT-30B FLOPs ~= 6 * 30e9 [params] * 1.05e12 [tokens] = 1.89e23 FLOPs""",MPT 30B,5,Llama 2: Open Foundation and Fine-Tuned Chat Models,http://arxiv.org/abs/2307.09288,,recORhwxKRN3CKT5H
mpt-7b,0.2793,2023-05-05,MosaicML,United States of America,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k.""",MPT-7B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recciuLJt00P8UdBH
mpt-7b,0.268,2023-05-05,MosaicML,United States of America,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k.""",MPT-7B,2,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,,recThSGOCYopyGBAn
mpt-7b,0.267,2023-05-05,MosaicML,United States of America,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k.""",MPT-7B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recnP0IxuIUI5wyAV
mpt-7b,0.274,2023-05-05,MosaicML,United States of America,4.2000000000000004e+22,"""As shown in Table 3, nearly all of the training budget was spent on the base MPT-7B model, which took ~9.5 days to train on 440xA100-40GB GPUs, and cost ~$200k.""",MPT-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recjktWCt7fCSUqIu
Nemotron-4 15B,0.587,2024-02-26,NVIDIA,United States of America,7.5005116e+23,"6ND = 6 FLOP/token/parameter * 15*10^9 parameters * 8*10^12 tokens = 7.2e+23 FLOP

""Nemotron-4 was trained using 384 DGX H100 nodes; each node contains 8 H100 80GB SXM5 GPUs based on the NVIDIA Hopper architecture (NVIDIA, 2022). Each H100 GPU has a peak throughput of 989 teraFLOP/s when doing 16-bit floating point (bfloat16) arithmetic without sparsity.

Table 2 reports more detailed training schedule:

989*10^12 FLOP/sec * 3600 sec/hour * 24 hours * (768 gpus * 0.343 [reported utilization] * 0.8 days +  1536 gpus * 0.333 [reported utilization] * 0.4 days + 2304 gpus * 0.305 [reported utilization] * 11.9 days) = 7.5005116e+23 FLOP",Nemotron-4 15B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,recFoOkw8h4blebt7
,0.295,,,,,,OLMo (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recSXeZwsr3iejrsy
,0.538,,,,,,OLMo 1.7 (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recfz827JQEYK0ML2
open_llama_7b,0.299,2023-06-07,,,,,OpenLLaMA-7B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recS3RPFpiAohJDyr
open_llama_7b,0.286,2023-06-07,,,,,OpenLLaMA-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recKitarPK5ZkGKHY
opt-13b,0.251,2022-05-11,,,,,OPT-13B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recnCITHR0D69TaSW
opt-13b,0.244,2022-05-11,,,,,OPT-13B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recptjItsyCc9FRVH
,0.254,,,,,,PaLM 8B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,rec86hiwuXU11mYYm
PaLM 62B,0.537,2022-04-04,,,,,PaLM 62B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,rec9nwU5JvWa3KMNP
PaLM 540B,0.693,2022-04-04,Google Research,United States of America,2.5272e+24,"See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers."" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains",PaLM 540B,5,LLaMA: Open and Efficient Foundation Language Models,http://arxiv.org/abs/2302.13971,,recq6tXUJ4hdJZHiK
,0.692,,,,,,PaLM-2 (Bison),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,receRMAyrZg4S22XH
,0.786,,,,,,PaLM-2 (Unicorn),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recrhlTc9IULolRYJ
,0.621,,,,,,Palmyra X V2 (33B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recgWVSaehLn2uc2n
,0.786,,,,,,Palmyra X V3 (72B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recraXT1q5kVa9Df9
,0.739,,,,,,Palmyra-X-004,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recxRfXGUWwfxzfMT
phi-1_5,0.376,2023-09-11,Microsoft,United States of America,1.17e+21,"150B training tokens

150B*1.3B*6 = 1.17e21

also, took 1.5k GPU-hours with A100s, per Table 1

1500 * 312 trillion * 3600 * 0.3 (utilization assumption) = 5.05e20

so utilization was likely ~60%. may be high due to the relatively small cluster.
",phi-1.5,2,Textbooks Are All You Need II: phi-1.5 technical report,http://arxiv.org/abs/2309.05463,"We use the harness-eval zero-shot accuracy on PIQA, Hellaswag, OpenbookQA, 2-shot performance on MMLU,",recIRX5Xrwuj0aVsw
phi-2,0.584,2023-12-12,Microsoft,United States of America,2.27e+22,"2.7B params, trained on 1.4T tokens

2.7 billion * 1.4 trillion * 6 = 2.27e22

96*14 A100-days

14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22",Phi-2,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recOjVJ9mVKxRJb67
phi-2,0.563,2023-12-12,Microsoft,United States of America,2.27e+22,"2.7B params, trained on 1.4T tokens

2.7 billion * 1.4 trillion * 6 = 2.27e22

96*14 A100-days

14 * 96 * 312 trillion * 24 * 3600 * 0.3 = 1.09e22",Phi-2,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recZMvyAbMp0eIx88
,0.757,,,,,,Phi-3 (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recjSB9vhRshMrbfW
Phi-3-medium-128k-instruct,0.775,2024-04-23,Microsoft,United States of America,4.032e+23,counting operations: 6√ó4.8√ó10^12 tokens √ó 14√ó10^9 parameters ‚âà 4.032√ó10^23 FLOPS,Phi-3 (14B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recIC2lbeIoOMCnU9
Phi-3-medium-128k-instruct,0.779,2024-04-23,Microsoft,United States of America,4.032e+23,counting operations: 6√ó4.8√ó10^12 tokens √ó 14√ó10^9 parameters ‚âà 4.032√ó10^23 FLOPS,Phi-3 14b,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"Specifically, we used log-likelihood evaluations for MMLU (5-shot)",recokPFC1wM7clrm7
Phi-3-medium-128k-instruct,0.78,2024-04-23,Microsoft,United States of America,4.032e+23,counting operations: 6√ó4.8√ó10^12 tokens √ó 14√ó10^9 parameters ‚âà 4.032√ó10^23 FLOPS,Phi-3-medium,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recCmqAcPDN7z3eOd
Phi-3-mini-4k-instruct,0.688,2024-04-23,Microsoft,United States of America,7.524e+22,"counting operations: 6√ó3.3√ó10^12 tokens √ó3.8√ó10^9 parameters ‚âà7.524√ó10^22 FLOPS
hardware estimate: 7 days √ó24 hours / day√ó3600 sec / hour *989,000,000,000,000 FLOP/s*512 GPUs*0.3 [assumed utilization]=9.187540992√ó10^22",Phi-3-mini,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,recBm3x8Mkas41pg2
Phi-3-small-8k-instruct,0.757,2024-04-23,Microsoft,United States of America,2.1312e+23,6ND = 6 FLOP / parameter / token * 7.4B parameters * 4.8T tokens = 2.1312e+23 FLOP,Phi-3-small,5,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,http://arxiv.org/abs/2404.14219,,reclGoqueJP6ji9WZ
phi-4,0.848,2024-12-12,Microsoft Research,United States of America,9.3202015e+23,"6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP

989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP

geometric mean
sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23",Phi-4 14b,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"""Specifically, we used log-likelihood evaluations for MMLU (5-shot)""",recPr3djloYa7CtTW
Qwen-1_8B,0.282,2023-11-30,,,,,Qwen 1.8B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recJEnP48cP1MNsrP
Qwen-7B,0.45,2023-09-28,Alibaba,China,1.01e+23,"2.4T tokens per Table 1

7b*2.4T*6 = 1.01e23",Qwen 7B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,recZMyuEA1HyQDC9A
Qwen-14B,0.663,2023-09-28,Alibaba,China,2.5e+23,"3T tokens per Table 1

14B*3T*6 = 2.5e23",QWEN 14B,5,Nemotron-4 15B Technical Report,http://arxiv.org/abs/2402.16819,,recbr9lo3CeOOBkvY
Qwen-14B,0.534,2023-09-28,Alibaba,China,2.5e+23,"3T tokens per Table 1

14B*3T*6 = 2.5e23",Qwen 14B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,reci4VBEctqPF5t2B
Qwen-14B-Chat,0.64,2023-09-24,Alibaba,China,2.5e+23,"3T tokens per Table 1

14B*3T*6 = 2.5e23",Qwen-Chat 14B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,rechXjvOrdKm234Fr
Qwen-14B-Chat,0.65,2023-09-24,Alibaba,China,2.5e+23,"3T tokens per Table 1

14B*3T*6 = 2.5e23",Qwen-Chat 14B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recfT37OwcoOgwoYl
Qwen1.5-7B,0.626,2024-02-04,Alibaba,China,1.68e+23,6 FLOP / parameter / token * 7*10^9 parameters * 4*10^12 tokens =  1.68e+23 FLOP,Qwen1.5 (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recFmlmIXPZeKd0uT
qwen1.5-14B,0.686,2024-02-04,Alibaba,China,3.36e+23,6 FLOP / parameter / token * 14*10^9 parameters * 4*10^12 tokens =  3.36e+23 FLOP,Qwen1.5 (14B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recgNYd9sZSHzqlh3
qwen1.5-32B,0.744,2024-02-04,Alibaba,China,,"upper bound is taken from Qwen1.5 72B training compute estimation

lower bound is taken from Qwen1.5 14B training compute estimation",Qwen1.5 (32B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recpbIA4MZ2d0bMHo
,0.774,,,,,,Qwen1.5 (72B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recN7w14y80rqwibJ
,0.768,,,,,,Qwen1.5 Chat (110B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec4IdPlkVEWoAke2
qwen2-72b-instruct,0.824,2024-06-07,Alibaba,China,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",Qwen2 Instruct (72B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec4k5yqvGWnSlnLK
qwen2.5-14b-instruct,0.799,2025-02-26,Alibaba,China,1.58760000000001e+24,"Training dataset size was 18 trillion

6ND = 6 * 14.7 billion parameters * 18 trillion tokens = 1.59e24",Qwen2.5 14b instruct,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"Specifically, we used log-likelihood evaluations for MMLU (5-shot)",recaLqeKOZiJFuyWC
Qwen2.5-72B,0.85,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",Qwen2.5 72B Base,5,DeepSeek-V3 Technical Report,https://arxiv.org/pdf/2412.19437,,rec1e9rTGH12ziqp2
qwen2.5-72b-instruct,0.853,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",Qwen2.5 72b instruct,5,Phi-4 Technical Report,http://arxiv.org/abs/2412.08905,"Specifically, we used log-likelihood evaluations for MMLU (5-shot)",recbl49vkZltt7Qu7
qwen2.5-7b-instruct,0.729,2025-02-26,Alibaba,China,8.2188e+23,"Training dataset size was 18 trillion

6ND = 6 * 7.61 billion parameters * 18 trillion tokens = 8.2188e+23",Qwen2.5 Instruct Turbo (7B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,rec4M7QClQ8t5k7Fi
qwen2.5-72b-instruct,0.834,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",Qwen2.5 Instruct Turbo (72B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recUfCycerC91cgsO
Qwen2.5-Coder-0.5B,0.42,2024-09-18,,,,,Qwen2.5-Coder-0.5B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recA0z9ynBjuFVr0i
Qwen2.5-Coder-1.5B,0.536,2024-09-18,Alibaba,China,5.082e+22,6ND =  6*1540000000 parameters *5.5T tokens =5.082e+22,Qwen2.5-Coder-1.5B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recsb2ABruR7vDJyc
Qwen2.5-Coder-32B,0.612,2024-09-18,Alibaba,China,1.0725e+24,"Assuming 1 epoch

6ND = 6*32.5 parameters *10^9*5.5*10^12 tokens = 1.0725e+24",Qwen2.5-Coder-3B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recFedHhPHlgqWxpq
Qwen2.5-Coder-7B,0.68,2024-09-18,Alibaba,China,2.5113e+23,6ND = 6 FLOP / token / parameter *7610000000 parameters *5.5T tokens =2.5113e+23 FLOP,Qwen2.5-Coder-7B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recot1Px8adoUoPs3
Qwen2.5-Coder-14B,0.752,2024-09-18,,,,,Qwen2.5-Coder-14B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recXLZnQpE0oht07R
Qwen2.5-Coder-32B,0.791,2024-09-18,Alibaba,China,1.0725e+24,"Assuming 1 epoch

6ND = 6*32.5 parameters *10^9*5.5*10^12 tokens = 1.0725e+24",Qwen2.5-Coder-32B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recgIGiZ5lVnVM7rT
RedPajama-INCITE-7B-Base,0.263,2023-05-04,,,,,Redpajama-7B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,reclqOVSaOoy7NX2U
RedPajama-INCITE-7B-Base,0.258,2023-05-04,,,,,Redpajama-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recQGR1shnVCw79MB
,0.776,,,,,,Solar Pro,5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recvPdPH4F3YB9X2m
StableBeluga2,0.686,2023-07-20,Stability AI,United Kingdom of Great Britain and Northern Ireland,,,StableBeluga2 70B,5,Qwen Technical Report,https://arxiv.org/pdf/2309.16609,,reca4GRikJuwzhgQj
starcoder2-3b,0.366,2024-02-22,"Hugging Face,ServiceNow,NVIDIA,BigCode",,5.94e+22,estimation is given in Table 6 ,StarCoder2-3B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,rec7SmDXXVMzS12UD
starcoder2-7b,0.388,2024-02-20,"Hugging Face,ServiceNow,NVIDIA,BigCode",,1.55e+23,estimation is given in Table 6 ,StarCoder2-7B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recbF6EudRQDiZM0w
starcoder2-15b,0.641,2024-02-20,"Hugging Face,ServiceNow,NVIDIA,BigCode",,3.87e+23,estimation is given in Table 6 ,StarCoder2-15B,,Qwen2.5-Coder Technical Report,http://arxiv.org/abs/2409.12186,,recslUwoLlToAjB27
,0.52,,,,,,Vicuna-13B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,recjDwTuCMcGYnymQ
xgen-7b-8k-base,0.363,2023-06-27,Salesforce,United States of America,8.02e+22,"270,336 TPUv4-hours per the carbon emissions section. They must mean chip-hours, not core-hours, because they multiply by 192W, which is the max power consumption of a TPU-v4 chip.

https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4

This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 

270336 * 275 teraflop/s * 3600 * 0.3 = 8.02e22 FLOP

also, using 6ND:

1484 billion tokens * 6.7 billion * 6 = 5.97e22",XGen-7B,5,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recmtaX4LELTcHv2G
xgen-7b-8k-base,0.321,2023-06-27,Salesforce,United States of America,8.02e+22,"270,336 TPUv4-hours per the carbon emissions section. They must mean chip-hours, not core-hours, because they multiply by 192W, which is the max power consumption of a TPU-v4 chip.

https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4

This is probably the total for the whole paper? They released two versions of the base model, one of which is just trained further than the other, and they have two fine-tuned versions. If fine-tuning compute is minor, then this total is close to the largest training compute for any single model in the paper. 

270336 * 275 teraflop/s * 3600 * 0.3 = 8.02e22 FLOP

also, using 6ND:

1484 billion tokens * 6.7 billion * 6 = 5.97e22",XGen-7B,0,XGen-7B Technical Report,http://arxiv.org/abs/2309.03450,,recNXliE4Qvt56aRx
,0.5521,,,,,,XVERSE-13B,5,Baichuan 2: Open Large-scale Language Models,http://arxiv.org/abs/2309.10305,,rec3RbWkc7eNqnBO3
Yi-6B,0.64,2023-11-02,01.AI,China,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Yi (6B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recQDuDGOq1wY5Ouj
Yi-34B,0.762,2023-11-02,01.AI,China,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Yi (34B),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recjx70KTlRekGQ02
Yi-6B,0.632,2023-11-02,01.AI,China,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Yi 6B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,"We report the overall results for MMLU[27](5-shot), CMMLU[42] (5-shot), Gaokao-Bench[90] (5-shot), and BigBench[72] Hard (BBH[74]) (3-shot).",recGm5EDNDIETSYi6
Yi-34B,0.763,2023-11-02,01.AI,China,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Yi 34B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,"We report the overall results for MMLU[27](5-shot), CMMLU[42] (5-shot), Gaokao-Bench[90] (5-shot), and BigBench[72] Hard (BBH[74]) (3-shot).",recwCEStuvtsY9T89
Yi-large,0.793,2024-05-13,01.AI,China,1.8e+24,"6ND = 6*100000000000*3000000000000=1.8e+24

(speculative confidence because training dataset size is very uncertain)",Yi Large (Preview),5,Stanford CRFM Leaderboard,https://crfm.stanford.edu/helm/lite/latest/#/leaderboard/mmlu,,recbChoe3AEIXUMHg
Yi-9B,0.684,2024-03-01,,,,,Yi-9B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recrn20mUuMopGxXJ
Yi-6B-Chat,0.582,2023-11-22,01.AI,China,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Yi-Chat 6B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recS1feH5g4KJrpW8
Yi-6B-Chat,0.61,2023-11-22,01.AI,China,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Yi-Chat 6B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,receVD87dnzgVzRts
Yi-34B-Chat,0.676,2023-11-22,01.AI,China,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Yi-Chat 34B,0,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,recswVGACIIFbdPtu
Yi-34B-Chat,0.735,2023-11-22,01.AI,China,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Yi-Chat 34B,5,Yi: Open Foundation Models by 01.AI,http://arxiv.org/abs/2403.04652,,reckSm2dCL3rMGbBd
