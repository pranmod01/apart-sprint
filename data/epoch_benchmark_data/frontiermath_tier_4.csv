Model version,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gpt-5-mini-2025-08-07_high,0.0625,2025-08-07,OpenAI,United States of America,,,0.03530829267912701,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/5rh6TyPLbkJmQZs8iY6msJ.eval,2025-10-30T16:22:42.856Z,5rh6TyPLbkJmQZs8iY6msJ
gpt-5-nano-2025-08-07_high,0.0,2025-08-07,OpenAI,United States of America,,,0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/CvK572G5SSd5Yk8ZGikDR6.eval,2025-10-30T16:22:14.713Z,CvK572G5SSd5Yk8ZGikDR6
gpt-5-2025-08-07_high,0.125,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.04824031156174749,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/6LwyJZsWEzuD8mhSRUF2xn.eval,2025-10-30T02:14:04.513Z,6LwyJZsWEzuD8mhSRUF2xn
claude-haiku-4-5-20251001_32K,0.020833333333333332,2025-10-15,Anthropic,United States of America,,,0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/YRVwm4B88qK6aUEFBdNQmP.eval,2025-10-22T15:24:54.352Z,YRVwm4B88qK6aUEFBdNQmP
claude-sonnet-4-5-20250929_32K,0.041666666666666664,2025-09-29,Anthropic,United States of America,,,0.029147663515556026,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/d8WWwzb6b57mWqddhBXuDi.eval,2025-10-22T14:19:03.957Z,d8WWwzb6b57mWqddhBXuDi
gpt-5-pro-2025-10-06_high,0.125,2025-10-07,OpenAI,United States of America,,,0.048,,,2025-10-09T13:40:00.000Z,VshpC5TEp4F72cutXRRmhs
claude-sonnet-4-5-20250929,0.020833333333333332,2025-09-29,Anthropic,United States of America,,,0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/2o8XZ8V6W5jKD7TuY4frux.eval,2025-09-29T17:41:54.543Z,2o8XZ8V6W5jKD7TuY4frux
grok-4-0709,0.020833333333333332,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Cqdzvvh9tdrdmDjvu8bntM.eval,2025-08-11T12:47:20.407Z,Cqdzvvh9tdrdmDjvu8bntM
o4-mini-2025-04-16_medium,0.020833333333333332,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/oBjz3oZSvAkw5ArLtZMwWW.eval,2025-08-07T22:00:02.124Z,oBjz3oZSvAkw5ArLtZMwWW
gpt-5-2025-08-07_medium,0.0625,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.03530829267912701,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/RrmHCttQmYWNaADyThwFwf.eval,2025-08-07T18:43:40.577Z,RrmHCttQmYWNaADyThwFwf
gpt-5-mini-2025-08-07_medium,0.041666666666666664,2025-08-07,OpenAI,United States of America,,,0.02914766351555603,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/EAqUKKCdvdkYTGegACdean.eval,2025-08-07T18:43:35.509Z,EAqUKKCdvdkYTGegACdean
gpt-5-nano-2025-08-07_medium,0.020833333333333332,2025-08-07,OpenAI,United States of America,,,0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/F6Tw2b9fDKde6N9eBu6ory.eval,2025-08-07T18:43:33.699Z,F6Tw2b9fDKde6N9eBu6ory
Kimi-K2-Instruct,0.0,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/BiKWBn36DdZwNa8m4okN7H.eval,2025-08-07T15:42:35.128Z,BiKWBn36DdZwNa8m4okN7H
claude-opus-4-1-20250805_27K,0.041666666666666664,2025-08-05,Anthropic,United States of America,,,0.029147663515556026,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/a6VpLAmwU23BmxsqrZTGZg.eval,2025-08-05T22:14:34.193Z,a6VpLAmwU23BmxsqrZTGZg
gemini-2.5-pro,0.041666666666666664,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02914766351555603,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/XtEW42hsVgroBKHK465wqA.eval,2025-07-03T15:44:08.541Z,XtEW42hsVgroBKHK465wqA
gemini-2.5-pro-preview-06-05,0.020833333333333332,2025-06-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/iwLqUvjgt378uQoKiCdSXG.eval,2025-07-03T13:56:46.043Z,iwLqUvjgt378uQoKiCdSXG
gpt-4.1-2025-04-14,0.0,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/oDrEMnoqz3AapWeyrxZd6C.eval,2025-07-01T17:05:38.433Z,oDrEMnoqz3AapWeyrxZd6C
claude-opus-4-20250514,0.0,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/MUYxn78tdpBMLETSQg5rgN.eval,2025-07-01T17:05:23.211Z,MUYxn78tdpBMLETSQg5rgN
o3-2025-04-16_high,0.020833333333333332,2025-04-16,OpenAI,United States of America,,,0.020833333333333332,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/a9fSMpBem4uhc6pxuEiQqM.eval,2025-07-01T17:05:19.927Z,a9fSMpBem4uhc6pxuEiQqM
claude-3-5-sonnet-20240620,0.0,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ELsuPViQBj3UPywy6WyB5H.eval,2025-07-01T17:05:16.422Z,ELsuPViQBj3UPywy6WyB5H
o3-mini-2025-01-31_high,0.041666666666666664,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.029147663515556026,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/YZ6Ts7tSVrvzjSRGxJowdn.eval,2025-07-01T17:05:14.302Z,YZ6Ts7tSVrvzjSRGxJowdn
claude-opus-4-20250514_27K,0.041666666666666664,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.029147663515556026,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/Ee6YnzHKyAUxmYeL23TL9s.eval,2025-07-01T17:05:12.990Z,Ee6YnzHKyAUxmYeL23TL9s
claude-3-5-sonnet-20241022,0.0,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/8mhf6cwsHtrRkAwxZSBgfV.eval,2025-07-01T17:05:10.267Z,8mhf6cwsHtrRkAwxZSBgfV
o4-mini-2025-04-16_high,0.0625,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.03530829267912701,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/WgEkfyLvjHYXfPje5Z2CPX.eval,2025-07-01T17:05:06.108Z,WgEkfyLvjHYXfPje5Z2CPX
grok-3-beta,0.0,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZyHLq7azxVkZJZRLTj3kMB.eval,2025-07-01T17:05:01.787Z,ZyHLq7azxVkZJZRLTj3kMB
claude-sonnet-4-20250514,0.0,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.0,,https://epoch-benchmarks-production-private.s3.us-east-2.amazonaws.com/inspect_ai_logs/XMYUtk8tkqDFgohY8iYfmy.eval,2025-07-01T17:04:25.401Z,XMYUtk8tkqDFgohY8iYfmy
gemini-2.5-deep-think-2025-08-01-webapp,0.104,2025-08-01,"Google,Google DeepMind","United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.044,,,,manual_gemini_2.5_deep_think_run_frontiermath_tier_4
