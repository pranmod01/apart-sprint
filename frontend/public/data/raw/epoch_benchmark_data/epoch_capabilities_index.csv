Model version,ECI Score,Release date,Organization,Country,Model accessibility,Training compute (FLOP),Confidence,Model name,Description,Display name
gpt-5-nano-2025-08-07_high,138.80903825583073,2025-08-07,OpenAI,United States of America,API access,,Unknown,GPT-5 nano,"The smallest version of OpenAI's latest flagship frontier model, benchmarked using high reasoning effort.",GPT-5 nano (high)
gpt-5-mini-2025-08-07_high,144.10801576005468,2025-08-07,OpenAI,United States of America,API access,,Unknown,GPT-5 mini,"A smaller version of OpenAI's latest flagship frontier model, benchmarked using high reasoning effort.",GPT-5 mini (high)
gpt-5-2025-08-07_high,149.63110287022786,2025-08-07,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,"OpenAI's latest flagship frontier model, benchmarked using high reasoning effort.",GPT-5 (high)
claude-sonnet-4-5-20250929_16K,,2025-09-29,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4.5,"Anthropic's latest mid-size model, part of the Claude 4.5 series, evaluated with up to 16,000 reasoning tokens.",Claude Sonnet 4.5 (16k thinking)
claude-sonnet-4-5-20250929_59K,,,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4.5,,Claude Sonnet 4.5 (59k thinking)
gpt-4-0613,122.10983844787572,2023-06-13,OpenAI,United States of America,API access,2.1e+25,Likely,GPT-4,"The June 2023 version of GPT-4, OpenAI's 2023 multimodal flagship model.",GPT-4 (Jun 2023)
gpt-4-0314,125.6972661914446,2023-03-14,OpenAI,United States of America,API access,2.1e+25,Likely,GPT-4,"The March 2023 version of GPT-4, OpenAI's 2023 multimodal flagship model.",GPT-4 (Mar 2023)
claude-haiku-4-5-20251001_32K,,2025-10-15,Anthropic,United States of America,API access,,Unknown,Claude Haiku 4.5,,
claude-sonnet-4-5-20250929_32K,142.50439363361184,2025-09-29,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4.5,"Anthropic's latest mid-size model, part of the Claude 4.5 series, evaluated with up to 32,000 reasoning tokens.",Claude Sonnet 4.5 (32k thinking)
claude-haiku-4-5-20251001,,2025-10-15,Anthropic,United States of America,API access,,Unknown,Claude Haiku 4.5,,
qwen3-max-2025-09-23,140.77336656932906,2025-09-24,Alibaba,China,API access,1.512e+25,Speculative,Qwen3-Max,A 1-trillion total parameter scale model in the Qwen 3 series. ,Qwen3-Max-Instruct
gpt-5-pro-2025-10-06_high,,2025-10-07,OpenAI,United States of America,API access,,Unknown,GPT-5 Pro,,
claude-sonnet-4-5-20250929,138.07161736895296,2025-09-29,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4.5,"Anthropic's latest mid-size model, part of the Claude 4.5 series.",Claude Sonnet 4.5 (no thinking)
DeepSeek-V3.1,,2025-08-21,DeepSeek,China,Open weights (unrestricted),3.594058e+24,Confident,DeepSeek-V3.1,An updated version of DeepSeek V3.,
glm-4.5,,2025-08-03,"Zhipu AI,Tsinghua University",China,Open weights (unrestricted),4.42e+24,Confident,GLM 4.5,Zhipu AI & Tsinghua University’s large open source model.,
gpt-5-2025-08-07_medium,150.0,2025-08-07,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,"OpenAI's latest flagship frontier model, benchmarked using medium reasoning effort.",GPT-5 (medium)
gpt-5-nano-2025-08-07_medium,139.22795097334506,2025-08-07,OpenAI,United States of America,API access,,Unknown,GPT-5 nano,"The smallest version of OpenAI's latest flagship frontier model, benchmarked using medium reasoning effort.",GPT-5 nano (medium)
gpt-5-mini-2025-08-07_medium,141.95766101410933,2025-08-07,OpenAI,United States of America,API access,,Unknown,GPT-5 mini,"A smaller version of OpenAI's latest flagship frontier model, benchmarked using medium reasoning effort.",GPT-5 mini (medium)
o3-2025-04-16_medium,144.14473450786326,2025-04-16,OpenAI,United States of America,API access,,Unknown,o3,"OpenAI's second generation o-series reasoning model, evaluated with medium reasoning effort.",o3 (medium)
kimi-k2-0711-preview,,2025-07-12,Moonshot,China,Open weights (restricted use),2.976e+24,Confident,Kimi K2,A preview of an updated version of Kimi K2.,
grok-4-0709,145.89868574783725,2025-07-09,xAI,United States of America,API access,5.0000000000001e+26,Speculative,Grok 4,,
o4-mini-2025-04-16_medium,141.88888317937534,2025-04-16,OpenAI,United States of America,API access,,Unknown,o4-mini,"The latest smaller size o-series reasoning model from OpenAI, evaluated with medium reasoning effort.",o4-mini (medium)
gpt-4o-2024-11-20,129.08328664033564,2024-11-20,OpenAI,United States of America,API access,,Speculative,GPT-4o,"The November 2024 version of GPT-4o, OpenAI's then-flagship multimodal language model.",GPT-4o (Nov 2024)
Kimi-K2-Instruct,134.87104890643565,2025-07-12,Moonshot,China,Open weights (restricted use),2.976e+24,Confident,Kimi K2,Moonshot AI’s 1-trillion parameter scale model.,
claude-opus-4-20250514_27K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 27,000 reasoning tokens.",
claude-opus-4-1-20250805_27K,140.62684603138112,2025-08-05,Anthropic,United States of America,API access,,Unknown,Claude Opus 4.1,"Anthropic’s largest flagship model, benchmarked with up to 27,000 reasoning tokens.",
claude-opus-4-1-20250805,138.58617893798265,2025-08-05,Anthropic,United States of America,API access,,Unknown,Claude Opus 4.1,Anthropic’s largest flagship model.,
claude-opus-4-1-20250805_16K,,2025-08-05,Anthropic,United States of America,API access,,Unknown,Claude Opus 4.1,"Anthropic’s largest flagship model, benchmarked with up to 16,000 reasoning tokens.",
gpt-4o-mini-2024-07-18,126.07682300726024,2024-07-18,OpenAI,United States of America,API access,,Speculative,GPT-4o mini,A smaller version of OpenAI's GPT-4o.,
claude-opus-4-20250514,138.35642099722173,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1).,
claude-sonnet-4-20250514,136.3014418099942,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,Anthropic's mid-sized version of Claude 4 series of reasoning models.,
gemini-2.5-pro,,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,Google’s general-purpose frontier reasoning model.,
gemini-2.5-pro-preview-06-05,146.23747589064803,2025-06-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,"A June 2025 preview version of Gemini 2.5 Pro, Google DeepMind's flagship frontier model.",Gemini 2.5 Pro Preview (Jun 2025)
gpt-4.1-2025-04-14,136.24913796539968,2025-04-14,OpenAI,United States of America,API access,,Unknown,GPT-4.1,A coding-optimized GPT-4 series model from OpenAI.,GPT-4.1
o3-2025-04-16_high,144.24768124923224,2025-04-16,OpenAI,United States of America,API access,,Unknown,o3,"OpenAI's second generation o-series reasoning model, evaluated with high reasoning effort.",o3 (high)
grok-3-mini-beta_high,140.04918245445444,2025-04-09,xAI,United States of America,API access,,Unknown,Grok-3 mini,"A beta release of a smaller version of XAI's third generation Grok model, evaluated with high reasoning effort.",
claude-3-5-sonnet-20241022,133.59354254461562,2024-10-22,Anthropic,United States of America,API access,2.700000000000001e+25,Speculative,Claude 3.5 Sonnet,"An updated version of the top model in the Claude 3.5 series, a previous flagship model series from Anthropic.",Claude 3.5 Sonnet (Oct 2024)
claude-3-5-sonnet-20240620,130.0,2024-06-20,Anthropic,United States of America,API access,2.700000000000001e+25,Speculative,Claude 3.5 Sonnet,"The first version of the top model in the Claude 3.5 series, a previous flagship model series from Anthropic.",Claude 3.5 Sonnet (Jun 2024)
o3-mini-2025-01-31_high,140.48579879707853,2025-01-31,OpenAI,United States of America,API access,,Unknown,o3-mini,"A smaller version of OpenAI's o3 reasoning model, evaluated with high reasoning effort.",o3-mini (high)
claude-sonnet-4-20250514_59K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,"Anthropic's mid-sized model in the Claude 4 series, benchmarked with up to 59,000 reasoning tokens.",
claude-3-7-sonnet-20250219_64K,139.22500862973092,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 64,000 reasoning tokens allowed.",Claude 3.7 Sonnet (64k thinking)
o4-mini-2025-04-16_high,143.53249629941192,2025-04-16,OpenAI,United States of America,API access,,Unknown,o4-mini,"The latest smaller size o-series reasoning model from OpenAI, evaluated with high reasoning effort.",o4-mini (high)
DeepSeek-R1-0528,139.6979429366786,2025-05-28,DeepSeek,China,Open weights (unrestricted),4.020010000000001e+24,Confident,DeepSeek-R1,"An updated May 2025 version of DeepSeek's reasoning model, R1.",DeepSeek-R1 (May 2025)
grok-3-beta,138.0731900059859,2025-04-09,xAI,United States of America,API access,3.5000000000000006e+26,Likely,Grok 3,A beta release of XAI's third generation Grok model.,
magistral-small-2506,,2025-06-10,Mistral AI,France,Open weights (unrestricted),,Confident,Magistral Small 1.1,,
qwen3-235b-a22b,135.53653027193312,2025-04-29,Alibaba,China,Open weights (unrestricted),4.752e+24,Likely,Qwen3-235B-A22B,,
gemini-2.5-pro-preview-05-06,140.83725714262218,2025-05-06,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,"A May 2025 preview version of Gemini 2.5 Pro, a flagship reasoning model from Google DeepMind.",Gemini 2.5 Pro Preview (Jun 2025)
DeepSeek-R1,138.0353561386285,2025-01-20,DeepSeek,China,Open weights (unrestricted),4.020010000000001e+24,Confident,DeepSeek-R1,,
claude-sonnet-4-20250514_32K,140.19454794298508,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,"Anthropic's mid-sized model in the Claude 4 series, benchmarked with up to 32,000 reasoning tokens.",
claude-opus-4-20250514_16K,139.93912221849905,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 16,000 reasoning tokens.",
claude-sonnet-4-20250514_16K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,"Anthropic's mid-sized model in the Claude 4 series, benchmarked with up to 16,000 reasoning tokens.",
gemini-2.5-flash-preview-05-20,139.36959547276436,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"The May 2025 preview version of Gemini 2.5 Flash, a small reasoning model from Google DeepMind's Gemini 2.5 series.",
qwen-plus-2025-04-28,,2025-04-28,Alibaba,China,API access,,Unknown,Qwen Plus,,
o3-mini-2025-01-31_medium,138.58286847070045,2025-01-31,OpenAI,United States of America,API access,,Unknown,o3-mini,"A smaller version of OpenAI's o3 reasoning model, evaluated with medium reasoning effort.",o3-mini (medium)
DeepSeek-V3-0324,136.2146698374888,2025-03-24,DeepSeek,China,Open weights (restricted use),3.4078e+24,Confident,DeepSeek-V3,An updated version of the DeepSeek V3 model.,DeepSeek-V3 (Mar 2025)
gemini-2.5-pro-preview-03-25,140.81151789062466,2025-04-09,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,"A March 2025 preview version of Gemini 2.5 Pro, Google DeepMind's flagship model.",Gemini 2.5 Pro Preview (Mar 2025)
mistral-medium-2505,134.26649449623218,2025-05-07,Mistral AI,France,API access,,Unknown,Mistral Medium 3,A 2025 language model from Mistral.,
gpt-4.1-mini-2025-04-14,134.64712610557288,2025-04-14,OpenAI,United States of America,API access,,Unknown,GPT-4.1 mini,A smaller version of OpenAI's GPT-4.1.,GPT-4.1 mini
gemini-2.0-flash-001,134.60946754553157,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash,The first version of Google DeepMind's Gemini 2.0 Flash model.,Gemini 2.0 Flash (Feb 2025)
grok-3-mini-beta_low,137.49554314106257,2025-04-09,xAI,United States of America,API access,,Unknown,Grok-3 mini,"A beta release of a smaller version of XAI's third generation Grok model, evaluated with low reasoning effort.",
claude-3-7-sonnet-20250219,136.22375401642412,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,A previous flagship reasoning model from Anthropic.,
o4-mini-2025-04-16_low,,2025-04-16,OpenAI,United States of America,API access,,Unknown,o4-mini,"The latest smaller size o-series reasoning model from OpenAI, evaluated with low reasoning effort.",o4-mini (low)
gemini-2.5-flash-preview-04-17,138.5650081230874,2025-04-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"An April 2025 preview of Gemini 2.5 Flash, a small model in Google DeepMind's Gemini 2.5 series.",Gemini 2.5 Flash Preview (Apr 2025)
o3-2025-04-16_low,,2025-04-16,OpenAI,United States of America,API access,,Unknown,o3,"OpenAI's latest full-size o-series reasoning model release, evaluated on low reasoning effort.",o3 (low)
gpt-4.1-nano-2025-04-14,129.86932416954568,2025-04-14,OpenAI,United States of America,API access,,Unknown,GPT-4.1 nano,The smallest version of OpenAI's GPT-4.1.,GPT-4.1 nano
qwq-plus,,2025-04-08,Alibaba,China,API access,,Unknown,QWQ-Plus,,
Llama-4-Maverick-17B-128E-Instruct-FP8,133.55734422644446,2025-04-05,Meta AI,United States of America,Open weights (restricted use),2.244000000001e+24,Likely,Llama 4 Maverick,"The 17 billion active (400 billion total)-parameter model in Meta's Llama 4 series, quantized to FP8.",Llama 4 Maverick (FP8)
Llama-4-Scout-17B-16E-Instruct,128.87951307007526,2025-04-05,Meta AI,United States of America,Open weights (restricted use),4.08e+24,Likely,Llama 4 Scout,The 17 billion active (109 billion total)-parameter model in Meta's Llama 4 series.,
qwen-turbo-2024-11-01,,2024-11-01,Alibaba,China,API access,,Unknown,Qwen-Turbo,,
qwen-plus-2025-01-25,,2025-01-25,Alibaba,China,API access,,Unknown,Qwen Plus,,
qwen-max-2025-01-25,132.56773708560073,2025-01-25,Alibaba,China,API access,,Unknown,Qwen2.5-Max,,
Hermes-2-Theta-Llama-3-70B,,2024-06-20,"Nous Research,Arcee AI",United States of America,Open weights (restricted use),,Confident,Hermes 2 Theta Llama-3 70B,Nous Research’s fine-tuned Llama-3 70B optimized for instruction following and chat.,
gemini-2.5-pro-exp-03-25,143.06864185825884,2025-03-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,A March 2025 preview version of Gemini 2.5 Pro.,Gemini 2.5 Pro Exp (Mar 2025)
mistral-small-2501,126.74170254231626,2025-01-25,Mistral AI,France,Open weights (unrestricted),1.152e+24,Confident,Mistral Small 3,,
mistral-small-2503,127.31802733494608,2025-03-17,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral Small 3.1,"An updated version of Mistral small, a 24 billion-parameter model from Mistral.",
gemma-3-27b-it,129.63682471159188,2025-03-12,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),2.268e+24,Confident,Gemma 3 27B,An instruction-tuned version of the 27 billion-parameter model Google DeepMind's Gemma 3 series.,
claude-3-7-sonnet-20250219_32K,139.07826283189505,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 32,000 reasoning tokens allowed.",Claude 3.7 Sonnet (32k thinking)
claude-3-5-haiku-20241022,127.4540939811275,2024-10-22,Anthropic,United States of America,API access,,Unknown,Claude 3.5 Haiku,The smallest model in Anthropic’s Claude 3.5 Series. ,Claude 3.5 Haiku (Oct 2024)
gemini-1.5-flash-8b-001,,2024-10-03,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Confident,Gemini 1.5 Flash 8B,,
DeepSeek-R1-Distill-Qwen-14B,,2025-01-20,DeepSeek,China,Open weights (unrestricted),,Speculative,DeepSeek-R1-Distill-Qwen-14B,"A model based on Qwen 14B and additionally trained on outputs from DeepSeek’s R1 model, published as part of DeepSeek R1’s release. ",
DeepSeek-R1-Distill-Llama-70B,136.42484837371433,2025-01-20,DeepSeek,China,Open weights (unrestricted),,Speculative,DeepSeek-R1-Distill-Llama-70B,"A model based on LLaMA 70B and additionally trained on outputs from DeepSeek’s R1 model, published as part of DeepSeek R1’s release. ",
DeepSeek-V3,132.39011298250054,2024-12-26,DeepSeek,China,Open weights (restricted use),3.4078e+24,Confident,DeepSeek-V3,DeepSeek’s 2024 mixture-of-experts model.,
Llama-3.1-Tulu-3-70B-DPO,,2024-11-21,"Allen Institute for AI,University of Washington",United States of America,Open weights (restricted use),,Confident,Tulu 3 (Tülu 3) 70B,,
gemma-2-27b-it,122.8027053328558,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),2.106e+24,Confident,Gemma 2 27B,An instruction-optimized version of Google DeepMind's Gemma 2 27B.,
gemma-2-9b-it,119.3049222943616,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),4.32e+23,Confident,Gemma 2 9B,,
claude-2.1,,2023-11-21,Anthropic,United States of America,API access,,Unknown,Claude 2.1,An updated version in Anthropic's Claude 2 series.,
claude-2.0,119.77402787499454,2023-07-11,Anthropic,United States of America,API access,3.866e+24,Speculative,Claude 2,Anthropic's second generation Claude model.,
gemini-2.0-flash-thinking-exp-01-21,135.4386211446525,2025-01-21,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash Thinking,"A January 2025 experimental version of Gemini 2.0 Flash Thinking, a small reasoning model from Google DeepMind.",Gemini 2.0 Flash Thinking Exp
o1-preview-2024-09-12,134.67401049448446,2024-09-12,OpenAI,United States of America,API access,,Unknown,o1-preview,"The September 2024 preview version of OpenAI’s first reasoning model, o1. ",o1-preview
gemini-2.0-pro-exp-02-05,134.47185670959732,2025-02-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Hosted access (no API),,Unknown,Gemini 2.0 Pro,"A February 2025 experimental version of Google DeepMind's previous flagship model, Gemini 2.0 Pro.",Gemini 2.0 Pro Exp (Feb 2025)
o1-2024-12-17_high,140.64253480320434,2024-12-17,OpenAI,United States of America,API access,,Unknown,o1,"OpenAI's first reasoning model, evaluated with at the high reasoning effort level.",o1 (high)
gpt-4o-2024-08-06,129.0306943500137,2024-08-06,OpenAI,United States of America,API access,,Speculative,GPT-4o,"An updated version of GPT-4o, OpenAI's model that powered ChatGPT from mid-2024 to -2025.",GPT-4o (Aug 2024)
gemini-1.5-flash-002,129.7559477151026,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 1.5 Flash,The second version of Google DeepMind's Gemini 1.5 Flash.,
o1-mini-2024-09-12_high,136.81546956206145,2024-09-12,OpenAI,United States of America,API access,,Unknown,o1-mini,"A smaller version of OpenAI’s first reasoning model, o1, evaluated with high reasoning effort.",o1-mini (high)
o1-mini-2024-09-12_medium,135.37207841714974,2024-09-12,OpenAI,United States of America,API access,,Unknown,o1-mini,"A smaller version of OpenAI’s first reasoning model, o1, evaluated with medium reasoning effort.",o1-mini (medium)
claude-3-7-sonnet-20250219_16K,138.2861901117185,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 16,000 reasoning tokens allowed.",Claude 3.7 Sonnet (16k thinking)
grok-2-1212,130.29106052793486,2024-12-12,xAI,United States of America,API access,2.9599999999999996e+25,Confident,Grok-2,XAI's second generation Grok model.,
mistral-large-2411,128.47801620534253,2024-11-18,Mistral AI,France,Open weights (non-commercial),2.13e+25,Likely,Mistral Large 2,,
gpt-4.5-preview-2025-02-27,136.48917793325506,2025-02-27,OpenAI,United States of America,API access,2.1000001e+26,Likely,GPT-4.5,The largest model in OpenAI’s GPT series.,GPT-4.5 Preview (Feb 2025)
gpt-4-turbo-2024-04-09,127.63576724715188,2024-04-09,OpenAI,United States of America,API access,,Unknown,GPT-4 Turbo,"The April 2024 version of GPT-4 Turbo, OpenAI's then-flagship language model.",
o1-2024-12-17_medium,140.8330426004227,2024-12-17,OpenAI,United States of America,API access,,Unknown,o1,"OpenAI's first reasoning model, evaluated with at the medium reasoning effort level.",o1 (medium)
Meta-Llama-3-70B-Instruct,121.8105861196573,2024-04-18,Meta AI,United States of America,Open weights (restricted use),7.861e+24,Confident,Llama 3-70B,An instruction-tuned version of the 70 billion-parameter model in Meta’s LLaMA 3 series.,
gemini-1.5-pro-001,126.39171420680049,2024-05-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Speculative,Gemini 1.5 Pro,"The first of two versions of a previous flagship model from Google DeepMind, Gemini 1.5 Pro.",
Llama-3.3-70B-Instruct,127.46968167219563,2024-12-06,Meta AI,United States of America,Open weights (restricted use),6.8649768e+24,Confident,Llama 3.3 70B,An instruction-tuned version of the 70 billion-parameter model in Meta's Llama 3.3 series.,
Llama-3.2-90B-Vision-Instruct,125.44099499041587,2024-09-24,Meta AI,United States of America,Open weights (restricted use),,Confident,Llama 3.2 90B,An instruction-tuned version of the 90 billion-parameter vision model in Meta's Llama 3.3 series.,
Llama-2-70b-chat-hf,,2023-07-18,Meta AI,United States of America,Open weights (restricted use),8.1e+23,Confident,Llama 2-70B,A chat-optimized version of the 70-billion parameter model in Meta's Llama 2 series.,
gemini-1.5-pro-002,132.3347076954521,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Speculative,Gemini 1.5 Pro,"The first of two versions of a previous flagship model from Google DeepMind, Gemini 1.5 Pro.",
qwen2.5-32b-instruct,128.43410550684354,2024-09-17,Alibaba,China,Open weights (unrestricted),3.51e+24,Confident,Qwen2.5-32B,,
mistral-large-2402,,2024-02-26,Mistral AI,France,API access,1.1199999999999999e+25,Likely,Mistral Large,,
claude-3-sonnet-20240229,120.0789645713176,2024-02-29,Anthropic,United States of America,API access,,Unknown,Claude 3 Sonnet,The mid-sized model in Anthropic's Claude 3 model series.,
gemini-1.0-pro-001,117.24401323205632,2024-02-15,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Speculative,Gemini 1.0 Pro,An earlier flagship model from Google DeepMind.,
Llama-3.1-8B-Instruct,115.6180631627177,2024-07-23,Meta AI,United States of America,Open weights (restricted use),1.224e+24,Likely,Llama 3.1-8B,An instruction-tuned version of the 8 billion-parameter model in Meta’s LLaMA 3.1 series.,
Llama-3.1-405B-Instruct,128.04915861353754,2024-07-23,Meta AI,United States of America,Open weights (restricted use),3.8e+25,Confident,Llama 3.1-405B,An instruction-tuned version of the 405 billion-parameter model in Meta’s LLaMA 3.1 series.,
qwen2.5-72b-instruct,129.68976725568717,2024-09-19,Alibaba,China,Open weights (unrestricted),7.8e+24,Confident,Qwen2.5-72B,An instruction-tuned version of the 72 billion-parameter model in the Qwen 2.5 series.,
gpt-4o-2024-05-13,128.37832179007023,2024-05-13,OpenAI,United States of America,API access,,Speculative,GPT-4o,"The first version of GPT-4o, OpenAI's last multimodal model in the GPT-4 series, which powered ChatGPT from mid-2024 to -2025.",GPT-4o (May 2024)
Llama-3.1-70B-Instruct,125.5640397175186,2024-07-23,Meta AI,United States of America,Open weights (restricted use),7.929e+24,Confident,Llama 3.1-70B,"An instruction-optimized version of Llama 3.1 70B, the 70 billion-parameter model in Meta's Llama 3.1 series.",
gemini-1.5-flash-001,121.9783367693354,2024-05-23,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 1.5 Flash,The first version of Google DeepMind's Gemini 1.5 Flash.,Gemini 1.5 Flash (May 2024)
claude-3-haiku-20240307,117.86170477747896,2024-03-07,Anthropic,United States of America,API access,,Unknown,Claude 3 Haiku,The smallest model in Anthropic's Claude 3 series.,
Meta-Llama-3-8B-Instruct,116.39642238244522,2024-04-18,Meta AI,United States of America,Open weights (restricted use),7.2e+23,Confident,Llama 3-8B,An instruction-tuned version of the 8 billion-parameter model in Meta's Llama 3 series.,
claude-3-opus-20240229,126.75047134402728,2024-02-29,Anthropic,United States of America,API access,,Speculative,Claude 3 Opus,The largest model in Anthropic's Claude 3 model series.,
phi-4,130.0969230497018,2024-12-12,Microsoft Research,United States of America,Open weights (unrestricted),9.3202015e+23,Confident,Phi-4,,
mistral-large-2407,127.3629568966188,2024-07-24,Mistral AI,France,Open weights (non-commercial),2.13e+25,Likely,Mistral Large 2,,
Phi-3-medium-128k-instruct,120.40241099265413,2024-04-23,Microsoft,United States of America,Open weights (unrestricted),4.032e+23,Likely,phi-3-medium 14B,,
gpt-4-0125-preview,,2024-01-25,OpenAI,United States of America,API access,,Unknown,GPT-4 Turbo,"A January 2024 preview version of GPT-4 Turbo, OpenAI's updated GPT-4 series model.",GPT-4 Turbo Preview (January 2024)
gpt-3.5-turbo-1106,119.41346056111522,2023-11-06,OpenAI,United States of America,API access,,Speculative,GPT-3.5 Turbo,"A November 2023 preview of GPT-3.5 turbo, OpenAI's updated GPT-3.5 series model.",GPT-3.5 Turbo (Nov 2023)
gpt-4-1106-preview,,2023-11-06,OpenAI,United States of America,API access,,Unknown,GPT-4 Turbo,"A November 2023 preview of GPT-4 turbo, OpenAI's updated GPT-4 series model.",GPT-4 Turbo Preview (Nov 2023)
gpt-3.5-turbo-0125,114.22615458287927,2024-01-25,OpenAI,United States of America,API access,,Speculative,GPT-3.5 Turbo,"A January 2024 preview version of GPT-3.5 Turbo, OpenAI's updated GPT-3.5 series model.",GPT-3.5 Turbo (Jan 2024)
Yi-1.5-34B-Chat,,2024-05-13,01.AI,China,Open weights (restricted use),7.344e+23,Confident,Yi-1.5-34B,,
Yi-34B-Chat,,2023-11-22,01.AI,China,Open weights (restricted use),6.1e+23,Confident,Yi-34B,,
qwen2-72b-instruct,125.39194514919993,2024-06-07,Alibaba,China,Open weights (unrestricted),3.02e+24,Confident,Qwen2-72B,Qwen's 72 billion-parameter model in the Qwen 2 series.,
qwen1.5-72b-chat,,2024-02-04,Alibaba,China,Open weights (restricted use),1.3e+24,Confident,Qwen1.5-72B,,
qwen1.5-32b-chat,,2024-04-03,Alibaba,China,Open weights (restricted use),,Confident,Qwen1.5-32B,A chat-optimized 32 billion-parameter model in the Qwen 1.5 series.,
Mistral-7B-Instruct-v0.3,,2024-05-27,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral 7B,"An instruction-tuned version of Mistral-7B-v0.3, an updated version of their 7 billion-parameter model.",
deepseek-llm-67b-chat,,2023-11-29,DeepSeek,China,Open weights (restricted use),8.04e+23,Confident,DeepSeek LLM 67B,,
Mixtral-8x7B-Instruct-v0.1,,2023-12-11,Mistral AI,France,Open weights (unrestricted),7.74e+23,Speculative,Mixtral 8x7B,An instruction-tuned version of Mistral's 7 billion active (56 billion total)-parameter mixture-of-experts model,
WizardLM-2-8x22B,,2024-04-15,Microsoft,United States of America,Open weights (unrestricted),,Confident,WizardLM-2 8x22B,,
dbrx-instruct,,2024-03-27,Databricks,United States of America,Open weights (restricted use),2.6e+24,Confident,DBRX,,
ministral-3b-2410,,2024-10-16,Mistral AI,France,API access,,Confident,Ministral 3B,,
open-mixtral-8x22b,,2024-04-17,Mistral AI,France,Open weights (unrestricted),2.34e+24,Speculative,Mixtral 8x22B,,
ministral-8b-2410,,2024-10-16,Mistral AI,France,Open weights (non-commercial),,Confident,Ministral 8B,,
open-mixtral-8x7b,,2023-12-11,Mistral AI,France,Open weights (unrestricted),7.74e+23,Speculative,Mixtral 8x7B,,
open-mistral-7b,,2023-09-27,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral 7B,,
open-mistral-nemo-2407,,2024-07-18,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral NeMo,,
Eurus-2-7B-PRIME,,2024-12-31,"Tsinghua University,University of Illinois Urbana-Champaign (UIUC),Shanghai AI Lab,Peking University,Shanghai Jiao Tong University,CUHK Shenzhen Research Institute","United States of America,China",Open weights (unrestricted),,Speculative,Eurus-2-7B-PRIME,,
gemini-2.5-deep-think-2025-08-01-webapp,,2025-08-01,"Google,Google DeepMind","United States of America,United Kingdom of Great Britain and Northern Ireland",Hosted access (no API),,Unknown,Gemini 2.5 Deep Think,,
mpt-7b,94.2998837148508,2023-05-05,MosaicML,United States of America,Open weights (unrestricted),4.2e+22,Confident,MPT-7B,,
chatglm2-6b,,2023-06-24,,,,,,,,
internlm-7b,,2023-07-05,,,,,,,,
internlm-20b,,2023-09-18,,,,,,,,
Baichuan-2-7B-Base,94.353239677763,2023-09-20,Baichuan,China,Open weights (restricted use),1.092e+23,Confident,Baichuan 2-7B,,
Baichuan-2-13B-Base,102.51022598385993,2023-09-06,Baichuan,China,Open weights (restricted use),2.03e+23,Confident,Baichuan2-13B,,
LLaMA-7B,96.0794493642578,2023-02-24,Meta AI,United States of America,Open weights (non-commercial),4.00000001e+22,Confident,LLaMA-7B,Meta's smallest model in the original Llama series.,
LLaMA-13B,100.61949981052346,2023-02-27,Meta AI,United States of America,Open weights (non-commercial),7.8e+22,Confident,LLaMA-13B,Meta's 13 billion-parameter model in the original Llama series.,
LLaMA-33B,108.7365909074187,2023-02-27,Meta AI,United States of America,Open weights (non-commercial),2.7300000000001e+23,Confident,LLaMA-33B,Meta's 33 billion-parameter model in the original Llama series,
LLaMA-65B,111.75738942693508,2023-02-24,Meta AI,United States of America,Open weights (non-commercial),5.5e+23,Confident,LLaMA-65B,Meta's largest model in the original Llama series.,
Llama-2-7b,98.40289020295636,2023-07-18,Meta AI,United States of America,Open weights (restricted use),8.4e+22,Confident,Llama 2-7B,Meta's 7 billion-parameter model in the Llama 2 series of open source models.,
Llama-2-13b,105.34904664449547,2023-07-18,Meta AI,United States of America,Open weights (restricted use),1.6e+23,Confident,Llama 2-13B,Meta's 13 billion-parameter model in the Llama 2 series of open source models.,
Llama-2-70b-hf ,116.75423942181996,2023-07-18,Meta AI,United States of America,Open weights (restricted use),8.1e+23,Confident,Llama 2-70B,A chat-optimized of a 70 billion-parameter model in the Llama 2 series.,
StableBeluga2,117.87747867195048,2023-07-20,Stability AI,United Kingdom of Great Britain and Northern Ireland,Open weights (non-commercial),,Likely,Stable Beluga 2,,
Qwen-1_8B,,2023-11-30,,,,,,,,Qwen-1.8B
Qwen-7B,107.00693492109085,2023-09-28,Alibaba,China,Open weights (restricted use),1.01e+23,Confident,Qwen-7B,Qwen's 7 billion-parameter model in the original Qwen series.,
Qwen-14B,113.46325222769129,2023-09-28,Alibaba,China,Open weights (restricted use),2.5e+23,Confident,Qwen-14B,Qwen's 14 billion-parameter model in the original Qwen series.,
text-davinci-001,,2022-01-27,OpenAI,United States of America,API access,3.19181e+23,Confident,InstructGPT 175B,,
Gopher (280B),,2021-12-08,DeepMind,United Kingdom of Great Britain and Northern Ireland,Unreleased,6.31e+23,Confident,Gopher (280B),A 280 billion-parameter model from DeepMind in 2021.,
Megatron-Turing NLG 530B,,2022-01-28,"Microsoft,NVIDIA",United States of America,Unreleased,8.586e+23,Confident,Megatron-Turing NLG 530B,,
Chinchilla (70B),,2022-03-29,DeepMind,United Kingdom of Great Britain and Northern Ireland,Unreleased,5.76e+23,Confident,Chinchilla,,
PaLM 540B,,2022-04-04,Google Research,United States of America,Unreleased,2.5272e+24,Confident,PaLM (540B),Google's largest model in the PaLM series.,
Inflection-1,119.17230051697888,2023-06-22,Inflection AI,United States of America,Hosted access (no API),1.0001e+24,Speculative,Inflection-1,,
falcon-7b,95.11773385249916,2023-04-24,Technology Innovation Institute,United Arab Emirates,Open weights (unrestricted),6.3e+22,Confident,Falcon-7B,The 7 billion-parameter model in the Falcon series.,
falcon-40b,104.47160849817834,2023-03-15,Technology Innovation Institute,United Arab Emirates,Open weights (unrestricted),2.4e+23,Confident,Falcon-40B,The 40 billion-parameter model in the Falcon series.,
falcon-180B,114.0371979847717,2023-09-06,Technology Innovation Institute,United Arab Emirates,Open weights (restricted use),3.76e+24,Confident,Falcon-180B,The 180 billion-parameter model in the Falcon series.,
amazon.nova-pro-v1:0,125.71126032501576,2024-12-03,CMU,United States of America,API access,6.000010000000001e+24,Speculative,OpenHands + Amazon Nova Pro V1:0,,
Mistral-7B-Instruct-v0.2,,,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral 7B,,
gemma-2b,94.1800117410086,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),4.5115822e+22,Confident,Gemma 2B,Google's 2 billion-parameter model in the Gemma series of open source models.,
gemma-7b,111.3155319823652,2024-02-21,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),3.07e+23,Confident,Gemma 7B,Google's 7 billion-parameter model in the Gemma series of open source models.,
Baichuan-7B,92.50052768115052,2023-06-01,Baichuan,China,Open weights (non-commercial),5.04e+22,Confident,Baichuan1-7B,,
Phi-3.5-mini-instruct,,2024-08-16,Microsoft,United States of America,Open weights (unrestricted),3.7101154e+22,Confident,phi-3.5-mini,"An instruction-tuned version of phi-3.5-mini, a small model in Microsoft's Phi 3.5 open source model series.",
Phi-3.5-MoE-instruct,,2024-08-17,Microsoft,United States of America,Open weights (unrestricted),3.0202896e+23,Confident,Phi-3.5-MoE,,
Mistral-7B-v0.1,112.0697365203904,2023-09-27,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral 7B,,
Mistral-Nemo-Base-2407,,2024-07-18,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral NeMo,,
gemma-2-9b,,,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),4.32e+23,Confident,Gemma 2 9B,A 9 billion-parameter open source model from Google DeepMind in the Gemma 2 series.,
DeepSeek-V2,125.8075560077454,2024-05-07,DeepSeek,China,Open weights (restricted use),1.02e+24,Confident,DeepSeek-V2 (MoE-236B),,
Qwen2.5-72B,126.25379759145424,2024-09-19,Alibaba,China,Open weights (unrestricted),7.8e+24,Confident,Qwen2.5-72B,A 72 billion-parameter model in the Qwen 2.5 series.,
Llama-3.1-405B,130.29707569350853,2024-07-23,Meta AI,United States of America,Open weights (restricted use),3.8e+25,Confident,Llama 3.1-405B,The 405 billion-parameter model in Meta’s LLaMA 3.1 series.,
DeepSeek-V3-Base,,2024-12-26,DeepSeek,China,Open weights (restricted use),3.4078e+24,Confident,DeepSeek-V3,,
mpt-30b,99.17825259561889,2023-06-22,MosaicML,United States of America,Open weights (unrestricted),1.8900000000001e+23,Confident,MPT-30B,,
Llama-2-34b,106.30733408686524,2023-07-18,Meta AI,United States of America,Unreleased,4.08e+23,Confident,Llama 2-34B,Meta's 34 billion-parameter model in the Llama 2 series.,
PaLM 62B,,2022-04-04,,,,,,,,
Mistral-7B-Instruct-v0.1,,2023-09-27,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral 7B,,
Mixtral-8x7B-v0.1,118.38940805504868,2023-12-11,Mistral AI,France,Open weights (unrestricted),7.74e+23,Speculative,Mixtral 8x7B,,
Nemotron-4 15B,106.06200505514288,2024-02-26,NVIDIA,United States of America,Unreleased,7.5005116e+23,Confident,Nemotron-4 15B,A 15 billion-parameter language model trained by Nvidia.,
vicuna-13b-v1.1,,2023-04-12,,,,,,,,
opt-1.3b,,2022-05-11,Meta AI,United States of America,Open weights (non-commercial),,Confident,OPT-1.3B,,
gpt-neo-2.7B,,2023-03-30,EleutherAI,United States of America,Open weights (unrestricted),7.9e+21,Confident,GPT-Neo-2.7B,,
gpt2-xl,,2019-11-05,OpenAI,United States of America,Open weights (unrestricted),1.920000000001e+21,Speculative,GPT-2 (1.5B),"The largest version of GPT-2, OpenAI's second generation transformer-based large pretrained language model.",
xgen-7b-8k-base,93.30117087747902,2023-06-27,Salesforce,United States of America,Open weights (unrestricted),8.02e+22,Confident,XGen-7B,,
open_llama_7b,,2023-06-07,,,,,,,,
RedPajama-INCITE-7B-Base,,2023-05-04,,,,,,,,
gpt-neox-20b,,2022-04-07,EleutherAI,United States of America,Open weights (unrestricted),9.31627008e+22,Confident,GPT-NeoX-20B,,
opt-13b,,2022-05-11,,,,,,,,
gpt-j-6b,,2021-08-05,"EleutherAI,LAION","United States of America,Germany",Open weights (unrestricted),1.5e+22,Confident,GPT-J-6B,,
dolly-v2-12b,75.41611602751016,2023-04-11,Databricks,United States of America,Open weights (unrestricted),,Confident,Dolly 2.0-12b,,
Cerebras-GPT-13B,77.37679386680217,2023-03-20,Cerebras Systems,United States of America,Open weights (unrestricted),2.3e+22,Confident,Cerebras-GPT-13B,,
stablelm-tuned-alpha-7b,,2023-04-19,,,,,,,,
Yi-6B,103.48993914425256,2023-11-02,01.AI,China,Open weights (restricted use),1.26e+23,Confident,Yi 6B,,
Yi-34B,,2023-11-02,01.AI,China,Open weights (restricted use),6.1e+23,Confident,Yi-34B,,
gpt-3.5-turbo-0613,,2023-06-13,OpenAI,United States of America,API access,,Speculative,GPT-3.5 Turbo,,GPT-3.5 Turbo (June 2023)
Baichuan-13B-Base,,2023-07-11,Baichuan,China,Open weights (restricted use),9.36e+22,Confident,Baichuan 1-13B,,
Phi-3-mini-4k-instruct,119.00935770302898,2024-04-23,Microsoft,United States of America,Open weights (unrestricted),7.524e+22,Confident,phi-3-mini 3.8B,A 4 billion-parameter model in Microsoft's Phi-3 open source model series with a 4000-token context length.,
Phi-3-small-8k-instruct,121.27200742336426,2024-04-23,Microsoft,United States of America,Open weights (unrestricted),2.1312e+23,Confident,phi-3-small 7.4B,A 7 billion-parameter model in Microsoft's Phi-3 open source model series with a 8000-token context length.,
phi-2,112.08793265072175,2023-12-12,Microsoft,United States of America,Open weights (unrestricted),2.27e+22,Confident,Phi-2,,
Llama-2-13b-chat,,2023-07-18,Meta AI,United States of America,Open weights (restricted use),1.6e+23,Confident,Llama 2-13B,A chat-optimized version of Meta's 13 billion-parameter model in the Llama 2 series.,
Llama-2-70b-chat,,2023-07-18,Meta AI,United States of America,Open weights (restricted use),8.1e+23,Confident,Llama 2-70B,A chat-optimized version of Meta's 70 billion-parameter model in the Llama 2 series.,
Baichuan2-13B-Chat,,2023-09-06,,,,,,,,
Qwen-14B-Chat,,2023-09-24,Alibaba,China,Open weights (restricted use),2.5e+23,Confident,Qwen-14B,A chat-optimized version of Qwen's first-generation 14 billion-parameter model.,
internlm-chat-20b,,2023-09-17,,,,,,,,
Yi-6B-Chat,,2023-11-22,01.AI,China,Open weights (restricted use),1.26e+23,Confident,Yi 6B,,
Yi-9B,,2024-03-01,,,,,,,,
PaLM 2-S,,2023-05-17,,,,,,,,
PaLM 2-M,,2023-05-17,,,,,,,,
PaLM 2-L,,2023-05-17,,,,,,,,
Qwen2.5-Coder-0.5B,,2024-09-18,,,,,,,,
deepseek-coder-1.3b-base,,2023-11-02,"DeepSeek,Peking University",China,Open weights (restricted use),1.56e+22,Likely,DeepSeek Coder 1.3B,,
Qwen2.5-Coder-1.5B,104.22287645079513,2024-09-18,Alibaba,China,Open weights (unrestricted),5.082e+22,Confident,Qwen2.5-Coder (1.5B),Qwen's 1.5 billion-parameter model in the Qwen 2.5 Coder series of open source models.,
starcoder2-3b,,2024-02-22,"Hugging Face,ServiceNow,NVIDIA,BigCode",,Open weights (restricted use),5.94e+22,Confident,StarCoder 2 3B,,
Qwen2.5-Coder-3B,,2024-09-18,,,,,,,,
starcoder2-7b,,2024-02-20,"Hugging Face,ServiceNow,NVIDIA,BigCode",,Open weights (restricted use),1.55e+23,Confident,StarCoder 2 7B,,
deepseek-coder-6.7b-base,,2023-11-02,"DeepSeek,Peking University",China,Open weights (restricted use),8.04e+22,Likely,DeepSeek Coder 6.7B,,
DeepSeek-Coder-V2-Lite-Base,,2024-06-13,,,,,,,,
CodeQwen1.5-7B,,2024-04-15,,,,,,,,
Qwen2.5-Coder-7B,113.63585313874802,2024-09-18,Alibaba,China,Open weights (unrestricted),2.5113e+23,Confident,Qwen2.5-Coder (7B),Qwen's 7 billion-parameter model in the Qwen 2.5 Coder series of open source models.,
starcoder2-15b,,2024-02-20,"Hugging Face,ServiceNow,NVIDIA,BigCode",,Open weights (restricted use),3.87e+23,Confident,StarCoder 2 15B,,
Qwen2.5-Coder-14B,,2024-09-18,,,,,,,,
deepseek-coder-33b-base,,2023-11-02,"DeepSeek,Peking University",China,Open weights (restricted use),3.96e+23,Likely,DeepSeek Coder 33B,,
DeepSeek-Coder-V2-Base,,2024-06-17,DeepSeek,China,Open weights (restricted use),1.2852e+24,Confident,DeepSeek-Coder-V2 236B,,
Qwen2.5-Coder-32B,119.85744390996264,2024-09-18,Alibaba,China,Open weights (unrestricted),1.0725e+24,Confident,Qwen2.5-Coder (32B),Qwen's 32 billion-parameter model in the Qwen 2.5 Coder series of open source models.,
phi-1_5,92.61473308375017,2023-09-11,Microsoft,United States of America,Open weights (unrestricted),1.17e+21,Confident,Phi-1.5,,
text-davinci-002,,2022-03-15,OpenAI,United States of America,API access,2.578e+24,Speculative,GPT-3.5,,
claude-instant-1.1,,,Anthropic,United States of America,API access,,Unknown,Claude Instant,,
claude-instant-1.2,,2023-08-09,Anthropic,United States of America,API access,,Unknown,Claude Instant,,
T5-Large,,,,,,,,,,
T5-11B,,,Google,United States of America,Open weights (unrestricted),3.3e+22,Confident,T5-11B,"The largest, 11 billion-parameter version of T5, an early large language model from Google. ",
T5-3B,,,Google,United States of America,Open weights (unrestricted),9.0000000001e+21,Confident,T5-3B,"The 3 billion-parameter version of T5, an early large language model from Google. ",
Mixtral-8x22B-Instruct-v0.1,,2024-04-17,Mistral AI,France,Open weights (unrestricted),2.34e+24,Speculative,Mixtral 8x22B,An instruction-tuned version of Mistral's 8 billion active (176 billion total)-parameter mixture-of-experts model,
gemini-2.0-flash-02-05,,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash,"A February 2025 version of Gemini 2.0 Flash, a small language model from Google DeepMind.",
GLaM (MoE),,2021-12-13,Google,United States of America,Unreleased,3.6363112434e+23,Confident,GLaM,,
amazon.nova-lite-v1:0,,2024-12-03,Amazon,United States of America,API access,,Unknown,Amazon Nova Lite,,
amazon.nova-micro-v1:0,,2024-12-03,Amazon,United States of America,API access,,Unknown,Amazon Nova Micro,,
claude-1.3,,2023-04-18,Anthropic,United States of America,API access,,Unknown,Claude 1.3,An updated version of Anthropic's first generation Claude model.,
c4ai-command-r-08-2024,,2024-08-30,,,,,,,,
c4ai-command-r-plus-08-2024,,2024-08-30,"Cohere,Cohere for AI",Canada,Open weights (non-commercial),,Confident,Command R+,,
falcon-11b,,2024-05-09,Technology Innovation Institute,United Arab Emirates,Open weights (restricted use),3.6e+23,Confident,Falcon 2 11B,,Falcon 2-11B
gemini-1.5-flash-0514,,2024-05-14,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 1.5 Flash,,
gemini-2.0-flash-exp,130.48137460625426,2024-12-11,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash,"A December 2024 experimental version of Gemini 2.0 Flash, a small langauge model from Google DeepMind.",
gpt-4-turbo,,2023-11-06,OpenAI,United States of America,API access,,Unknown,GPT-4 Turbo,An updated version of GPT-4 and OpenAI's then-new flagship language model.,
Llama-3.2-11B-Vision-Instruct,,2024-09-24,Meta AI,United States of America,Open weights (restricted use),5.79e+23,Confident,Llama 3.2 11B,An 11 billion-parameter instruction-tuned vision language model in Meta's Llama 3.2 series.,
mistral-small-2402,,2024-02-26,,,,,,,,
Mixtral-8x22B-v0.1,,2024-04-17,Mistral AI,France,Open weights (unrestricted),2.34e+24,Speculative,Mixtral 8x22B,,
Qwen1.5-7B,,2024-02-04,Alibaba,China,Open weights (unrestricted),1.68e+23,Confident,Qwen1.5-7B,A 7 billion-parameter model in the Qwen 1.5 series.,
qwen1.5-14B,,2024-02-04,Alibaba,China,Open weights (unrestricted),3.36e+23,Confident,Qwen1.5-14B,A 14 billion-parameter model in the Qwen 1.5 series.,
qwen1.5-32B,,2024-02-04,Alibaba,China,Open weights (restricted use),,Confident,Qwen1.5-32B,A 32 billion-parameter model in the Qwen 1.5 series.,
qwen2.5-14b-instruct,,2025-02-26,Alibaba,China,Open weights (unrestricted),1.58760000000001e+24,Confident,Qwen2.5-14B,An instruction-tuned version of Qwen 2.5 14b.,
qwen2.5-7b-instruct,,2025-02-26,Alibaba,China,Open weights (unrestricted),8.2188e+23,Confident,Qwen2.5-7B,An instruction-tuned version of Qwen 2.5 7b.,
Yi-large,,2024-05-13,01.AI,China,API access,1.8e+24,Speculative,Yi-Large,,
claude-sonnet-4-5-20250929_2K,,2025-09-29,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4.5,,Claude Sonnet 4.5 (2k thinking)
gpt-5-2025-08-07_low,,2025-08-07,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,"OpenAI's latest flagship frontier model, benchmarked using low reasoning effort.",GPT-5 (low)
claude-opus-4-20250514_2K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 2,000 reasoning tokens.",
gpt-5-2025-08-07_minimal,,2025-08-07,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,"OpenAI's latest flagship frontier model, benchmarked using minimal reasoning effort.",GPT-5 (minimal)
claude-sonnet-4-20250514_2K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,"Anthropic's mid-sized Claude 4 series model, evaluated at up to 2,000 reasoning tokens.",
claude-3-7-sonnet-20250219_2K,,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 2,000 reasoning tokens allowed.",Claude 3.7 Sonnet (2k thinking)
o3-pro-2025-06-10_high,,2025-06-10,OpenAI,United States of America,,,Unknown,o3-pro,"An enhanced version of OpenAI's previous flagship reasoning model o3, evaluated with high reasoning effort.",
claude-opus-4-20250514_8K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 8,000 reasoning tokens.",
gemini-2.5-pro-preview-06-05_1K,,2025-06-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,"A June 2025 preview version of Gemini 2.5 Pro, Google DeepMind's flagship frontier model, evaluated with up to 1,000 reasoning tokens.",Gemini 2.5 Pro Preview (Jun 2025)
gemini-2.5-flash-preview-04-17 (24K thinking),,2025-04-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"An April 2025 preview version of Gemini 2.5 Flash, a small reasoning model in the Gemini 2.5 series from DeepMind, evaluated with up to 24,000 reasoning tokens.",
gemini-2.5-flash-preview-05-20_1K,,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"An April 2025 preview version of Gemini 2.5 Flash, a small reasoning model in the Gemini 2.5 series from DeepMind, evaluated with up to 1,000 reasoning tokens.",
gemini-2.5-flash-preview-05-20_8K,,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"An April 2025 preview version of Gemini 2.5 Flash, a small reasoning model in the Gemini 2.5 series from DeepMind, evaluated with up to 8,000 reasoning tokens.",
claude-sonnet-4-20250514_8K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,,
o3-pro-2025-06-10_low,,2025-06-10,OpenAI,United States of America,,,Unknown,o3-pro,"An enhanced version of OpenAI's previous flagship reasoning model o3, evaluated with low reasoning effort.",
gemini-2.5-flash-preview-05-20_16K,,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"An April 2025 preview version of Gemini 2.5 Flash, a small reasoning model in the Gemini 2.5 series from DeepMind, evaluated with up to 16,000 reasoning tokens.",
o3-pro-2025-06-10_medium,,2025-06-10,OpenAI,United States of America,,,Unknown,o3-pro,"An enhanced version of OpenAI's previous flagship reasoning model o3, evaluated with medium reasoning effort.",
codex-mini-2025-05-16,,2025-05-16,,,,,,,,
o1-pro-2025-03-19_low,,2025-03-19,OpenAI,United States of America,API access,,Unknown,o1,"An An enhanced version of OpenAI's first reasoning model,o1, evaluated with low reasoning effort.",
claude-3-7-sonnet-20250219_8K,,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 9,000 reasoning tokens allowed.",Claude 3.7 Sonnet (8k thinking)
claude-sonnet-4-20250514_1K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,"Anthropic's mid-sized Claude 4 series model, evaluated at up to 1,000 reasoning tokens.",
o1-2024-12-17_low,,2024-12-17,OpenAI,United States of America,API access,,Unknown,o1,"OpenAI's first reasoning model, evaluated with at the low reasoning tokens level.",o1 (low)
claude-3-7-sonnet-20250219_1K,,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 1,000 reasoning tokens allowed.",Claude 3.7 Sonnet (1k thinking)
Llama-4-Maverick-17B-128E-Instruct,129.36617561800267,2025-04-05,Meta AI,United States of America,Open weights (restricted use),2.244000000001e+24,Likely,Llama 4 Maverick,The 17 billion active (400 billion total)-parameter model in Meta's Llama 4 series.,Llama 4 Maverick
o3-mini-2025-01-31_low,,2025-01-31,OpenAI,United States of America,API access,,Unknown,o3-mini,"A smaller version of OpenAI's o3 reasoning model, evaluated with low reasoning effort.",
claude-opus-4-20250514_1K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 1,000 reasoning tokens.",
grok-3,,2025-04-09,xAI,United States of America,API access,3.5000000000000006e+26,Likely,Grok 3,XAI's third generation flagship model.,
gemini-2.5-pro-preview-06-05_32K,,,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Pro,"A June 2025 preview version of Gemini 2.5 Pro, Google DeepMind's flagship frontier model, evaluated with up to 32,000 reasoning tokens.",Gemini 2.5 Pro Preview (Jun 2025)
claude-opus-4-20250514_32K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 32,000 reasoning tokens.",
gemini-2.5-flash-preview-05-20_23K,,2025-05-20,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"A May 2025 preview version of Gemini 2.5 Flash, a small model in Google DeepMind's Gemini 2.5 series, evaluated with up to 23,000 reasoning tokens.",
chatgpt-4o-03-27-2025,,2025-03-27,OpenAI,United States of America,API access,,Speculative,GPT-4o,"A version of GPT-4o that was behind the ChatGPT interface, released in March 2025.",
Qwen3-32B,,2025-04-29,Alibaba,China,Open weights (unrestricted),7.0848e+24,Confident,Qwen3-32B,The 32 billion-parameter model in the Qwen 3 series.,
gemini-exp-1206,,2024-12-06,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash,"A December 2024 experimental version of Gemini 2.0 Flash, a small langauge model from Google DeepMind.",
chatgpt-4o-01-29-2025,,2025-01-29,OpenAI,United States of America,API access,,Speculative,GPT-4o,"A version of GPT-4o that was behind the ChatGPT interface, released in January 2025.",
QwQ-32B,,2025-03-05,Alibaba,China,Open weights (unrestricted),3.51e+24,Speculative,QwQ-32B,,
DeepSeek-V2.5,,2024-09-06,DeepSeek,China,Open weights (restricted use),1.7892e+24,Confident,DeepSeek-V2.5,,
Qwen2.5-Coder-32B-Instruct,,2024-11-21,Alibaba,China,Open weights (unrestricted),1.0725e+24,Confident,Qwen2.5-Coder (32B),An instruction-tuned and coding-optimized 32 billion-parameter model in the Qwen 2.5 series.,
yi-lightning,,2024-12-02,01.AI,China,API access,1.5e+24,Confident,Yi-Lightning,,
c4ai-command-a-03-2025,,2025-03-13,Cohere,Canada,Open weights (non-commercial),,Confident,Cohere Command A,,
codestral-2501,,2025-01-13,Mistral AI,France,Open weights (non-commercial),,Confident,Codestral,A January 2025 version of a 2024 coding-optimized model from Mistral.,
openhands-lm-32b-v0.1,,2024-03-26,,,,,,,,
text-davinci-003,,2022-11-28,OpenAI,United States of America,API access,2.578e+24,Speculative,GPT-3.5,,
gpt-4-32k-0314,,2023-03-14,OpenAI,United States of America,API access,2.1e+25,Likely,GPT-4,,
opt-175b,,2022-05-02,Meta AI,United States of America,Open weights (non-commercial),4.3e+23,Confident,OPT-175B,Facebook's 66 billion-parameter model in the OPT series.,
davinci,,,OpenAI,United States of America,API access,3.14e+23,Confident,GPT-3 175B (davinci),,
opt-66b,,2022-05-03,Meta AI,United States of America,Open weights (non-commercial),1.100000000001e+23,Confident,OPT-66B,Facebook's 66 billion-parameter model in the OPT series.,
bloom,,2022-07-06,"Hugging Face,BigScience","United States of America,France",Open weights (restricted use),3.65664e+23,Confident,BLOOM-176B,,
curie,,,OpenAI,United States of America,,1.2e+22,Confident,GPT-3 6.7B,,
text-curie-001,,,OpenAI,United States of America,API access,,Confident,InstructGPT 6B,,
text-babbage-001,,,OpenAI,United States of America,API access,,Confident,InstructGPT 1.3B,,
babbage,,,OpenAI,United States of America,,2.38e+21,Confident,GPT-3 XL,,
ada,,,OpenAI,United States of America,,6.41e+20,Confident,GPT-3 Medium,,
text-ada-001,,,OpenAI,United States of America,,,Confident,InstructGPT 350M,,
ml-elephant,,,,,,,,,,Zoo Ml-Ephant
QwQ-32B-Preview-Q8_0-GGUF,,2024-12-27,Alibaba,China,Open weights (unrestricted),3.51e+24,Speculative,QwQ-32B,,
llama3.3:70b-instruct-q8_0,,2024-12-06,Meta AI,United States of America,Open weights (restricted use),6.8649768e+24,Confident,Llama 3.3 70B,,
llama3.1:405b-instruct-q4_K_M,,2024-07-23,Meta AI,United States of America,Open weights (restricted use),3.8e+25,Confident,Llama 3.1-405B,,
phi4:14b-q8_0,,2025-02-26,Microsoft Research,United States of America,Open weights (unrestricted),9.3202015e+23,Confident,Phi-4,,
llama3.1:8b-instruct-q8_0,,2024-07-23,Meta AI,United States of America,Open weights (restricted use),1.224e+24,Likely,Llama 3.1-8B,,
gemma2:27b-instruct-q8_0,,2024-06-27,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),2.106e+24,Confident,Gemma 2 27B,,
gemini-2.5-flash-preview-04-17 (16K thinking),,2025-04-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,"An April 2025 preview version of Gemini 2.5 Flash, a small reasoning model in the Gemini 2.5 series from DeepMind, evaluated with up to 16,000 reasoning tokens.",
Qwen3-30B-A3B,,2025-04-29,Alibaba,China,Open weights (unrestricted),6.48e+23,Likely,Qwen3-30B-A3B,,
Meta-Llama-3-8B,,2024-04-18,Meta AI,United States of America,Open weights (restricted use),7.2e+23,Confident,Llama 3-8B,Meta's 8 billion-parameter model in the Llama 3 series.,
Meta-Llama-3-70B,,2024-04-18,Meta AI,United States of America,Open weights (restricted use),7.861e+24,Confident,Llama 3-70B,Meta's 70 billion-parameter model in the Llama 3 series.,
reka-flash-3,,2025-03-10,Reka AI,United States of America,Open weights (unrestricted),,Confident,Reka Flash 3,,
DeepSeek-R1-Distill-Qwen-32B,,2025-01-20,DeepSeek,China,Open weights (unrestricted),,Speculative,DeepSeek-R1-Distill-Qwen-32B,"A model based on Qwen 32B and additionally trained on outputs from DeepSeek’s R1 model, published as part of DeepSeek R1’s release. ",
Mistral-Nemo-Instruct-2407,,2024-07-18,Mistral AI,France,Open weights (unrestricted),,Confident,Mistral NeMo,,
Qwen2-VL-72B-Instruct,,2024-08-29,,,,,,,,
Llama-3.2-3B-Instruct,,2024-09-24,Meta AI,United States of America,Open weights (restricted use),1.7334e+23,Confident,Llama 3.2 3B,A 3 billion-parameter instruction-tuned model in Meta's Llama 3.2 series.,
Llama-3.2-1B-Instruct,,2024-09-24,Meta AI,United States of America,Open weights (restricted use),6.642e+22,Confident,Llama 3.2 1B,A 1 billion-parameter instruction-tuned model in Meta's Llama 3.2 series.,
Qwen2-VL-7B-Instruct,,2024-08-29,,,,,,,,
gemini-2.5-flash,,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,A small model from Google DeepMind's Gemini 2.5 series.,
claude-opus-4-20250514_12K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Opus 4,"Anthropic's largest model in the Claude 4 series (superseded by Claude Opus 4.1), benchmarked with up to 12,000 reasoning tokens.",
claude-sonnet-4-5-20250929_12K,,2025-09-29,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4.5,,Claude Sonnet 4.5 (12k thinking)
claude-3-7-sonnet-20250219_12K,,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 12,000 reasoning tokens allowed.",Claude 3.7 Sonnet (12k thinking)
claude-sonnet-4-20250514_12K,,2025-05-22,Anthropic,United States of America,API access,,Unknown,Claude Sonnet 4,"Anthropic's mid-sized Claude 4 series model, evaluated at up to 12,000 reasoning tokens.",
gpt-oss-120b,,2025-08-05,OpenAI,United States of America,Open weights (unrestricted),4.94e+24,Confident,gpt-oss-120b,The larger one of OpenAI's two latest open source language models.,
glm-4.6,,2025-09-30,"Zhipu AI,Tsinghua University",China,Open weights (unrestricted),4.42e+24,Likely,GLM 4.6,,
GLM-4.5-Air,,2025-07-20,,,,,,,,
Qwen3-Coder-480B-A35B-Instruct,,2025-07-31,Alibaba,China,Open weights (unrestricted),1.575e+24,Confident,Qwen3-Coder-480B-A35B,,
DeepSeek-V3.1_thinking,,2025-08-21,DeepSeek,China,Open weights (unrestricted),3.594058e+24,Confident,DeepSeek-V3.1,,
Qwen3-235B-A22B-Instruct-2507,,2025-07-25,Alibaba,China,Open weights (unrestricted),4.752e+24,Likely,Qwen3-235B-A22B,A 22 billion active (235 billion total)-parameter mixture-of-experts instruction-tuned model in the Qwen 3 series.,
qwen3-coder-plus,,2025-09-23,,,,,,,,
grok-code-fast-1,,2025-08-28,,,,,,,,
MiniMax-M1-80k,,2025-06-13,MiniMax,China,Open weights (unrestricted),4.3240062e+24,Likely,MiniMax-M1-80k,,
qwen2.5-max,,2025-01-28,Alibaba,China,API access,,Unknown,Qwen2.5-Max,The largest model in the Qwen 2.5 series.,
computer-use-preview-2025-03-11,,2025-03-11,,,,,,,,
Qwen2.5-VL-72B-Instruct,129.99451863697354,2024-09-19,Alibaba,China,Open weights (unrestricted),7.8e+24,Confident,Qwen2.5-72B,A 72 billion-parameter instruction-tuned vision language model in the Qwen 2.5 series.,
claude-3-7-sonnet-20250219_15K,,2025-02-24,Anthropic,United States of America,API access,3.3499999999999998e+25,Likely,Claude 3.7 Sonnet,"A previous flagship reasoning model from Anthropic, benchmarked with up to 15,000 reasoning tokens allowed.",Claude 3.7 Sonnet (15k thinking)
Pixtral-12B-2409,,2024-09-17,Mistral AI,France,Open weights (unrestricted),,Confident,Pixtral 12B,,
gpt-5-codex,,2025-09-15,OpenAI,United States of America,API access,6.6e+25,Speculative,GPT-5,,
glaive-swe-v1,,,,,,,,,,
grok-4-fast,,2025-09-19,xAI,United States of America,API access,,Unknown,Grok 4 Fast,,
mpt-30b-instruct,,2023-06-22,MosaicML,United States of America,Open weights (unrestricted),1.8900000000001e+23,Confident,MPT-30B,,
vicuna-13b-v1.3,,2023-06-18,"Large Model Systems Organization,University of California (UC) Berkeley",United States of America,Open weights (restricted use),,Confident,Vicuna-13B-v1.3,,
T5-Small,,,,,,,,,,
T5-Base,,,,,,,,,,
Switch-Base,,,,,,,,,,
Switch-Large,,,,,,,,,,
claude-opus-4-1-20250805_32K,,2025-08-05,Anthropic,United States of America,API access,,Unknown,Claude Opus 4.1,"Anthropic’s largest flagship model, benchmarked with up to 32,000 reasoning tokens.",
davinci-002,,,OpenAI,United States of America,API access,3.14e+23,Confident,GPT-3 175B (davinci),,
gpt-3.5-turbo-instruct,,2023-09-18,OpenAI,United States of America,API access,,Speculative,GPT-3.5 Turbo,An instruction-tuned version of OpenAI's GPT-3.5 turbo.,
code-davinci-002,,,OpenAI,United States of America,API access,2.578e+24,Speculative,GPT-3.5,,
falcon-40b-instruct,,2023-05-25,Technology Innovation Institute,United Arab Emirates,Open weights (unrestricted),2.4e+23,Confident,Falcon-40B,,
DeepSeek-Coder-V2-Lite-Instruct,,2024-06-13,,,,,,,,
DeepSeek-Coder-V2-Instruct,,2024-06-17,DeepSeek,China,Open weights (restricted use),1.2852e+24,Confident,DeepSeek-Coder-V2 236B,"An instruction-tuned version of DeepSeek Coder V2, a 236 billion-parameter coding-optimized model from DeepSeek.",
Qwen2.5-Coder-3B-Instruct,,2024-11-06,,,,,,,,
Qwen2.5-Coder-7B-Instruct,,2024-09-17,Alibaba,China,Open weights (unrestricted),2.5113e+23,Confident,Qwen2.5-Coder (7B),An instruction-tuned and coding-optimized 7 billion-parameter model in the Qwen 2.5 series.,
Qwen2.5-Coder-14B-Instruct,,2024-11-06,,,,,,,,
QwQ-32B (16K thinking),,2025-03-05,Alibaba,China,Open weights (unrestricted),3.51e+24,Speculative,QwQ-32B,,
gemini-2.5-flash-preview-09-2025,,2025-09-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash,,
gemini-robotics-er-1.5-preview,,2025-09-26,,,,,,,,
gemini-2.5-flash-lite-preview-09-2025,,2025-09-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash-Lite,,
CUA,,2025-01-23,OpenAI,United States of America,Hosted access (no API),,Unknown,Computer-Using Agent (CUA),,
gpt-4-1106-vision-preview,,2023-11-06,OpenAI,United States of America,API access,,Unknown,GPT-4V,,
gemini-2.0-flash-lite,,2025-02-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash-Lite,The smallest model in Google DeepMind's Gemini 2.0 series.,
gemini-2.0-flash-lite-preview-02-05,,2025-02-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.0 Flash-Lite,A February 2025 preview version of the smallest model in Google DeepMind's Gemini 2.0 series.,
Dracarys2-72B-Instruct,,2024-09-30,,,,,,,,
learnlm-1.5-pro-experimental,,,,,,,,,,
sonar,,,,,,,,,,Perplexity Sonar
Dracarys2-Llama-3.1-70B-Instruct,,2024-08-14,,,,,,,,
QwQ-32B-Preview,,2024-11-28,Alibaba,China,Open weights (unrestricted),3.51e+24,Speculative,QwQ-32B,,
OLMo-2-1124-13B-Instruct,,2024-12-31,"Allen Institute for AI,University of Washington,New York University (NYU)",United States of America,Open weights (unrestricted),4.6e+23,Confident,OLMo 2 Furious 13B,,
blip2-opt-2.7b,,2023-02-06,Salesforce Research,United States of America,Open weights (unrestricted),1.20000000001e+21,Confident,BLIP-2 (Q-Former),,
Phi-3.5-vision-instruct,,2024-08-16,,,,,,,,
MM1-3B-Chat,,2024-03-14,,,,,,,,
MM1-7B-Chat,,2024-03-14,,,,,,,,
llava-v1.6-vicuna-7b,,2024-01-31,,,,,,,,
llama3-llava-next-8b,,2024-04-20,,,,,,,,
Qwen-VL-Chat,,2023-08-20,,,,,,,,
gemini-1.0-pro-vision,,2024-01-04,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 1.0 Pro Vision,A vision language model from Google DeepMind.,
falcon-11B-vlm,,2024-05-21,,,,,,,,
llava-v1.6-vicuna-13b,,2024-01-31,,,,,,,,
llava-v1.6-mistral-7b,,2024-01-31,,,,,,,,
instructblip-vicuna-7b,,2023-05-22,,,,,,,,
instructblip-vicuna-13b,,2023-12-25,,,,,,,,
InternVL-Chat-ViT-6B-Vicuna-7B,,2023-12-25,,,,,,,,
InternVL-Chat-ViT-6B-Vicuna-13B,,2024-08-16,,,,,,,,
llava-v1.5-7b,,2023-10-05,,,,,,,,
chutes/QwQ-32B,,2025-03-05,Alibaba,China,Open weights (unrestricted),3.51e+24,Speculative,QwQ-32B,,
chutes/Qwen3-235B-A22B,,2025-04-28,Alibaba,China,Open weights (unrestricted),4.752e+24,Likely,Qwen3-235B-A22B,,
chutes/Qwen3-32B,,2025-04-28,Alibaba,China,Open weights (unrestricted),7.0848e+24,Confident,Qwen3-32B,,
chutes/Qwen3-30B-A3B,,2025-04-28,Alibaba,China,Open weights (unrestricted),6.48e+23,Likely,Qwen3-30B-A3B,,
chutes/Qwen3-14B,,2025-04-28,Alibaba,China,Open weights (unrestricted),3.1968e+24,Confident,Qwen3-14B,,
chutes/Qwen3-8B,,2025-04-28,Alibaba,China,Open weights (unrestricted),1.7712e+24,Confident,Qwen3-8B,,
grok-3-mini-beta_medium,,2025-04-09,xAI,United States of America,API access,,Unknown,Grok-3 mini,"A beta release of a smaller version of XAI's third generation Grok model, evaluated with medium reasoning effort.",
chutes/DeepSeek-V3-0324,,2025-03-24,DeepSeek,China,Open weights (restricted use),3.4078e+24,Confident,DeepSeek-V3,,DeepSeek-V3 (Mar 2025)
chutes/Gemma-3-27b-It,,2025-03-11,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",Open weights (restricted use),2.268e+24,Confident,Gemma 3 27B,,
chutes/Llama-4-Maverick-17B-128E-Instruct,,2025-04-06,Meta AI,United States of America,Open weights (restricted use),2.244000000001e+24,Likely,Llama 4 Maverick,,
chutes/Llama-4-Scout-17B-16E Instruct,,2025-04-06,Meta AI,United States of America,Open weights (restricted use),4.08e+24,Likely,Llama 4 Scout,,
chutes/DeepSeek-R1-0528,,2025-05-28,DeepSeek,China,Open weights (unrestricted),4.020010000000001e+24,Confident,DeepSeek-R1,,DeepSeek-R1 (May 2025)
gemini-2.5-flash-lite-preview-06-17-thinking,,2025-06-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",API access,,Unknown,Gemini 2.5 Flash-Lite,A June 2025 preview of Gemini 2.5 Flash Lite.,gemini-2.5-flash-lite-preview-06-17 (Default thinking length)
chutes/GLM-4.5-FP8,,2025-07-27,,,,,,,,
chutes/Qwen3-235B-A22B-Thinking-2507,,2025-07-25,Alibaba,China,Open weights (unrestricted),4.752e+24,Likely,Qwen3-235B-A22B,,
chutes/gpt-oss-120b_high,,2025-08-05,OpenAI,United States of America,Open weights (unrestricted),4.94e+24,Confident,gpt-oss-120b,,
chutes/Qwen3-Next-80B-A3B-Instruct,,2025-09-11,,,,,,,,
chutes/gpt-oss-120b,,2025-08-05,OpenAI,United States of America,Open weights (unrestricted),4.94e+24,Confident,gpt-oss-120b,,
DeepSeek-V3.2-Exp,,2025-09-29,,,,,,,,
fireworks/Kimi-K2-Instruct-0905,,2025-09-05,Moonshot,China,Open weights (restricted use),2.976e+24,Confident,Kimi K2,,
parasail-qwen3-235b-a22b-instruct-2507,,2025-09-01,Alibaba,China,Open weights (unrestricted),4.752e+24,Likely,Qwen3-235B-A22B,,
nvidia-nemotron-nano-9b-v2,,2025-08-18,,,,,,,,
deepinfra/Qwen3-Next-80B-A3B-Instruct,,,,,,,,,,
