Model version,Score,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Name,Shots,Source,Source link,Notes,id
text-davinci-001,0.529,2022-01-27,OpenAI,United States of America,3.19181e+23,"""training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)""

60/3640 = +1.65% to base model compute

base model was reported 3.14e+23 FLOP

3.14e+23 * 1.0165 = 319181000000000000000000",GPT-3,,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,https://arxiv.org/pdf/2201.05320,,rec2N4ugHyIuFFHCg
T5-Large,0.546,,,,,,T5-Large,,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,https://arxiv.org/pdf/2201.05320,,recXRfZ560YNIjqvj
,0.549,,,,,,Unicorn-Large,,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,https://arxiv.org/pdf/2201.05320,,recxLFYG3VFl6Zp2H
T5-11B,0.678,,Google,United States of America,3.3e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4, 4.05e22

update: 3.3e22 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf

6ND rule suggests somewhat more FLOPs:
6 * 1T * 11B = 6.6e22",T5-11B,,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,https://arxiv.org/pdf/2201.05320,,recjb7oltbflJgcnj
,0.702,,,,,,Unicorn-11B,,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,https://arxiv.org/pdf/2201.05320,,rece5VQKCulKYKcIX
,0.549,,,,,,Unicorn 770M,,Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification,https://arxiv.org/abs/2305.05921,,recQ5nmRkgX6o08i1
T5-3B,0.602,,Google,United States of America,9.0000000001e+21,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 â‰ˆ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5

update: 9.0E+21 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf",T5-3B,,Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification,https://arxiv.org/abs/2305.05921,,recDaVPKR6YwbKmLZ
,0.618,,,,,,RACo,,Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification,https://arxiv.org/abs/2305.05921,,rec4OHB4cwgb9XLmT
gpt-3.5-turbo-0613,0.57,2023-06-13,OpenAI,United States of America,,,GPT-3.5 Turbo,few,,https://arxiv.org/pdf/2309.13165v1,Assuming original 3.5-turbo release,recLJTHqISFdhqJwy
Llama-2-70b-hf ,0.5,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",LLama-70B,few,,https://arxiv.org/pdf/2309.13165v1,"Assuming LLAMA-70b was LLAMA 2
",recdvV2jdEN8V6o76
