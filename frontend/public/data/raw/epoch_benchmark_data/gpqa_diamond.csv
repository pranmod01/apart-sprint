Model version,Best score (across scorers),Release date,Organization,Country,Training compute (FLOP),Training compute notes,stderr,Log viewer,Logs,Started at,id
gpt-5-nano-2025-08-07_high,0.6944444444444444,2025-08-07,OpenAI,United States of America,,,0.027594508998251045,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F2pSDjMSGDh4HsAxkqUuNQP.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/2pSDjMSGDh4HsAxkqUuNQP.eval,2025-10-30T16:20:27.447Z,2pSDjMSGDh4HsAxkqUuNQP
gpt-5-mini-2025-08-07_high,0.75,2025-08-07,OpenAI,United States of America,,,0.022767295427113803,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FjP8EyLtPFsakgnsHe2oJM6.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/jP8EyLtPFsakgnsHe2oJM6.eval,2025-10-30T09:59:56.459Z,jP8EyLtPFsakgnsHe2oJM6
gpt-5-2025-08-07_high,0.8617424242424242,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.020787836099851995,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9nPyNrZxwtoT7eS4DZbQF6.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9nPyNrZxwtoT7eS4DZbQF6.eval,2025-10-29T22:29:36.692Z,9nPyNrZxwtoT7eS4DZbQF6
claude-sonnet-4-5-20250929_16K,0.7878787878787878,2025-09-29,Anthropic,United States of America,,,0.02912652283458678,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FLkVuUxvsHNzpcg3sUiH7f3.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/LkVuUxvsHNzpcg3sUiH7f3.eval,2025-10-28T13:46:39.393Z,LkVuUxvsHNzpcg3sUiH7f3
claude-sonnet-4-5-20250929_59K,0.8232323232323232,,Anthropic,United States of America,,,0.027178752639044908,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FXaGuWDxPtqVvyxqdMncKFj.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/XaGuWDxPtqVvyxqdMncKFj.eval,2025-10-28T13:46:37.314Z,XaGuWDxPtqVvyxqdMncKFj
gpt-4-0314,0.3573232323232323,2023-03-14,OpenAI,United States of America,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.02363099375599328,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FFkbLkW5ed8rnZGFJMevDFn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/FkbLkW5ed8rnZGFJMevDFn.eval,2025-10-23T12:16:00.615Z,FkbLkW5ed8rnZGFJMevDFn
claude-haiku-4-5-20251001_32K,0.7121212121212122,2025-10-15,Anthropic,United States of America,,,0.03225883512300997,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FGEaAdb9UmoUFbEtBx7WzAR.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/GEaAdb9UmoUFbEtBx7WzAR.eval,2025-10-22T09:43:52.881Z,GEaAdb9UmoUFbEtBx7WzAR
claude-sonnet-4-5-20250929_32K,0.817258883248731,2025-09-29,Anthropic,United States of America,,,0.027603867020557334,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fii6JE9j57eSbuswEsJArTa.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ii6JE9j57eSbuswEsJArTa.eval,2025-10-21T15:15:58.761Z,ii6JE9j57eSbuswEsJArTa
claude-haiku-4-5-20251001,0.6047979797979798,2025-10-15,Anthropic,United States of America,,,0.028154535023020826,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FcgnopvpB6mzsfb9yGnYK2z.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/cgnopvpB6mzsfb9yGnYK2z.eval,2025-10-16T11:58:17.749Z,cgnopvpB6mzsfb9yGnYK2z
qwen3-max-2025-09-23,0.726010101010101,2025-09-24,Alibaba,China,1.512e+25,"6ND with:
36T tokens is taken from the qwen3 technical report
70B active params is based on it having >1T params, and the architectures of Qwen3-235B-A22B and Qwen3-Coder-480B-A35B",0.02734528103171235,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F46RCssznDADYeg8MuSgvsj.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/46RCssznDADYeg8MuSgvsj.eval,2025-10-06T16:27:30.400Z,46RCssznDADYeg8MuSgvsj
claude-sonnet-4-5-20250929,0.7373737373737373,2025-09-29,Anthropic,United States of America,,,0.031353050095330834,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfsZmzw8jjyUgMQGwxodBwN.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fsZmzw8jjyUgMQGwxodBwN.eval,2025-09-29T18:04:57.033Z,fsZmzw8jjyUgMQGwxodBwN
gpt-5-mini-2025-08-07_medium,0.7165404040404041,2025-08-07,OpenAI,United States of America,,,0.023192741878083397,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FCFNzodoTSVVVgp9PUXftUP.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/CFNzodoTSVVVgp9PUXftUP.eval,2025-08-07T19:34:31.180Z,CFNzodoTSVVVgp9PUXftUP
gpt-5-2025-08-07_medium,0.8535353535353535,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.021314825297984473,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FYQCwJRoGbGAqNCQoaL6cjk.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/YQCwJRoGbGAqNCQoaL6cjk.eval,2025-08-07T19:34:30.433Z,YQCwJRoGbGAqNCQoaL6cjk
gpt-5-nano-2025-08-07_medium,0.6742424242424242,2025-08-07,OpenAI,United States of America,,,0.026607011205052013,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FELAxkvAigvEVdMmhxiaMiy.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ELAxkvAigvEVdMmhxiaMiy.eval,2025-08-07T19:34:29.562Z,ELAxkvAigvEVdMmhxiaMiy
Kimi-K2-Instruct,0.5460858585858586,2025-07-12,Moonshot,China,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,0.029320262300622887,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fcc9xuQC7QDp2VjwnqGWmn9.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/cc9xuQC7QDp2VjwnqGWmn9.eval,2025-08-07T18:45:53.203Z,cc9xuQC7QDp2VjwnqGWmn9
claude-opus-4-1-20250805_27K,0.7676767676767676,2025-08-05,Anthropic,United States of America,,,0.030088629490217445,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F3obcsGCfuwYeEm9wRyqzPg.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/3obcsGCfuwYeEm9wRyqzPg.eval,2025-08-05T16:53:28.370Z,3obcsGCfuwYeEm9wRyqzPg
claude-opus-4-1-20250805_16K,0.7727272727272727,2025-08-05,Anthropic,United States of America,,,0.029857515673386438,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FerLgyKQs2mg2memE9SJvK5.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/erLgyKQs2mg2memE9SJvK5.eval,2025-08-05T16:51:47.698Z,erLgyKQs2mg2memE9SJvK5
claude-opus-4-1-20250805,0.7323232323232324,2025-08-05,Anthropic,United States of America,,,0.031544498882702825,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fj62bkFDjBSeFyVpnGzuzg4.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/j62bkFDjBSeFyVpnGzuzg4.eval,2025-08-05T16:45:23.948Z,j62bkFDjBSeFyVpnGzuzg4
magistral-small-2506,0.4842171717171717,2025-06-10,Mistral AI,France,,,0.022642414016982325,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F5hAnRpZh5ZZubbPgrrcm6D.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/5hAnRpZh5ZZubbPgrrcm6D.eval,2025-06-10T14:53:14.042Z,5hAnRpZh5ZZubbPgrrcm6D
gemini-2.5-pro-preview-06-05,0.8484848484848485,2025-06-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02554565042660364,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQ4qTzHnjLyiCCRvNpXioUq.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Q4qTzHnjLyiCCRvNpXioUq.eval,2025-06-05T16:07:50.021Z,Q4qTzHnjLyiCCRvNpXioUq
qwen3-235b-a22b,0.7070707070707071,2025-04-29,Alibaba,China,4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,0.027124209393506977,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FagebmxGseTvmvN4AHN8i4m.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/agebmxGseTvmvN4AHN8i4m.eval,2025-06-03T11:52:52.449Z,agebmxGseTvmvN4AHN8i4m
gemini-2.5-pro-preview-05-06,0.6666666666666666,2025-05-06,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.033586181457325226,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTT66gGiVoeYLGQMCESPrEG.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/TT66gGiVoeYLGQMCESPrEG.eval,2025-06-03T09:10:11.019Z,TT66gGiVoeYLGQMCESPrEG
DeepSeek-R1-0528,0.7632575757575758,2025-05-28,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.024023818843083005,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FLiRiryJsGb3h5UuxX8bmJv.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/LiRiryJsGb3h5UuxX8bmJv.eval,2025-05-29T13:30:43.463Z,LiRiryJsGb3h5UuxX8bmJv
claude-3-7-sonnet-20250219_64K,0.7850378787878788,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.026506883123060064,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F2t26VSNdsAE5pefQEMP25x.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/2t26VSNdsAE5pefQEMP25x.eval,2025-05-26T22:53:45.927Z,2t26VSNdsAE5pefQEMP25x
grok-3-mini-beta_high,0.7462121212121212,2025-04-09,xAI,United States of America,,,0.027675521820041432,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJFZMWFVGNi5R5TdZrvQUFw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/JFZMWFVGNi5R5TdZrvQUFw.eval,2025-05-26T18:38:04.231Z,JFZMWFVGNi5R5TdZrvQUFw
DeepSeek-R1,0.6922348484848484,2025-01-20,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.03070485693321271,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfFGyQR2mfDufvrSV2rzWyV.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fFGyQR2mfDufvrSV2rzWyV.eval,2025-05-26T18:38:03.702Z,fFGyQR2mfDufvrSV2rzWyV
claude-sonnet-4-20250514_59K,0.7781991873045173,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02674821434321096,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJ9wjcQqKJ7hCSYnnoa5E9i.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/J9wjcQqKJ7hCSYnnoa5E9i.eval,2025-05-26T18:38:03.090Z,J9wjcQqKJ7hCSYnnoa5E9i
grok-3-beta,0.6758207070707071,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.026863078746340353,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FFQ8Ykq4JWRSXnyF9rbmLc7.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/FQ8Ykq4JWRSXnyF9rbmLc7.eval,2025-05-26T17:42:41.696Z,FQ8Ykq4JWRSXnyF9rbmLc7
claude-sonnet-4-20250514_32K,0.7828282828282829,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02937661648494561,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F8hGbWtbpUjAQpebkKQfSi7.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/8hGbWtbpUjAQpebkKQfSi7.eval,2025-05-22T23:06:11.598Z,8hGbWtbpUjAQpebkKQfSi7
claude-opus-4-20250514_16K,0.7626262626262627,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.03031371053819892,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FCtxFusPxCraL3EeRJu2c5k.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/CtxFusPxCraL3EeRJu2c5k.eval,2025-05-22T22:54:06.726Z,CtxFusPxCraL3EeRJu2c5k
claude-sonnet-4-20250514_16K,0.7575757575757576,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.030532892233932022,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPPLoBnbKQ9CEYZNSk3E7LE.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PPLoBnbKQ9CEYZNSk3E7LE.eval,2025-05-22T22:54:05.316Z,PPLoBnbKQ9CEYZNSk3E7LE
claude-sonnet-4-20250514,0.6666666666666666,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.033586181457325226,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZF4eejw7J2KrcoNWd6hUbU.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZF4eejw7J2KrcoNWd6hUbU.eval,2025-05-22T17:26:47.959Z,ZF4eejw7J2KrcoNWd6hUbU
claude-opus-4-20250514,0.6919191919191919,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.03289477330098615,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FEnjJ8bGi9vRNNQFzfkLCkQ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/EnjJ8bGi9vRNNQFzfkLCkQ.eval,2025-05-22T17:26:47.111Z,EnjJ8bGi9vRNNQFzfkLCkQ
mistral-medium-2505,0.5953282828282829,2025-05-07,Mistral AI,France,,"Benchmarks match with models like GPT-4o, Mistral's previous largest runs were ~1e25 FLOP scale, so plausibly they might have trained Medium with this much compute.",0.02824697229654645,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FFNMbfvEEm9PRnrJnrqk2PH.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/FNMbfvEEm9PRnrJnrqk2PH.eval,2025-05-07T17:32:22.166Z,FNMbfvEEm9PRnrJnrqk2PH
o4-mini-2025-04-16_high,0.7960858585858586,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.024034764096121958,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9Sgsz5XCjscuZkexf7MX9c.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9Sgsz5XCjscuZkexf7MX9c.eval,2025-04-16T18:17:16.139Z,9Sgsz5XCjscuZkexf7MX9c
o3-2025-04-16_high,0.8181818181818182,2025-04-16,OpenAI,United States of America,,,0.021267123846387202,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FevANZQ9oQTdGYbDERw7VrD.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/evANZQ9oQTdGYbDERw7VrD.eval,2025-04-16T18:17:09.093Z,evANZQ9oQTdGYbDERw7VrD
gpt-4.1-mini-2025-04-14,0.6584595959595959,2025-04-14,OpenAI,United States of America,,,0.027051500965295915,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdXHddeGRX7JRVa5UNfad5C.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dXHddeGRX7JRVa5UNfad5C.eval,2025-04-14T22:49:57.790Z,dXHddeGRX7JRVa5UNfad5C
gpt-4.1-nano-2025-04-14,0.48926767676767674,2025-04-14,OpenAI,United States of America,,,0.024849734950435368,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FazWfeFPP6zSocuQX3JYBU5.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/azWfeFPP6zSocuQX3JYBU5.eval,2025-04-14T22:49:56.708Z,azWfeFPP6zSocuQX3JYBU5
gpt-4.1-2025-04-14,0.6691919191919192,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02821440093855908,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FGj8pd5gU9r4LqwDK8XZ4mL.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Gj8pd5gU9r4LqwDK8XZ4mL.eval,2025-04-14T18:34:33.975Z,Gj8pd5gU9r4LqwDK8XZ4mL
qwq-plus,0.6540404040404041,2025-04-08,Alibaba,China,,,0.028469238770829935,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPtxHF8BH8WuNdNfDnutGBb.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PtxHF8BH8WuNdNfDnutGBb.eval,2025-04-11T15:34:21.181Z,PtxHF8BH8WuNdNfDnutGBb
grok-3-mini-beta_low,0.7626262626262627,2025-04-09,xAI,United States of America,,,0.03031371053819892,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMcxfQdC5aypnD3STwZUBjH.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/McxfQdC5aypnD3STwZUBjH.eval,2025-04-10T20:30:33.024Z,McxfQdC5aypnD3STwZUBjH
Llama-4-Scout-17B-16E-Instruct,0.5183080808080808,2025-04-05,Meta AI,United States of America,4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",0.031549020124994755,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMsXAXDEKP3xBKtiA7aTBBG.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MsXAXDEKP3xBKtiA7aTBBG.eval,2025-04-08T08:34:50.470Z,MsXAXDEKP3xBKtiA7aTBBG
Llama-4-Maverick-17B-128E-Instruct-FP8,0.6698232323232324,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.028244823529728173,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FLyHy99ubGCqBhoaksQjkiF.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/LyHy99ubGCqBhoaksQjkiF.eval,2025-04-08T08:34:49.725Z,LyHy99ubGCqBhoaksQjkiF
qwen-turbo-2024-11-01,0.41792929292929293,2024-11-01,Alibaba,China,,,0.02617727508870251,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9Ke5At9bYuf56SmKJJ7Sz7.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9Ke5At9bYuf56SmKJJ7Sz7.eval,2025-04-07T21:38:51.185Z,9Ke5At9bYuf56SmKJJ7Sz7
qwen-plus-2025-01-25,0.4810606060606061,2025-01-25,Alibaba,China,,,0.027144936812277986,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQtraMoKiQRRpZc9txpFAYF.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QtraMoKiQRRpZc9txpFAYF.eval,2025-04-07T19:28:06.394Z,QtraMoKiQRRpZc9txpFAYF
qwen-max-2025-01-25,0.5612373737373737,2025-01-25,Alibaba,China,,,0.027858831475709448,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FGBf7rqHkGGzfX87V37F9mH.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/GBf7rqHkGGzfX87V37F9mH.eval,2025-04-01T19:22:56.011Z,GBf7rqHkGGzfX87V37F9mH
DeepSeek-V3-0324,0.6761363636363636,2025-03-24,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",0.026836302525030095,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVjfLwKbT6kTprYsMGcz2YT.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/VjfLwKbT6kTprYsMGcz2YT.eval,2025-04-01T14:40:59.363Z,VjfLwKbT6kTprYsMGcz2YT
gemini-2.5-pro-exp-03-25,0.8383838383838383,2025-03-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02622591986362927,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWKjzEQdfnBHnYeZoKeCWnB.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/WKjzEQdfnBHnYeZoKeCWnB.eval,2025-03-31T16:06:24.669Z,WKjzEQdfnBHnYeZoKeCWnB
mistral-small-2503,0.47474747474747475,2025-03-17,Mistral AI,France,,At least 1.152e+24 FLOP (base model Mistral Small 3 training compute),0.027616495140012243,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FS8bBDAbucsFceirHxeEDvu.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/S8bBDAbucsFceirHxeEDvu.eval,2025-03-18T16:08:06.162Z,S8bBDAbucsFceirHxeEDvu
gemma-3-27b-it,0.48863636363636365,2025-03-12,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",2.268e+24,6ND =  6 * 27B parameters * 14T training tokens = 2.268 × 10^24 FLOP,0.03026334695895556,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FFtxiPnS7ZH5qLrmFgp4ca2.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/FtxiPnS7ZH5qLrmFgp4ca2.eval,2025-03-13T16:05:18.002Z,FtxiPnS7ZH5qLrmFgp4ca2
claude-3-5-haiku-20241022,0.3813131313131313,2024-10-22,Anthropic,United States of America,,,0.026351515390511556,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJpLxrpvthx7A75sbaHZ9Et.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/JpLxrpvthx7A75sbaHZ9Et.eval,2025-03-12T23:51:44.119Z,JpLxrpvthx7A75sbaHZ9Et
gemini-1.5-flash-8b-001,0.32954545454545453,2024-10-03,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.02416551116211757,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FgFT36W4KU3wggLQ9nNCctg.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/gFT36W4KU3wggLQ9nNCctg.eval,2025-03-12T21:23:34.734Z,gFT36W4KU3wggLQ9nNCctg
DeepSeek-R1-Distill-Qwen-14B,0.44696969696969696,2025-01-20,DeepSeek,China,,,0.02863094678190378,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F8UqaLobhmNJLuPGTTpeaCw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/8UqaLobhmNJLuPGTTpeaCw.eval,2025-03-10T23:03:48.213Z,8UqaLobhmNJLuPGTTpeaCw
claude-3-7-sonnet-20250219_32K,0.7676767676767676,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.030088629490217445,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fiv9yfwZ9vTznxQfvJF6J7p.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/iv9yfwZ9vTznxQfvJF6J7p.eval,2025-03-10T22:05:35.890Z,iv9yfwZ9vTznxQfvJF6J7p
DeepSeek-R1-Distill-Llama-70B,0.5574494949494949,2025-01-20,DeepSeek,China,,"base model compute: 6.8649768e+24 FLOP
fine tune compute: 2.016e+22 FLOP",0.02973465219179067,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Ff6pnAATjxmrFJS353EjQKi.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/f6pnAATjxmrFJS353EjQKi.eval,2025-03-10T20:08:15.034Z,f6pnAATjxmrFJS353EjQKi
gpt-4.5-preview-2025-02-27,0.6868686868686869,2025-02-27,OpenAI,United States of America,2.1000001e+26,"Analysis of GPT-4.5's training cluster, in combination with 30% utilization relative to H100 16-bit output (or a plausible range of 20 to 50% utilization, given the possibility of FP8 training), yields a median estimate of ~2e25 FLOP. See notebook below for details.

https://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw 

This is consistent with OpenAI's statement that GPT-4.5 was a “new order of magnitude in compute” compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. 

In the ""Pretraining GPT-4.5"" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",0.033042050878136546,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdodibSv7pk8GtqeMy4YLQg.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dodibSv7pk8GtqeMy4YLQg.eval,2025-02-28T09:17:12.859Z,dodibSv7pk8GtqeMy4YLQg
gpt-4-turbo-2024-04-09,0.4659090909090909,2024-04-09,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.026576883332580552,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTaCoN7eyPQEfDWXqs2MpcF.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/TaCoN7eyPQEfDWXqs2MpcF.eval,2025-02-27T14:29:45.537Z,TaCoN7eyPQEfDWXqs2MpcF
claude-3-7-sonnet-20250219_16K,0.7676767676767676,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.030088629490217445,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVn3Ug88CRbxEaJhLbRzAWD.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Vn3Ug88CRbxEaJhLbRzAWD.eval,2025-02-26T16:56:43.203Z,Vn3Ug88CRbxEaJhLbRzAWD
mistral-large-2411,0.5132575757575758,2024-11-18,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",0.026887818295509087,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fn9gpXfUzo8SbWqvrG2q4xM.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/n9gpXfUzo8SbWqvrG2q4xM.eval,2025-02-25T12:31:34.069Z,n9gpXfUzo8SbWqvrG2q4xM
claude-3-7-sonnet-20250219,0.6603535353535354,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.026703858684060923,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPyNEkFCfMEReodeE2oiPhh.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PyNEkFCfMEReodeE2oiPhh.eval,2025-02-24T19:14:07.847Z,PyNEkFCfMEReodeE2oiPhh
o1-2024-12-17_high,0.7676767676767676,2024-12-17,OpenAI,United States of America,,,0.030088629490217445,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNAkCFiFiDSN3NXMj7HMvMS.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NAkCFiFiDSN3NXMj7HMvMS.eval,2025-02-13T17:12:11.445Z,NAkCFiFiDSN3NXMj7HMvMS
o1-mini-2024-09-12_high,0.6237373737373737,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.027427723511230753,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FX2gKewiGPZx5DLFEddUUqW.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/X2gKewiGPZx5DLFEddUUqW.eval,2025-02-13T17:07:50.981Z,X2gKewiGPZx5DLFEddUUqW
o3-mini-2025-01-31_high,0.7702020202020202,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.025855332698878485,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMzsTvCEEfEpzn4WvxYdJkX.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MzsTvCEEfEpzn4WvxYdJkX.eval,2025-02-13T17:07:50.336Z,MzsTvCEEfEpzn4WvxYdJkX
gemini-2.0-pro-exp-02-05,0.6565656565656566,2025-02-05,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in early 2025; very likely it used >1e25 FLOP.,0.03383201223244441,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fg5YE7G6DXb45EH4HxEdiht.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/g5YE7G6DXb45EH4HxEdiht.eval,2025-02-07T07:28:03.988Z,g5YE7G6DXb45EH4HxEdiht
gemini-2.0-flash-thinking-exp-01-21,0.5707070707070707,2025-01-21,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.03526552724601199,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fag9w2w4xa47ghZaWt5oZey.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ag9w2w4xa47ghZaWt5oZey.eval,2025-02-06T22:40:04.398Z,ag9w2w4xa47ghZaWt5oZey
gemini-2.0-flash-001,0.6414141414141414,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",0.027395618179027574,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfbszwTApm3f28z5VA6oXBK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fbszwTApm3f28z5VA6oXBK.eval,2025-02-06T15:51:21.852Z,fbszwTApm3f28z5VA6oXBK
gpt-4o-2024-11-20,0.4788510101010101,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.026336972178726107,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FgULKGqAe9nG9syxmYMtkhi.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/gULKGqAe9nG9syxmYMtkhi.eval,2025-02-05T08:31:15.914Z,gULKGqAe9nG9syxmYMtkhi
phi-4,0.5606060606060606,2024-12-12,Microsoft Research,United States of America,9.3202015e+23,"6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP

989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP

geometric mean
sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23",0.02590274664132679,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FRAwuPRwJedy7P2KeQhEmr8.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/RAwuPRwJedy7P2KeQhEmr8.eval,2025-01-31T23:03:31.781Z,RAwuPRwJedy7P2KeQhEmr8
Phi-3-medium-128k-instruct,0.2758838383838384,2024-04-23,Microsoft,United States of America,4.032e+23,counting operations: 6×4.8×10^12 tokens × 14×10^9 parameters ≈ 4.032×10^23 FLOPS,0.016117685500292727,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbDS2AGEf3AywbVcdDyAKZK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/bDS2AGEf3AywbVcdDyAKZK.eval,2025-01-31T22:41:25.625Z,bDS2AGEf3AywbVcdDyAKZK
o3-mini-2025-01-31_medium,0.742739898989899,2025-01-31,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.025411943362857046,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FehvA5nisC7GMbgbyy3Z4Et.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ehvA5nisC7GMbgbyy3Z4Et.eval,2025-01-31T19:21:57.338Z,ehvA5nisC7GMbgbyy3Z4Et
mistral-small-2501,0.4529671717171717,2025-01-25,Mistral AI,France,1.152e+24,6ND = 6*8T tokens * 24B parameters = 1.152e+24 FLOP,0.024794688677057853,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FoDq358GfyV5Au65scERdVo.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/oDq358GfyV5Au65scERdVo.eval,2025-01-30T21:10:36.084Z,oDq358GfyV5Au65scERdVo
qwen2.5-32b-instruct,0.46085858585858586,2024-09-17,Alibaba,China,3.51e+24,6 FLOP / parameter / token * 32.5B parameters * 18 trillion tokens = 3.51 × 10^24 FLOP,0.02595783004579065,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Fd6JihAFVqUy3u9suNt4P3L.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/d6JihAFVqUy3u9suNt4P3L.eval,2025-01-30T18:31:31.252Z,d6JihAFVqUy3u9suNt4P3L
gemini-1.5-flash-001,0.40372474747474746,2024-05-23,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.023852475604024478,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FHc2ycMP6DyCZxCPjKbZFQU.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Hc2ycMP6DyCZxCPjKbZFQU.eval,2025-01-27T00:00:00.000Z,Hc2ycMP6DyCZxCPjKbZFQU
claude-2.0,0.3465909090909091,2023-07-11,Anthropic,United States of America,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,0.025076255983551475,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FmTa4f52zsEuxdrPy7WfQvX.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/mTa4f52zsEuxdrPy7WfQvX.eval,2025-01-27T00:00:00.000Z,mTa4f52zsEuxdrPy7WfQvX
claude-3-opus-20240229,0.4715909090909091,2024-02-29,Anthropic,United States of America,,Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.026395393956533515,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FhTnDa4gn8RYduCEYqoNywe.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/hTnDa4gn8RYduCEYqoNywe.eval,2025-01-27T00:00:00.000Z,hTnDa4gn8RYduCEYqoNywe
o1-mini-2024-09-12_medium,0.5950126262626263,2024-09-12,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.02795124238637276,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F4NiUpCrLfz3oFaerkrMeBt.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/4NiUpCrLfz3oFaerkrMeBt.eval,2025-01-27T00:00:00.000Z,4NiUpCrLfz3oFaerkrMeBt
gpt-4o-2024-08-06,0.49210858585858586,2024-08-06,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.025999270277271707,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfairJpL9VekDBKxYHG9ZoG.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fairJpL9VekDBKxYHG9ZoG.eval,2025-01-27T00:00:00.000Z,fairJpL9VekDBKxYHG9ZoG
o1-preview-2024-09-12,0.5031565656565656,2024-09-12,OpenAI,United States of America,,,0.028257821072909393,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F7CkADTsoNbDd88uqYbqfwj.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/7CkADTsoNbDd88uqYbqfwj.eval,2025-01-27T00:00:00.000Z,7CkADTsoNbDd88uqYbqfwj
claude-3-sonnet-20240229,0.4059343434343434,2024-02-29,Anthropic,United States of America,,,0.023920223491748827,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FQLejPZXhjX6SQCQgzjNvCG.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/QLejPZXhjX6SQCQgzjNvCG.eval,2025-01-27T00:00:00.000Z,QLejPZXhjX6SQCQgzjNvCG
claude-3-5-sonnet-20241022,0.553030303030303,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.028208090147078847,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJCCVcVLz3J4TQXSU67hMJZ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/JCCVcVLz3J4TQXSU67hMJZ.eval,2025-01-27T00:00:00.000Z,JCCVcVLz3J4TQXSU67hMJZ
gpt-4-0125-preview,0.42266414141414144,2024-01-25,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.026795949166711065,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F4j3WRhhruXrfaq9Jki3NDc.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/4j3WRhhruXrfaq9Jki3NDc.eval,2025-01-27T00:00:00.000Z,4j3WRhhruXrfaq9Jki3NDc
claude-2.1,0.32954545454545453,2023-11-21,Anthropic,United States of America,,,0.02324885083265289,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FeMAisrJd7fAUHPfcPWXeXo.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/eMAisrJd7fAUHPfcPWXeXo.eval,2025-01-27T00:00:00.000Z,eMAisrJd7fAUHPfcPWXeXo
gemma-2-27b-it,0.3648989898989899,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",0.023897884428691632,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZUJxsJDpHnLXxNiDcvj22W.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZUJxsJDpHnLXxNiDcvj22W.eval,2025-01-27T00:00:00.000Z,ZUJxsJDpHnLXxNiDcvj22W
claude-3-haiku-20240307,0.3630050505050505,2024-03-07,Anthropic,United States of America,,,0.024180910672074975,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FT3FWdCCWBryVjxwTcoY5bK.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/T3FWdCCWBryVjxwTcoY5bK.eval,2025-01-27T00:00:00.000Z,T3FWdCCWBryVjxwTcoY5bK
claude-3-5-sonnet-20240620,0.5404040404040404,2024-06-20,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",0.027528555453348653,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F78Wpvp8WDByAcQRBas72cs.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/78Wpvp8WDByAcQRBas72cs.eval,2025-01-27T00:00:00.000Z,78Wpvp8WDByAcQRBas72cs
gpt-4o-2024-05-13,0.4889520202020202,2024-05-13,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.026254109873371994,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FY59q8WbEDp2HH8oovyJCj9.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Y59q8WbEDp2HH8oovyJCj9.eval,2025-01-27T00:00:00.000Z,Y59q8WbEDp2HH8oovyJCj9
gemini-1.5-flash-002,0.47316919191919193,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"""Gemini 1.5 Flash is a dense Transformer based model that is online distilled [...] from Gemini 1.5 Pro.""",0.027737408277040958,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPCLvrs4TEBtqQqcYeU8pAG.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PCLvrs4TEBtqQqcYeU8pAG.eval,2025-01-27T00:00:00.000Z,PCLvrs4TEBtqQqcYeU8pAG
gpt-4-0613,0.30650252525252525,2023-06-13,OpenAI,United States of America,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",0.02337322948270623,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2Ffu7pWqbSC4MUBdwv3GFNa5.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fu7pWqbSC4MUBdwv3GFNa5.eval,2025-01-27T00:00:00.000Z,fu7pWqbSC4MUBdwv3GFNa5
gemma-2-9b-it,0.2746212121212121,2024-06-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP",0.023032467954778795,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FfQjoHKyUEPAuwdrBGfL8SG.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/fQjoHKyUEPAuwdrBGfL8SG.eval,2025-01-27T00:00:00.000Z,fQjoHKyUEPAuwdrBGfL8SG
gpt-3.5-turbo-1106,0.2803030303030303,2023-11-06,OpenAI,United States of America,,,0.018720372561004013,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FenhRcQmfN6RFB2zu7R3Uxr.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/enhRcQmfN6RFB2zu7R3Uxr.eval,2025-01-27T00:00:00.000Z,enhRcQmfN6RFB2zu7R3Uxr
gemini-1.5-pro-001,0.4586489898989899,2024-05-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.02617295639558329,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPKdqpKZ9tqDWyWJ7GksBWz.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/PKdqpKZ9tqDWyWJ7GksBWz.eval,2025-01-27T00:00:00.000Z,PKdqpKZ9tqDWyWJ7GksBWz
gpt-4-1106-preview,0.4236111111111111,2023-11-06,OpenAI,United States of America,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.024076142618301676,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FAWewipyfSQb8hXtS8d4fNL.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/AWewipyfSQb8hXtS8d4fNL.eval,2025-01-27T00:00:00.000Z,AWewipyfSQb8hXtS8d4fNL
gpt-4o-mini-2024-07-18,0.37720959595959597,2024-07-18,OpenAI,United States of America,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",0.024068925424620136,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FL9yfwZC3snsKZshBXTaXjc.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/L9yfwZC3snsKZshBXTaXjc.eval,2025-01-27T00:00:00.000Z,L9yfwZC3snsKZshBXTaXjc
gemini-1.5-pro-002,0.5722853535353535,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,0.027959709636504244,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9TrLEEw8TvsyugyCjwvUvD.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9TrLEEw8TvsyugyCjwvUvD.eval,2025-01-27T00:00:00.000Z,9TrLEEw8TvsyugyCjwvUvD
gpt-3.5-turbo-0125,0.27178030303030304,2024-01-25,OpenAI,United States of America,,,0.01953579968788737,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FeZaQhpbhbzbo3sbKGS5Efa.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/eZaQhpbhbzbo3sbKGS5Efa.eval,2025-01-27T00:00:00.000Z,eZaQhpbhbzbo3sbKGS5Efa
Llama-3.1-8B-Instruct,0.25946969696969696,2024-07-23,Meta AI,United States of America,1.224e+24,"Huggingface page says 3.1-8B used 1.46M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 8B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 8B = 7.2e23 FLOPs

Hardware:
1.46M * 9.9e14 * 3600 * 0.4 = 2.08e24 FLOPs

Geometric mean: sqrt(7.2e23 * 2.08e24) = 1.224e24

Note that Llama 3-8B also said it used 15T tokens, but only 1.3M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.",0.01674966399258744,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWWS3aotkrymLmBBDhhtvWQ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/WWS3aotkrymLmBBDhhtvWQ.eval,2025-01-27T00:00:00.000Z,WWS3aotkrymLmBBDhhtvWQ
Llama-3.1-70B-Instruct,0.44191919191919193,2024-07-23,Meta AI,United States of America,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",0.024522863472149384,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMEwMcmSV32JHrsNoAq4BFL.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MEwMcmSV32JHrsNoAq4BFL.eval,2025-01-27T00:00:00.000Z,MEwMcmSV32JHrsNoAq4BFL
Llama-3.1-405B-Instruct,0.5091540404040404,2024-07-23,Meta AI,United States of America,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",0.025861992702770648,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FS5QYXSvQBRSbUbXSnAGbMm.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/S5QYXSvQBRSbUbXSnAGbMm.eval,2025-01-27T00:00:00.000Z,S5QYXSvQBRSbUbXSnAGbMm
Yi-1.5-34B-Chat,0.319760101010101,2024-05-13,01.AI,China,7.344e+23,6 FLOP / parameter / token * 34*10^9 parameters * 3.6*10^12 tokens = 7.344e+23 FLOP,0.020005291661900883,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FJwi8JTeeTxJuJY8qidxngw.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Jwi8JTeeTxJuJY8qidxngw.eval,2025-01-27T00:00:00.000Z,Jwi8JTeeTxJuJY8qidxngw
Yi-34B-Chat,0.14741161616161616,2023-11-22,01.AI,China,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",0.011372823721448395,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWxMpEBcTf8qPFPz4BrF86B.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/WxMpEBcTf8qPFPz4BrF86B.eval,2025-01-27T00:00:00.000Z,WxMpEBcTf8qPFPz4BrF86B
grok-2-1212,0.5378787878787878,2024-12-12,xAI,United States of America,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,0.027066342316607196,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F83DRJD4FvjwqU9eStRaNiU.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/83DRJD4FvjwqU9eStRaNiU.eval,2025-01-27T00:00:00.000Z,83DRJD4FvjwqU9eStRaNiU
qwen2-72b-instruct,0.4078282828282828,2024-06-07,Alibaba,China,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",0.02574288051254005,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FPex6SpYJX8ZPySncfNNy7H.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Pex6SpYJX8ZPySncfNNy7H.eval,2025-01-27T00:00:00.000Z,Pex6SpYJX8ZPySncfNNy7H
qwen1.5-72b-chat,0.2881944444444444,2024-02-04,Alibaba,China,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",0.01993691228122493,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FNeqeQSxHmdUNLk9Ytpo6qo.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/NeqeQSxHmdUNLk9Ytpo6qo.eval,2025-01-27T00:00:00.000Z,NeqeQSxHmdUNLk9Ytpo6qo
qwen1.5-32b-chat,0.307449494949495,2024-04-03,Alibaba,China,,"upper bound is taken from Qwen1.5 72B training compute estimation

lower bound is taken from Qwen1.5 14B training compute estimation",0.020200517973424446,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9K6c9b6v9zQrQYtw827BDn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9K6c9b6v9zQrQYtw827BDn.eval,2025-01-27T00:00:00.000Z,9K6c9b6v9zQrQYtw827BDn
Hermes-2-Theta-Llama-3-70B,0.3746843434343434,2024-06-20,"Nous Research,Arcee AI",United States of America,,,0.024803009993948306,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FDHdeZNynu4VtRp2VGkCGvd.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/DHdeZNynu4VtRp2VGkCGvd.eval,2025-01-27T00:00:00.000Z,DHdeZNynu4VtRp2VGkCGvd
Llama-2-70b-chat-hf,0.26325757575757575,2023-07-18,Meta AI,United States of America,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",0.019901808558284103,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FevPjJw9SKqq3TieSdxrSQn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/evPjJw9SKqq3TieSdxrSQn.eval,2025-01-27T00:00:00.000Z,evPjJw9SKqq3TieSdxrSQn
Meta-Llama-3-70B-Instruct,0.40561868686868685,2024-04-18,Meta AI,United States of America,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",0.026224849492112068,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FjrvutCoFFG4KxYtU7REdpe.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/jrvutCoFFG4KxYtU7REdpe.eval,2025-01-27T00:00:00.000Z,jrvutCoFFG4KxYtU7REdpe
Meta-Llama-3-8B-Instruct,0.26073232323232326,2024-04-18,Meta AI,United States of America,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2×10^23 FLOP

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872×10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",0.01703043361773576,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FiT3BBFWjsZk2rKHKeJDsM8.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/iT3BBFWjsZk2rKHKeJDsM8.eval,2025-01-27T00:00:00.000Z,iT3BBFWjsZk2rKHKeJDsM8
Mistral-7B-Instruct-v0.3,0.1518308080808081,2024-05-27,Mistral AI,France,,,0.010148891037227303,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FYbQkMPC2GtNLhxqTEhhVsa.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/YbQkMPC2GtNLhxqTEhhVsa.eval,2025-01-27T00:00:00.000Z,YbQkMPC2GtNLhxqTEhhVsa
deepseek-llm-67b-chat,0.24621212121212122,2023-11-29,DeepSeek,China,8.04e+23,67B parameters * 2T tokens * 6 FLOP / token / parameter = 8.04e23 FLOP,0.013623351950761947,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F82RTgqgiC2UuM5M37L3WWM.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/82RTgqgiC2UuM5M37L3WWM.eval,2025-01-27T00:00:00.000Z,82RTgqgiC2UuM5M37L3WWM
Mixtral-8x7B-Instruct-v0.1,0.3058712121212121,2023-12-11,Mistral AI,France,7.74e+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",0.01937059603941351,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FSn2qbB2HiySe7NcG28jnyM.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Sn2qbB2HiySe7NcG28jnyM.eval,2025-01-27T00:00:00.000Z,Sn2qbB2HiySe7NcG28jnyM
qwen2.5-72b-instruct,0.4914772727272727,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",0.026947767404178667,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FWtALgW6VSFdihDZWXRm2AZ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/WtALgW6VSFdihDZWXRm2AZ.eval,2025-01-27T00:00:00.000Z,WtALgW6VSFdihDZWXRm2AZ
WizardLM-2-8x22B,0.43434343434343436,2024-04-15,Microsoft,United States of America,,,0.023232093333444247,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FMQJA3RUjsQzSubGJiukZ7A.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/MQJA3RUjsQzSubGJiukZ7A.eval,2025-01-27T00:00:00.000Z,MQJA3RUjsQzSubGJiukZ7A
dbrx-instruct,0.32891414141414144,2024-03-27,Databricks,United States of America,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",0.03256944333564935,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTTa3hu7PgRxHgizVreZsdJ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/TTa3hu7PgRxHgizVreZsdJ.eval,2025-01-27T00:00:00.000Z,TTa3hu7PgRxHgizVreZsdJ
gemini-1.0-pro-001,0.33964646464646464,2024-02-15,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,"Training compute estimated to be 1.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing 

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",0.020110933405176737,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FibpS8bs4z3wD3Hb6F3kpdV.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ibpS8bs4z3wD3Hb6F3kpdV.eval,2025-01-27T00:00:00.000Z,ibpS8bs4z3wD3Hb6F3kpdV
ministral-3b-2410,0.25252525252525254,2024-10-16,Mistral AI,France,,,0.019305614722797293,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FbJV4D8SEo9zMJPEYTTXJNZ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/bJV4D8SEo9zMJPEYTTXJNZ.eval,2025-01-27T00:00:00.000Z,bJV4D8SEo9zMJPEYTTXJNZ
mistral-large-2402,0.38762626262626265,2024-02-26,Mistral AI,France,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",0.024776598535417284,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FdDeREdDJ5bVyci3APZxzWb.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/dDeREdDJ5bVyci3APZxzWb.eval,2025-01-27T00:00:00.000Z,dDeREdDJ5bVyci3APZxzWb
open-mixtral-8x22b,0.34059343434343436,2024-04-17,Mistral AI,France,2.34e+24,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 39 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 2.34e+24 FLOP",0.02068134779158647,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FifAM3DJBdm5eU9Y2DjaEvW.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ifAM3DJBdm5eU9Y2DjaEvW.eval,2025-01-27T00:00:00.000Z,ifAM3DJBdm5eU9Y2DjaEvW
ministral-8b-2410,0.27146464646464646,2024-10-16,Mistral AI,France,,,0.020863706671553732,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FDGEzZA6nvTne4MEGSuo3Tm.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/DGEzZA6nvTne4MEGSuo3Tm.eval,2025-01-27T00:00:00.000Z,DGEzZA6nvTne4MEGSuo3Tm
mistral-large-2407,0.49021464646464646,2024-07-24,Mistral AI,France,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",0.025918840988656967,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FZQPNnicTuhuscJvni85PFY.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/ZQPNnicTuhuscJvni85PFY.eval,2025-01-27T00:00:00.000Z,ZQPNnicTuhuscJvni85PFY
open-mixtral-8x7b,0.29829545454545453,2023-12-11,Mistral AI,France,7.74e+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",0.019146998037923697,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FY6KPSujFepdwqinKLf9mDF.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Y6KPSujFepdwqinKLf9mDF.eval,2025-01-27T00:00:00.000Z,Y6KPSujFepdwqinKLf9mDF
open-mistral-7b,0.13226010101010102,2023-09-27,Mistral AI,France,,,0.009888086213040367,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F5XuVuHeUDekHqjUy2P8gww.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/5XuVuHeUDekHqjUy2P8gww.eval,2025-01-27T00:00:00.000Z,5XuVuHeUDekHqjUy2P8gww
open-mistral-nemo-2407,0.2989267676767677,2024-07-18,Mistral AI,France,,,0.02096191098761666,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FXCSWtbDquA7FUiWfd7kh9q.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/XCSWtbDquA7FUiWfd7kh9q.eval,2025-01-27T00:00:00.000Z,XCSWtbDquA7FUiWfd7kh9q
Llama-3.2-90B-Vision-Instruct,0.41035353535353536,2024-09-24,Meta AI,United States of America,,,0.022948341091025694,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F9SSHEP95FYVZvHDdoHSRy3.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/9SSHEP95FYVZvHDdoHSRy3.eval,2025-01-27T00:00:00.000Z,9SSHEP95FYVZvHDdoHSRy3
Llama-3.1-Tulu-3-70B-DPO,0.46275252525252525,2024-11-21,"Allen Institute for AI,University of Washington",United States of America,,,0.02405836362093961,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2F5JgivNDZp2D7Lc7cpDhS28.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/5JgivNDZp2D7Lc7cpDhS28.eval,2025-01-27T00:00:00.000Z,5JgivNDZp2D7Lc7cpDhS28
Eurus-2-7B-PRIME,0.3390151515151515,2024-12-31,"Tsinghua University,University of Illinois Urbana-Champaign (UIUC),Shanghai AI Lab,Peking University,Shanghai Jiao Tong University,CUHK Shenzhen Research Institute","United States of America,China",,,0.02107317439394748,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FRJm4u2vXF6PUmThJGzAfHE.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/RJm4u2vXF6PUmThJGzAfHE.eval,2025-01-27T00:00:00.000Z,RJm4u2vXF6PUmThJGzAfHE
Llama-3.3-70B-Instruct,0.4744318181818182,2024-12-06,Meta AI,United States of America,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",0.028523027562061026,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FTxXS78Wg2pSCDbpmeJuzQn.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/TxXS78Wg2pSCDbpmeJuzQn.eval,2025-01-27T00:00:00.000Z,TxXS78Wg2pSCDbpmeJuzQn
o1-2024-12-17_medium,0.7575757575757576,2024-12-17,OpenAI,United States of America,,,0.030532892233932022,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FUczhz7MKLstjLkSbbGuwdN.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Uczhz7MKLstjLkSbbGuwdN.eval,2025-01-27T00:00:00.000Z,Uczhz7MKLstjLkSbbGuwdN
DeepSeek-V3,0.5653409090909091,2024-12-26,DeepSeek,China,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",0.02763990029950893,https://logs.epoch.ai/inspect-viewer/c79c08da/viewer.html?log_file=https%3A%2F%2Flogs.epoch.ai%2Finspect_ai_logs%2FVwo3nMA8g2gGBCyTdKFMQJ.eval,https://epoch-benchmarks-production-public.s3.us-east-2.amazonaws.com/inspect_ai_logs/Vwo3nMA8g2gGBCyTdKFMQJ.eval,2025-01-27T00:00:00.000Z,Vwo3nMA8g2gGBCyTdKFMQJ
grok-4-0709,0.87,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.02,,,,A85Zfq2qguE4X9xXBweBHP
