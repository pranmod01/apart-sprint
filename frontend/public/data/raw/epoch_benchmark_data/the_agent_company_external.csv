Model version,% Score,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Model,UUID,% Resolved,Average steps,Average costs,Date,Environment model,Source,Source link (site from table),Notes
claude-3-5-sonnet-20241022,0.344,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",OpenHands + Claude 3.5 Sonnet,recsHmIt77Rr4LD4D,0.24,29.17,6.34,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-sonnet-20241022,"Submission has open-source code, submission patch generations reproduced by Agent Company team. Assuming Claude version based on dates."
gemini-2.0-flash-001,0.19,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Googleâ€™s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",Gemini 2.0 Flash,recg0UBT3qOUyHeaR,0.114,39.85,0.79,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-gemini-2.0-flash,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
gpt-4o-2024-11-20,0.167,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,OpenHands + GPT 4o,recyTMvFANtWFk7jZ,0.086,14.55,1.29,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-gpt-4o-2024-08-06,"Submission has open-source code, submission patch generations reproduced by Agent Company team. Assuming latest 4o version based on date. "
Llama-3.1-405B-Instruct,0.141,2024-07-23,Meta AI,United States of America,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",OpenHands + Llama-3.1-405b,recpu7npNorRgBdGT,0.074,22.895,3.21,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-llama-3.1-405b,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
Llama-3.1-70B-Instruct,0.128,2024-07-23,Meta AI,United States of America,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",OpenHands + LLama-3.3-70b,recjM5TXNeULyzAM4,0.069,20.93,0.93,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-llama-3.3-70b,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
qwen2.5-72b-instruct,0.118,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",OpenHands + Qwen-2.5-72b,recJN85mxDtJTm4xa,0.057,23.99,1.53,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-qwen2.5-72b,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
gemini-1.5-pro-002,0.08,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,OpenHands + Gemini 1.5 Pro,recSEDEJf5qf4uo36,0.034,22.1,6.78,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-gemini-1.5-pro,"Submission has open-source code, submission patch generations reproduced by Agent Company team. Assuming pro-002 version based on dates."
Llama-3.1-70B-Instruct,0.065,2024-07-23,Meta AI,United States of America,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",OpenHands + Llama-3.1-70b,rechUIH1qLtkmlKsq,0.017,19.18,0.83,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-llama-3.1-70b,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
amazon.nova-pro-v1:0,0.057,2024-12-03,Amazon,United States of America,6.000010000000001e+24,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",OpenHands + Amazon Nova Pro V1:0,recsCakwmRnBzeLtI,0.017,19.59,1.55,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-nova-pro-v1%3A0,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
qwen2-72b-instruct,0.042,2024-06-07,Alibaba,China,3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",OpenHands + Qwen-2-72b,rec0ig6A3JLlKRUc5,0.011,23.7,0.28,2024-12-17,Claude 3.5 Sonnet,TheAgentCompany experiment results github,https://github.com/TheAgentCompany/experiments/tree/main/evaluation/1.0.0/20241217_OpenHands-0.14.2-qwen2-72b,"Submission has open-source code, submission patch generations reproduced by Agent Company team "
