Model version,Agent,Accuracy mean,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Accuracy SE,Notes,Source,Source link,id
claude-sonnet-4-5-20250929,Ante,0.603,2025-09-29,Anthropic,United States of America,,,0.011,Unknown thinking token budget,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recC4qflsnucoRuHB
claude-opus-4-1-20250805,Droid,0.588,2025-08-05,Anthropic,United States of America,,,0.009,Announcement confirms no thinking,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,rec9xYW8rLPkSKTrN
claude-sonnet-4-5-20250929,Droid,0.575,2025-09-29,Anthropic,United States of America,,,0.008,"Thinking budget not specified, but previous Droid runs with Opus 4.1 and Sonnet 4.5 used no thinking:
https://factory.ai/news/terminal-bench",Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recB9PenfFsDrUga7
,OB-1,0.567,,,,,,0.006,No model version selected due to there being multiple models,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,rechvU3SZ3Qs8wXWX
claude-sonnet-4-20250514,Ante,0.548,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.015,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recSKBZK17D2Fn7eq
gpt-5-2025-08-07_medium,Droid,0.525,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.021,Announcement clarifies this is with medium thinking: https://factory.ai/news/terminal-bench,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recrsAFNu9U4SbVfx
claude-sonnet-4-5-20250929,Chaterm,0.525,2025-09-29,Anthropic,United States of America,,,0.005,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,rec1FSrvunCfzzUDO
,Warp,0.52,,,,,,0.01,No model version selected due to there being multiple models,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recFWex768BAyAaDV
claude-sonnet-4-5-20250929,Terminus 2,0.51,2025-09-29,Anthropic,United States of America,,,0.008,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recI91PcJhNrmcSLR
claude-sonnet-4-5-20250929,DeepAgent Desktop,0.505,2025-09-29,Anthropic,United States of America,,,0.005,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recaD03rjFJCG3TR9
claude-sonnet-4-20250514,Droid,0.505,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.014,Announcement clarifies this is with no thinking: https://factory.ai/news/terminal-bench,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recVeI4yVDDkR16kE
claude-sonnet-4-20250514,Chaterm,0.493,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recTrSFpMh1Pvimpg
gpt-5-2025-08-07_medium,OB-1,0.49,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.011,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recTYCZIYKlPJFG2T
claude-opus-4-20250514,Goose,0.453,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.015,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recSYwcb4O8UFSLcT
claude-sonnet-4-20250514,Engine Labs,0.448,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.008,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,rec6wmJqssZgjwxpw
claude-opus-4-1-20250805,Terminus 2,0.438,2025-08-05,Anthropic,United States of America,,,0.014,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recxqFJExYe7BIkUk
claude-opus-4-20250514,Claude Code,0.432,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recJ3nXlRqC9WnAG2
gpt-5-codex,Codex CLI,0.428,2025-09-15,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.021,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recnFKSgAdCnokANq
claude-sonnet-4-20250514,Letta,0.425,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.008,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recXLxSONdDMtSY6a
claude-opus-4-20250514,Goose,0.42,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,rec0IOb2BpGeKVaId
claude-sonnet-4-20250514,OpenHands,0.413,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.007,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recWCekkW8mdwUF80
gpt-5-2025-08-07_medium,Terminus 2,0.413,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.011,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recjW2tFsAsHrec3h
claude-sonnet-4-20250514,Goose,0.413,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recHy8QKK7PoDySko
claude-opus-4-1-20250805,Orchestrator,0.405,2025-08-05,Anthropic,United States of America,,,0.003,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recD0mhV1kG0BH8LA
glm-4.5,Terminus,0.399,2025-08-03,"Zhipu AI,Tsinghua University",China,4.42e+24,(6 FLOP/token/parameter) * (23T tokens) * (32B active parameters) = 4.42e24 FLOP,0.01,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recJ4sPKZxcbLkH3H
claude-opus-4-20250514,Terminus 2,0.39,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.004,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recbRNdqIyuCL4Ft2
grok-4-0709,Terminus 2,0.39,2025-07-09,xAI,United States of America,5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",0.017,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recoesaYjoiDrtfKy
claude-sonnet-4-5-20250929,Alpha,0.383,2025-09-29,Anthropic,United States of America,,,0.011,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recojooYFu2C6pNdo
claude-sonnet-4-20250514,Orchestrator,0.37,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.02,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recvP8nuaZ3GC9dxF
claude-sonnet-4-20250514,Terminus 2,0.364,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.004,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recakYs6u3cqPpwnu
claude-sonnet-4-20250514,Claude Code,0.355,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.01,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recSkVDPx6rtq4E2s
glaive-swe-v1,Terminus,0.353,,,,,,0.007,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recbn7uXZBsVhX3gc
claude-3-7-sonnet-20250219,Claude Code,0.352,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recQJgn8smPVLOz3F
claude-sonnet-4-20250514,Goose,0.343,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.01,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recqxf2Cu2L8JBwhj
grok-4-fast,Terminus 2,0.313,2025-09-19,xAI,United States of America,,,0.014,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recf8L7V6uPE6G0zO
claude-3-7-sonnet-20250219,Terminus,0.306,2025-02-24,Anthropic,United States of America,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,0.019,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recHaZf9xmVOj8cxI
gpt-4.1-2025-04-14,Terminus,0.303,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.021,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recVIdPAUDakmSij0
o3-2025-04-16_medium,Terminus,0.302,2025-04-16,OpenAI,United States of America,,,0.009,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recV9BNYucqd7iG2f
gpt-5-2025-08-07_medium,Terminus,0.3,2025-08-07,OpenAI,United States of America,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",0.009,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recGXLQ2JtOQGxSwC
gpt-5-mini-2025-08-07_medium,Terminus 2,0.3,2025-08-07,OpenAI,United States of America,,,0.009,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,reccP5RKLXt7AfwRD
o4-mini-2025-04-16_medium,Goose,0.275,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recEf1xgRFBHW2DL9
gemini-2.5-pro-preview-05-06,Terminus,0.253,2025-05-06,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.028,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recX5HvDyy6ZhGNFj
o4-mini-2025-04-16_medium,Codex CLI,0.2,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.015,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recDbDDaq781rDLYy
Qwen3-Coder-480B-A35B-Instruct,Orchestrator,0.197,2025-07-31,Alibaba,China,1.575e+24,6 FLOP / parameter / token * 35 * 10^9 active parameters * 7.5 * 10^12 tokens = 1.575e+24 FLOP`,0.02,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recfsmHhxrcp7XSXH
o4-mini-2025-04-16_medium,Terminus,0.185,2025-04-16,OpenAI,United States of America,,"We can’t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",0.014,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recRd8hJ5wfQSe3Vi
grok-3-beta,Terminus,0.175,2025-04-09,xAI,United States of America,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",0.042,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recKOBOp12o2sMvxs
gemini-2.5-flash-preview-04-17,Terminus,0.168,2025-04-17,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,0.013,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recyscDbMlLslAuHr
Llama-4-Maverick-17B-128E-Instruct,Terminus,0.155,2025-04-05,Meta AI,United States of America,2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",0.017,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recrnEQfyhorGSYUx
Qwen3-32B,TerminalAgent,0.155,2025-04-29,Alibaba,China,7.0848e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 32.8  * 10^9 parameters = 7.0848e+24 FLOP,0.011,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recMALFWwwE9SZZ9M
claude-sonnet-4-20250514,Mini SWE-Agent,0.128,2025-05-22,Anthropic,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.002,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recHId0RMDNk0pJix
gpt-5-nano-2025-08-07_medium,Terminus 2,0.122,2025-08-07,OpenAI,United States of America,,,0.015,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recPSyX4KLVqGC0Oe
codex-mini-2025-05-16,Codex CLI,0.113,2025-05-16,,,,,0.016,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,recpCwaNamDZITNIF
gpt-4.1-2025-04-14,Codex CLI,0.083,2025-04-14,OpenAI,United States of America,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,0.014,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,reccyI4aWNN41GEYb
qwen3-235b-a22b,Terminus,0.066,2025-04-29,Alibaba,China,4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,0.014,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,reci9iwpZzPhCpaQl
DeepSeek-R1,Terminus,0.057,2025-01-20,DeepSeek,China,4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that’s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek’s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3’s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",0.007,,Terminal-Bench Leaderboard,https://www.tbench.ai/leaderboard,rec49wbTlhSEHbviQ
