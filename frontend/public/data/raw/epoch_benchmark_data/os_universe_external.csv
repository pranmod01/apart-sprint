Model version,Weighted Score,Release date,Organization,Country,Training compute (FLOP),Training compute notes,Name,Agent,Paper,Wood,Bronze,Silver,Gold,Cost,Duration (seconds),Notes,Source,Source link,id
computer-use-preview-2025-03-11,0.478,2025-03-11,,,,,Computer use agent with computer-use-preview-2025-03-11,Computer use agent,1.0,0.8621,0.75,0.3438,0.0909,124.0,55921.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,recbTcR0VcO22PhxR
claude-3-5-sonnet-20241022,0.2836,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Claude Computer Use with claude-3-5-sonnet-20241022,Claude Computer Use,1.0,0.5345,0.4375,0.2188,0.0,73.0,26611.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,reca8hBVJjJ8YCYuw
claude-3-5-sonnet-20241022,0.2344,2024-10-22,Anthropic,United States of America,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",AgentDesk-based ReACT with claude-3-5-sonnet-20241022 ,AgentDesk-based ReACT,0.9091,0.569,0.3958,0.0938,0.0,49.0,29335.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,rec7c6zquoDnXGgys
Qwen2.5-VL-72B-Instruct,0.1864,2024-09-19,Alibaba,China,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",QWEN-based ReACT with qwen2.5-vl-72b-instruct,QWEN-based ReACT,0.9091,0.4655,0.3125,0.0625,0.0,63.0,96500.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,recVwsWsTN123cM0Q
gemini-2.5-pro-exp-03-25,0.0959,2025-03-25,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,AgentDesk-based ReACT with gemini-2.5-pro-exp-03-25 ,AgentDesk-based ReACT,0.9091,0.1552,0.1875,0.0312,0.0,24.0,30997.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,recaPGaftqlOiGhLM
gemini-2.0-flash-001,0.0826,2025-02-05,"Google DeepMind,Google","United States of America,United Kingdom of Great Britain and Northern Ireland",,"""We used Trillium TPUs to train the new Gemini 2.0, Googleâ€™s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",AgentDesk-based ReACT with gemini-2.0-flash-001,AgentDesk-based ReACT,0.9091,0.1379,0.1458,0.0312,0.0,4.0,45558.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,recxtja3SQ7xBN9x6
gpt-4o-2024-11-20,0.0679,2024-11-20,OpenAI,United States of America,,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,AgentDesk-based ReACT with gpt-4o-2024-11-20 ,AgentDesk-based ReACT,1.0,0.069,0.125,0.0312,0.0,78.0,50337.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,recnnx44lqU1ivOYw
gemini-1.5-pro-002,0.0612,2024-09-24,Google DeepMind,"United States of America,United Kingdom of Great Britain and Northern Ireland",,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,AgentDesk-based ReACT with gemini-1.5-pro-002,AgentDesk-based ReACT,0.9091,0.0345,0.125,0.0312,0.0,23.0,29673.0,,OSUniverse Paper,https://arxiv.org/pdf/2505.03570,rec35N6v6vBDXRnUM
,,,,,,,,,,,,,,,,,,,recSJCBLZea7GoFB8
