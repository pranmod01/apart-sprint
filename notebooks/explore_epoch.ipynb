{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a757c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6399de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loaded data:\n",
      "Total rows: 2827\n",
      "Rows with missing model: 401\n",
      "Rows with missing org: 595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>benchmark</th>\n",
       "      <th>date</th>\n",
       "      <th>org</th>\n",
       "      <th>country</th>\n",
       "      <th>training_compute_flops</th>\n",
       "      <th>score</th>\n",
       "      <th>capability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-5-2025-08-07_high</td>\n",
       "      <td>aider_polyglot</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>6.600000e+25</td>\n",
       "      <td>88.000</td>\n",
       "      <td>code_generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-5-2025-08-07_medium</td>\n",
       "      <td>aider_polyglot</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>6.600000e+25</td>\n",
       "      <td>86.700</td>\n",
       "      <td>code_generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>o3-pro-2025-06-10_high</td>\n",
       "      <td>aider_polyglot</td>\n",
       "      <td>2025-06-10</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.900</td>\n",
       "      <td>code_generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemini-2.5-pro-preview-06-05_32K</td>\n",
       "      <td>aider_polyglot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>United States of America,United Kingdom of Gre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.100</td>\n",
       "      <td>code_generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-5-2025-08-07_low</td>\n",
       "      <td>aider_polyglot</td>\n",
       "      <td>2025-08-07</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>6.600000e+25</td>\n",
       "      <td>81.300</td>\n",
       "      <td>code_generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>opt-13b</td>\n",
       "      <td>hella_swag</td>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.699</td>\n",
       "      <td>commonsense_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>gpt-j-6b</td>\n",
       "      <td>hella_swag</td>\n",
       "      <td>2021-08-05</td>\n",
       "      <td>EleutherAI,LAION</td>\n",
       "      <td>United States of America,Germany</td>\n",
       "      <td>1.500000e+22</td>\n",
       "      <td>0.662</td>\n",
       "      <td>commonsense_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824</th>\n",
       "      <td>dolly-v2-12b</td>\n",
       "      <td>hella_swag</td>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>Databricks</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.708</td>\n",
       "      <td>commonsense_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>Cerebras-GPT-13B</td>\n",
       "      <td>hella_swag</td>\n",
       "      <td>2023-03-20</td>\n",
       "      <td>Cerebras Systems</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>2.300000e+22</td>\n",
       "      <td>0.594</td>\n",
       "      <td>commonsense_reasoning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>hella_swag</td>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.407</td>\n",
       "      <td>commonsense_reasoning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2827 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model       benchmark        date  \\\n",
       "0                gpt-5-2025-08-07_high  aider_polyglot  2025-08-07   \n",
       "1              gpt-5-2025-08-07_medium  aider_polyglot  2025-08-07   \n",
       "2               o3-pro-2025-06-10_high  aider_polyglot  2025-06-10   \n",
       "3     gemini-2.5-pro-preview-06-05_32K  aider_polyglot         NaN   \n",
       "4                 gpt-5-2025-08-07_low  aider_polyglot  2025-08-07   \n",
       "...                                ...             ...         ...   \n",
       "2822                           opt-13b      hella_swag  2022-05-11   \n",
       "2823                          gpt-j-6b      hella_swag  2021-08-05   \n",
       "2824                      dolly-v2-12b      hella_swag  2023-04-11   \n",
       "2825                  Cerebras-GPT-13B      hella_swag  2023-03-20   \n",
       "2826           stablelm-tuned-alpha-7b      hella_swag  2023-04-19   \n",
       "\n",
       "                   org                                            country  \\\n",
       "0               OpenAI                           United States of America   \n",
       "1               OpenAI                           United States of America   \n",
       "2               OpenAI                           United States of America   \n",
       "3      Google DeepMind  United States of America,United Kingdom of Gre...   \n",
       "4               OpenAI                           United States of America   \n",
       "...                ...                                                ...   \n",
       "2822               NaN                                                NaN   \n",
       "2823  EleutherAI,LAION                   United States of America,Germany   \n",
       "2824        Databricks                           United States of America   \n",
       "2825  Cerebras Systems                           United States of America   \n",
       "2826               NaN                                                NaN   \n",
       "\n",
       "      training_compute_flops   score             capability  \n",
       "0               6.600000e+25  88.000        code_generation  \n",
       "1               6.600000e+25  86.700        code_generation  \n",
       "2                        NaN  84.900        code_generation  \n",
       "3                        NaN  83.100        code_generation  \n",
       "4               6.600000e+25  81.300        code_generation  \n",
       "...                      ...     ...                    ...  \n",
       "2822                     NaN   0.699  commonsense_reasoning  \n",
       "2823            1.500000e+22   0.662  commonsense_reasoning  \n",
       "2824                     NaN   0.708  commonsense_reasoning  \n",
       "2825            2.300000e+22   0.594  commonsense_reasoning  \n",
       "2826                     NaN   0.407  commonsense_reasoning  \n",
       "\n",
       "[2827 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.read_csv('/Users/pranmodu/Library/Mobile Documents/com~apple~CloudDocs/Desktop/coding projects/apart-sprint/data/processed/combined_benchmarks.csv')\n",
    "\n",
    "# Show initial statistics\n",
    "print(f\"Initial loaded data:\")\n",
    "print(f\"Total rows: {len(combined)}\")\n",
    "print(f\"Rows with missing model: {combined['model'].isna().sum()}\")\n",
    "print(f\"Rows with missing org: {combined['org'].isna().sum()}\")\n",
    "\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e58e8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Removing rows with empty model column\n",
      "  Before: 2827 rows, 595 missing orgs\n",
      "  After: 2426 rows, 194 missing orgs\n",
      "  Removed: 401 rows\n",
      "\n",
      "Step 2: Filling missing org values from duplicate models\n",
      "  Unique models with missing org: 61\n",
      "\n",
      "Final results:\n",
      "  Total rows: 2426\n",
      "  Filled orgs: 0\n",
      "  Still missing org: 194\n",
      "  (These 194 models don't have org info anywhere in the dataset)\n"
     ]
    }
   ],
   "source": [
    "combined['date'] = pd.to_datetime(combined['date'])\n",
    "\n",
    "# Create year column as integer (Int64 to handle NaN values properly)\n",
    "combined['year'] = combined['date'].dt.year.astype('Int64')\n",
    "\n",
    "# First, remove rows with empty model column\n",
    "print(f\"Step 1: Removing rows with empty model column\")\n",
    "print(f\"  Before: {len(combined)} rows, {combined['org'].isna().sum()} missing orgs\")\n",
    "combined = combined.dropna(subset=['model'])\n",
    "print(f\"  After: {len(combined)} rows, {combined['org'].isna().sum()} missing orgs\")\n",
    "print(f\"  Removed: {2827 - len(combined)} rows\")\n",
    "\n",
    "# Now fill missing org values for models that have org in other rows\n",
    "print(f\"\\nStep 2: Filling missing org values from duplicate models\")\n",
    "models_missing_org = combined[combined['org'].isna()]['model'].unique()\n",
    "print(f\"  Unique models with missing org: {len(models_missing_org)}\")\n",
    "\n",
    "filled_count = 0\n",
    "# For each model with missing org, try to find org from other rows with the same model\n",
    "for model in models_missing_org:\n",
    "    # Get all rows with this model that have an org\n",
    "    model_with_org = combined[(combined['model'] == model) & (combined['org'].notna())]\n",
    "    \n",
    "    if len(model_with_org) > 0:\n",
    "        # Get the most common org for this model\n",
    "        org_value = model_with_org['org'].mode()[0] if len(model_with_org['org'].mode()) > 0 else model_with_org['org'].iloc[0]\n",
    "        \n",
    "        # Count how many will be filled\n",
    "        rows_to_fill = ((combined['model'] == model) & (combined['org'].isna())).sum()\n",
    "        filled_count += rows_to_fill\n",
    "        \n",
    "        # Fill missing org for this model\n",
    "        combined.loc[(combined['model'] == model) & (combined['org'].isna()), 'org'] = org_value\n",
    "        print(f\"  Filled org '{org_value}' for {rows_to_fill} row(s) of model '{model}'\")\n",
    "\n",
    "# Check final stats\n",
    "final_missing = combined['org'].isna().sum()\n",
    "print(f\"\\nFinal results:\")\n",
    "print(f\"  Total rows: {len(combined)}\")\n",
    "print(f\"  Filled orgs: {filled_count}\")\n",
    "print(f\"  Still missing org: {final_missing}\")\n",
    "print(f\"  (These {final_missing} models don't have org info anywhere in the dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf34df23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLEANING DATA\n",
      "============================================================\n",
      "\n",
      "Starting with: 2426 rows\n",
      "\n",
      "1. Removed 34 rows with missing score\n",
      "   Remaining: 2392 rows\n",
      "\n",
      "2. Removed 62 rows with missing date\n",
      "   Remaining: 2330 rows\n",
      "\n",
      "3. Removed 245 exact duplicate rows\n",
      "   Remaining: 2085 rows\n",
      "\n",
      "============================================================\n",
      "FINAL CLEANED DATA: 2085 rows\n",
      "============================================================\n",
      "Missing values:\n",
      "  model: 0\n",
      "  score: 0\n",
      "  date: 0\n",
      "  year: 0\n",
      "  org: 166\n",
      "\n",
      "Saved to: /Users/pranmodu/Library/Mobile Documents/com~apple~CloudDocs/Desktop/coding projects/apart-sprint/data/processed/combined_benchmarks_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Apply recommended cleaning steps\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nStarting with: {len(combined)} rows\")\n",
    "\n",
    "# Step 1: Remove rows with missing score\n",
    "before = len(combined)\n",
    "combined = combined.dropna(subset=['score'])\n",
    "removed = before - len(combined)\n",
    "print(f\"\\n1. Removed {removed} rows with missing score\")\n",
    "print(f\"   Remaining: {len(combined)} rows\")\n",
    "\n",
    "# Step 2: Remove rows with missing date/year\n",
    "before = len(combined)\n",
    "combined = combined.dropna(subset=['date'])\n",
    "removed = before - len(combined)\n",
    "print(f\"\\n2. Removed {removed} rows with missing date\")\n",
    "print(f\"   Remaining: {len(combined)} rows\")\n",
    "\n",
    "# Step 3: Remove exact duplicate rows\n",
    "before = len(combined)\n",
    "combined = combined.drop_duplicates()\n",
    "removed = before - len(combined)\n",
    "print(f\"\\n3. Removed {removed} exact duplicate rows\")\n",
    "print(f\"   Remaining: {len(combined)} rows\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"FINAL CLEANED DATA: {len(combined)} rows\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Missing values:\")\n",
    "print(f\"  model: {combined['model'].isna().sum()}\")\n",
    "print(f\"  score: {combined['score'].isna().sum()}\")\n",
    "print(f\"  date: {combined['date'].isna().sum()}\")\n",
    "print(f\"  year: {combined['year'].isna().sum()}\")\n",
    "print(f\"  org: {combined['org'].isna().sum()}\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '/Users/pranmodu/Library/Mobile Documents/com~apple~CloudDocs/Desktop/coding projects/apart-sprint/data/processed/combined_benchmarks_cleaned.csv'\n",
    "combined.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fcfbbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org\n",
       "OpenAI                                                                                                                                                                 380\n",
       "Meta AI                                                                                                                                                                338\n",
       "Anthropic                                                                                                                                                              234\n",
       "Google DeepMind                                                                                                                                                        146\n",
       "Alibaba                                                                                                                                                                118\n",
       "Mistral AI                                                                                                                                                              95\n",
       "Technology Innovation Institute                                                                                                                                         83\n",
       "DeepSeek                                                                                                                                                                75\n",
       "Microsoft                                                                                                                                                               55\n",
       "MosaicML                                                                                                                                                                51\n",
       "xAI                                                                                                                                                                     50\n",
       "Google DeepMind,Google                                                                                                                                                  30\n",
       "01.AI                                                                                                                                                                   29\n",
       "Baichuan                                                                                                                                                                28\n",
       "DeepMind                                                                                                                                                                20\n",
       "Microsoft,NVIDIA                                                                                                                                                        20\n",
       "Google Research                                                                                                                                                         20\n",
       "Hugging Face,ServiceNow,NVIDIA,BigCode                                                                                                                                  12\n",
       "DeepSeek,Peking University                                                                                                                                              12\n",
       "Google                                                                                                                                                                  11\n",
       "Databricks                                                                                                                                                              10\n",
       "Moonshot                                                                                                                                                                10\n",
       "EleutherAI                                                                                                                                                              10\n",
       "Cerebras Systems                                                                                                                                                         8\n",
       "Stability AI                                                                                                                                                             8\n",
       "EleutherAI,LAION                                                                                                                                                         8\n",
       "Salesforce                                                                                                                                                               8\n",
       "Microsoft Research                                                                                                                                                       8\n",
       "Amazon                                                                                                                                                                   8\n",
       "NVIDIA                                                                                                                                                                   7\n",
       "Inflection AI                                                                                                                                                            4\n",
       "Nous Research,Arcee AI                                                                                                                                                   3\n",
       "Allen Institute for AI,University of Washington                                                                                                                          3\n",
       "Cohere,Cohere for AI                                                                                                                                                     3\n",
       "Hugging Face,BigScience                                                                                                                                                  3\n",
       "Google,Google DeepMind                                                                                                                                                   2\n",
       "Large Model Systems Organization,University of California (UC) Berkeley                                                                                                  2\n",
       "MiniMax                                                                                                                                                                  1\n",
       "Tsinghua University,University of Illinois Urbana-Champaign (UIUC),Shanghai AI Lab,Peking University,Shanghai Jiao Tong University,CUHK Shenzhen Research Institute      1\n",
       "Reka AI                                                                                                                                                                  1\n",
       "Allen Institute for AI,University of Washington,New York University (NYU)                                                                                                1\n",
       "Salesforce Research                                                                                                                                                      1\n",
       "Cohere                                                                                                                                                                   1\n",
       "Zhipu AI,Tsinghua University                                                                                                                                             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['org'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42768f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apart-venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
